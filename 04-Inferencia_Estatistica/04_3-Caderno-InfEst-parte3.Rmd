---
title: "04_3-Caderno-InfEst-parte3"
author: "Helena R. S. D'Espindula"
output:
  html_document: 
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
      number_sections: true
  pdf_document:
date: "`r Sys.Date()`"
---

```{r setup, echo=TRUE, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, warning = TRUE, error = TRUE)
library(ggplot2)
library(glmnet)
library(Matrix)
```




# √Ålgebra Matricial

## Vetores e escalares
- Um vetor √© uma lista de n n√∫meros (escalares) escritos em linha ou coluna.
- Nota√ß√£o (primeiro a em negrito)

$$
a = (a_{i1} ... a_{in})
$$
ou

$$
a = \begin{bmatrix}
a_{i1}\\
.\\
.\\
.\\
a_{in}\\
\end{bmatrix}
$$

- Vetor linha e vetor coluna.
- Um elemento do vetor √© chamado de ai , sendo i  a sua posi√ß√£o.
- O tamanho de um vetor √© o seu n√∫mero de elementos.
- O m√≥dulo de um vetor √© o seu comprimento
**[[ARRUMAR]]**

- Vetor unit√°rio √© aquele que tem tamanho
$$ a = a |a|$$

- Dois vetores s√£o iguais se tem o mesmo tamanho e os seus elementos em posi√ß√µes equivalentes s√£o iguais.

### Opera√ß√µes com vetores

1. Soma $a + b = (ai  + b i ) = (a1 + b 1, ‚Ä¶ , an + b n)$.

$a = (1, 2, 3)$
$b = (3, 2, 1)$
$a+b = (4, 4, 4)$
$a-b = (-2, 0, 2)$

2. Subtra√ß√£o $a - b = (ai  - b i ) = (a1 - b 1, ‚Ä¶ , an - b n)$.

$$
a-b = (-2, 0, 2)
$$

3. Multiplica√ß√£o por escalar $\alpha a = (\alpha a1, ‚Ä¶ , \alpha an)$.

$$
5 * a = (5*1, 5*2, 5*3)
$$

4. Transposta de um vetor:
**[[ARRUMAR]]**

5. Produto interno ou escalar entre dois vetores resulta em um escalar (mutiplica dois vetores e d√° um n√∫mero s√≥ como resultado)
$a * b = (a1b 1 + a2b 2 + ‚Ä¶ + anb n)$

- **Condi√ß√µes: os vetores devem ser do mesmo tipo e tamanho.**

### Vetores ortogonais
- Dois vetores s√£o ortogonais entre si se o √¢ngulo $\theta$ entre eles √© de 90 graus.(= correla√ß√£o de Pearson)
- Implica√ß√µes: 
$$cos(\theta) = 0 e aTb = 0.$$
$$ cov (a,b) / raiz(variacia[a]) * raiz(variacia[b])$$

- O co-seno do √¢ngulo $\theta$ entre os vetores √© dado por:
$$cos(\theta) = aTb / \sqrt aTa\sqrt bTb .$$

### Opera√ß√µes com vetores em R
- Declarando vetores
```{r}
a <- c(4,5,6)
b <- c(1,2,3)
```

- Sendo a e b compat√≠veis
```{r}
#### Soma
a + b
## [1] 5 7 9
#### Substra√ß√£o
a - b
## [1] 3 3 3
```


- Multiplica√ß√£o por escalar
```{r}
alpha = 10
alpha*a
## [1] 40 50 60
```
- Produto de Hadamard (n√£o √© produto interno)

```{r}
a*b
## [1] 4 10 18
```
- Produto vetorial (ou produto interno)

```{r}
a%*%b
##    [,1]
## [1,] 32
```
- Co-seno do √¢ngulo entre dois vetores
```{r}
cos <- t(a)%*%b/(sqrt(t(a)%*%a)*sqrt(t(b)%*%b))
```
- Lei da reciclagem (n√£o avalia se pode somar antes de somar)
```{r}
a <- c(4,5,6,5,6,7)
b <- c(1,2,3)
a + b
## [1] 5 7 9 6 8 10
```

## Matrizes

- Uma matriz √© um arranjo retangular ou quadrado de n√∫meros ou vari√°veis.
- A matriz costuma ser representada por uma letra maiuscula em negrito

- Uma matriz ($n \times m$) tem n linhas e m  colunas:

```{r latex}
# $$A = \begin{pmatrix}\
# a_{11} & a_{12} & ... & a_{1m}\\
# a_{21} & a_{22} & ... & a_{2m}\\
# ... & ... & ... & ... \\
# a_{n1} & a_{11} & ... & a_{nm}\\
# \end{pmatrix}$$
```


$$A = \begin{pmatrix}\
a_{11} & a_{12} & ... & a_{1m}\\
a_{21} & a_{22} & ... & a_{2m}\\
... & ... & ... & ... \\
a_{n1} & a_{11} & ... & a_{nm}\\
\end{pmatrix}$$

- O primeiro subscrito representa linha e o segundo representa coluna.
- A dimens√£o de uma matriz √© o seu n√∫mero de linhas e colunas.
- Duas matrizes s√£o iguais se tem a mesma dimens√£o e se os elementos das correspondentes
posi√ß√µes s√£o iguais.

### Matriz transposta
- A opera√ß√£o de transposi√ß√£o rearranja uma matriz de forma que suas linhas s√£o transformadas em colunas e vice-versa.
**[[ARRUMAR]]**

- Note que (AT)T = A.
- Computacionalmente
- Declarando matrizes

```{r}
a <- c(1,2,3,4,5,6)
A <- matrix(a, nrow = 3, ncol = 2)
A
## [,1] [,2]
## [1,] 1 4
## [2,] 2 5
## [3,] 3 6
```

- O default preenche por colunas
- Transposta de uma matriz
```{r}
t(A)
## [,1] [,2] [,3]
## [1,] 1 2 3
## [2,] 4 5 6
```

### Opera√ß√µes com matrizes

- Multiplica√ß√£o matriz por escalar.
$$\alpha * A = \begin{pmatrix}\
\alpha * a_{11} & \alpha * a_{12} & \alpha * ... & \alpha * a_{1m}\\
\alpha * a_{21} & \alpha * a_{22} & \alpha * ... & \alpha * a_{2m}\\
\alpha * ... & \alpha * ... & \alpha * ... & \alpha * ... \\
\alpha * a_{n1} & \alpha * a_{n2} & \alpha * ... & \alpha * a_{nm}\\
\end{pmatrix}$$

- Computacionalmente
```{r}
A <- matrix(c(1,2,3,4,5,6),
nrow = 3, ncol = 2)
alpha <- 10
alpha*A
## [,1] [,2]
## [1,] 10 40
## [2,] 20 50

```

- Duas matrizes podem ser somadas ou
subtra√≠das somente se tiverem o mesmo
tamanho.
1. Soma 
**[[ARRUMAR]]**
2. Subtra√ß√£o 
**[[ARRUMAR]]**

- Exemplo
$$A = \begin{pmatrix}\
1 & 2\\
3 & 4\\
5 & 6\\
\end{pmatrix}$$

$$B = \begin{pmatrix}\
10 & 20\\
30 & 40\\
50 & 60\\
\end{pmatrix}$$

$$A + B = \begin{pmatrix}\
11 & 22\\
33 & 44\\
55 & 66\\
\end{pmatrix}$$

- Soma de duas matrizes
```{r}
A <- matrix(c(1,2,3,4,5,6),
nrow = 3, ncol = 2)
B <- matrix(c(10,20,30,40,50,60),
nrow = 3, ncol = 2)
C = A + B
C
## [,1] [,2]
## [1,] 11 44
## [2,] 22 55
## [3,] 33 66
```

- Condi√ß√£o para multiplicar matrizes
$$
C_{m, n} = A_{m,q} B_{q,n}
$$
(q tem que ser igual)

**[[ARRUMAR]]**

- Computacionalmente.
- Matrizes compat√≠veis
```{r}
A <- matrix(c(2,8,6,-1,3,7),
nrow = 3, ncol = 2)
B <- matrix(c(4,-5,9,2,1,4,-3,6),
nrow = 2, ncol = 4)
C = A%*%B
C
## [,1] [,2] [,3] [,4]
## [1,] 13 16 -2 -12
## [2,] 17 78 20 -6
## [3,] -11 68 34 24
```

- Matrizes n√£o compat√≠veis
```{r message=TRUE, warning=TRUE}
B %*% A
## Error in B %*% A: argumentos n√£o compat√≠veis
```


Produto de Hadamard
- Produto simples ou de Hadamard

$$A \odot B = \begin{pmatrix}\
a_{11}*b_{11} & a_{12}*b_{12} & ... & a_{1m}*b_{1m}\\
a_{21}*b_{21} & a_{22}*b_{22} & ... & a_{2m}*b_{2m}\\
... & ... & ... & ... \\
a_{n1}*b_{n1} & a_{n2}*b_{n2} & ... & a_{nm}*b_{nm}\\
\end{pmatrix}$$


- Computacionalmente
```{r}
A <- matrix(c(1,2,3,4),
nrow = 2, ncol = 2)
B <- matrix(c(10,20,30,40),
nrow = 2, ncol = 2)
A*B
## [,1] [,2]
## [1,] 10 90
## [2,] 40 160
```


Propriedades envolvendo opera√ß√µes com matrizes
- Sendo A, B, C e D compat√≠veis temos,
1. $A + B = B + A$
2. $(A + B) + C = A + (B + C)$.
3. $\alpha (A + B) = \alpha A + \alpha B$.
4. $(\alpha  + \beta )A = \alpha A + \beta A$.
5. $\alpha (AB) = (\alpha A)B = A(\alpha B)$.
6. $A(B ¬± C) = AB ¬± AC$.
7. $(A ¬± B)C = AC ¬± BC$.
8. $(A-B)(C-D) = AC-BC-AD+BD$.

- Propriedades envolvendo transposta e
multiplica√ß√£o
1. Se $A$ √© $n \times m$ e $B$ √© $m \times n$, ent√£o $(AB)^T = B^T A^T$.
2. Se $A$, $B$ e $C$ s√£o compat√≠veis 
$$
(ABC)^{T}= C^{T}B^{T}A^{T}.
$$

### Matrizes de formas especiais
- Matriz quadrada ($m = n$)

Exemplo 4x4
```{r}
A <- matrix(c("a11","a21","a31","a41","a12","a22","a32","a42","a13","a23","a33","a43","a14","a24","a34","a44"), nrow = 4, ncol = 4)
A
```

- ai i  s√£o os elementos da diagonal.
- ai j  para i  (diferente) j  ‚Üí fora da diagonal.
- ai j  para j  > i  ‚Üí acima da diagonal.
- ai j  para i  > j  ‚Üí abaixo da diagonal.
- Matriz diagonal
$$D = \begin{pmatrix}\
a_{11} & 0 & 0 & 0\\
0 & a_{22} & 0 & 0\\
0 & 0 & a_{33} & 0\\
0 & 0 & 0 & a_{44}\\
\end{pmatrix}$$



- Matriz identidade
$$I = \begin{pmatrix}\
1 & 0 & 0 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1\\
\end{pmatrix}$$



### Matrizes de formas especiais
- Triangular superior
$$U = \begin{pmatrix}\
a_{11} & a_{12} & a_{13} & a_{14}\\
0 & a_{22} & a_{23} & a_{24}\\
0 & 0 & a_{33} & a_{34} \\
0 & 0 & 0 & a_{44}\\
\end{pmatrix}$$

- Triangular inferior
$$L = \begin{pmatrix}\
a_{11} & 0 & 0 & 0\\
a_{21} & a_{22} & 0 & 0\\
a_{31} & a_{32} & a_{33} & 0 \\
a_{41} & a_{42} & a_{43} & a_{44}\\
\end{pmatrix}$$

```{r}
# $$L = \begin{pmatrix}\
# a_{11} & a_{12} & a_{13} & a_{14}\\
# a_{22} & a_{22} & a_{23} & a_{24}\\
# a_{22} & a_{22} & a_{33} & a_{34} \\
# a_{22} & a_{22} & a_{22} & a_{44}\\
# \end{pmatrix}$$
```



- Matriz nula
$$0 = \begin{pmatrix}\
0 & 0 & 0 & 0\\
0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0\\
\end{pmatrix}$$


- Matriz quadrada sim√©trica
$$0 = \begin{pmatrix}\
1 & 0,8 & 0,6 & 0,4\\
0,8 & 1 & 0,2 & 0,4\\
0,6 & 0,2 & 1 & 0,1 \\
0,4 & 0,4 & 0,1 & 1,0\\
\end{pmatrix}$$


### Combina√ß√µes lineares
- Um conjunto de vetores a1, a2, ‚Ä¶ , an √© dito ser linearmente dependente se puderem ser
encontrados escalares c 1, c 2, ‚Ä¶ , c n e estes escalares n√£o sejam todos iguais a 0 de tal forma
que
$$
c 1a1 + c 2a2 + ‚Ä¶ + c nan = 0.
$$

Exemplo:
```{r}
a1 <- matrix(c(1,0), nrow = 2, ncol = 1)
a1
a2 <- matrix(c(0,1), nrow = 2, ncol = 1)
a2

#O unico caso que esses c1*a1 + c2*a2 = (0, 0) √© se c1 = 0 E c2 =0
#Ou seja Linearmente independente

a1 <- matrix(c(1,2), nrow = 2, ncol = 1)
a1
a2 <- matrix(c(-1,-2), nrow = 2, ncol = 1)
a2

#Existem casos fora os cs = 0 que fazem c1*a1 + c2*a2 = (0, 0)
#Ou seja Linearmente dependente

```


- Caso contr√°rio √© dito ser linearmente independente.
- Nota√ß√£o matricial
$$
Ac = 0.
$$
- As colunas de A s√£o linearmente independentes se Ac = 0 implicar que c = 0.

### Rank ou posto de uma matriz
- O rank ou posto de qualquer matriz quadrada ou retangular A √© definido como
rank(A) = n√∫mero de colunas ou linhas linearmente independentes em A.
- Sendo A uma matriz retangular n x  m  o maior rank poss√≠vel para A √© o min(n,m ).
- O rank da matrix nula √© 0.
- Se o rank da matriz √© o min(n,m ) dizemos que a matriz tem rank completo.

### Matriz n√£o singular e matriz inversa
- Uma matriz quadrada de posto completo √© chamada de n√£o singular.
- Sendo A quadrada de posto completo a matriz inversa de A √© √∫nica tal que (s√≥ se a matriz for quadrada e de ranking completo)
$$
AA^{-1} = I.
$$
- N√£o quadrada (posto incompleto) ‚Üí n√£o ter√° inversa e √© dita ser singular.
- Note que 
$$
A^{(-1^{-1})} =A
$$
A^{-1}^{-1} = A

## Matriz inversa
- Computacionalmente
```{r}
A <- matrix(c(4, 2, 7, 6), 2, 2)
A

A_inv <- solve(A)
A_inv

I = A %*% A_inv
I

```

- Verificando
```{r}
A%*%A_inv
## [,1] [,2]
## [1,] 1 0
## [2,] 0 1
```


- Propriedades envolvendo inversas
1. Se A √© n√£o singular, ent√£o AT √© n√£o singular e sua inversa √© dada por $(AT)-1 = (A-1)T$
2. Se A e B s√£o matrizes n√£o singulares de mesmo tamanho, ent√£o o produto AB √©
n√£o singular e $(AB)-1 = B-1 A-1$

### Inversa generalizada
- A inversa generalizada de uma matriz A n x  p √© qualquer matriz A- que satisfa√ßa 
$$
AA^{-}A = A.
$$

- N√£o √© √∫nica exceto quando A √© n√£o-singular (inversa usual).
- Exemplo

$$

$$
**[[ARRUMAR]]**


- a- = (1, 0, 0, 0)

- Verificando

```{r}
a <- matrix(c(1, 2, 3, 4), 4, 1)
a_invg <- matrix(c(1,0,0,0), 1, 4)
a%*%a_invg%*%a
## [,1]
## [1,] 1
## [2,] 2
## [3,] 3
## [4,] 4
```

- Moore-Penrose generalized inverse
```{r}
#### Matriz singular (col 3 = col 2 + col 1)
A <- matrix(c(2, 1, 3, 2, 0,
2, 3, 1, 4), 3, 3)
library(MASS)
A_ginv <- ginv(A)
A%*%A_ginv%*%A ## Verificando
```

## Matrizes positivas definidas

### Formas quadr√°ticas
- Soma de quadrados s√£o importantes em ci√™ncia de dados.
- Considere uma matriz A sim√©trica e y um vetor, o produto
$$
y^{T}Ay = 
\sum(a_{ij}y^{2}_{i}) + 
\sum_{i (diferente) j}(a_{ij}y_{i}y_{j})
$$
√© chamado de forma quadr√°tica.

$$
y^{T}Iy = \sum^{n}_{i=0}(y^{2}_{i})
$$


- Sendo y de dimens√£o n x  1, 
$$
yTIy = y 2
1 + y 2
2 + ‚Ä¶ , y 2
n
$$

- Consequentemente, yTy √© a soma de quadrados dos elementos do vetor y.
- A raiz quadrada da soma de quadrados √© o comprimento de y.

Matriz positiva definida
- Sendo A uma matriz sim√©trica com a propriedade yTAy > 0 para todos os poss√≠veis y
exceto para quando y = 0, ent√£o a forma quadr√°tica yTAy √© chamada positiva definida,
e A √© dita ser uma matriz positiva definida.
- Exemplo
**[[ARRUMAR]]**

A forma quadr√°tica associada √© dada por (ver abaixo) que √© claramente positiva, desde que y 1 e y 2 sejam diferentes de zero.
$$
yTAy = (y 1 y 2) ( 2 -1
-1 3 ) (y 1
y 2
) = 2y 2
1 - 2y 1y 2 + 3y 2
2 ,
$$

### Propriedades de matrizes positivas definidas
1. Se A √© positiva definida, ent√£o todos os valores da diagonal de A s√£o positivos.
2. Se A √© positiva semi-definida, ent√£o os elementos da diagonal de A s√£o maiores ou iguais a zero.
3. Sendo P uma matriz n√£o-singular e A uma matriz positiva definida, o produto PTAP √© positiva definida.
4. Sendo P uma matriz n√£o-singular e A uma matriz positiva semi-definida, o produto PTAP √© positiva semi-definida.
5. Uma matriz positiva definida √© n√£o-singular.

### Determinante de uma matriz
- O determinante de uma matriz A √© o escalar (= numero)
$$
|A| = \sum((-1)^k a_{1j_{1}} a_{2j_{2}} ... a_{nj_{n}})
$$
onde a soma √© realizada para todas as n! permuta√ß√µes de grau n, e k  √© o n√∫mero de
mudan√ßas necess√°rias para que os segundos subscritos sejam colocados na ordem
$1,2, ‚Ä¶ , n$

- Considere a matriz
**[[ARRUMAR]]**

Determinante de uma matriz
- Computacionalmente.
```{r}
A <- matrix(c(3,-2,-2,4),2,2)
determinant(A, logarithm = FALSE)$modulus
## [1] 8
## attr(,"logarithm")
## [1] FALSE
```

- Determinante em escala log.
```{r}
determinant(A, logarithm = TRUE)$modulus
## [1] 2.079442
## attr(,"logarithm")
## [1] TRUE
```

- Alguns aspectos interessantes sobre determinantes s√£o:
1. Se A √© singular, |A| = 0.
2. Se A √© n√£o singular, |A| (diferente) 0.
3. Se A √© positiva definida, |A| > 0.
4. |AT| = |A|.
5. Se A √© n√£o singular, |A-1| = 1
|A| .

Tra√ßo de uma matriz
- O tra√ßo de uma matriz A n x  n √© um escalar definido como a soma dos elementos da diagonal, 
**[[ARRUMAR]]**

- Propriedades
1. Se A e B s√£o n x  n, ent√£o tr(A + B) = tr(A) + tr(B).
2. Se A √© n x  p e B e p x  n, ent√£o tr(AB) = tr(BA).

- Computacionalmente
```{r}
A <- matrix(c(3,-2,-2,4),2,2)
sum(diag(A))
## [1] 7
```

## C√°lculo vetorial e matricial

### C√°lculo vetorial
- Seja $y = f(x)$ uma fun√ß√£o das vari√°veis $x_{1}, x_{2}, x_{3}, ... , x_{p}$ e $\partial y$  as respectivas derivadas parciais.
**[[ARRUMAR]]**

Assim,
**[[ARRUMAR]]**

### C√°lculo vetorial
- Sendo aT = (a1, a2, ‚Ä¶ , ap) um vetor de constantes e A uma matriz sim√©trica de constantes.
1. Seja y  = aTx = xTa. Ent√£o,
**[[ARRUMAR]]**

2. Seja y  = xTAx. Ent√£o,
**[[ARRUMAR]]**

### C√°lculo Matricial
- Se y  = f (X) onde X √© uma matriz p x  p. As derivadas parciais de y  em rela√ß√£o a cada x i j 
s√£o organizadas em uma matriz.
**[[ARRUMAR]]**


- Algumas derivadas importantes envolvendo matrizes s√£o apresentadas abaixo.
1. Seja y  = tr(XA) sendo X p x  p e definida positiva e A p x  p constantes. Ent√£o,
**[[ARRUMAR]]**

2. Sendo A n√£o singular com derivadas $\partial A$
**[[ARRUMAR]]**

3. Sendo A n x  n positiva definida. Ent√£o,
**[[ARRUMAR]]**

## Regress√£o linear m√∫ltipla

### Regress√£o linear m√∫ltipla: especifica√ß√£o usual

- Regress√£o linear simples
$$
y_{i} = \beta_{0} +\beta_{1}x_{1} + erro_{i}
$$
- Regress√£o linear m√∫ltipla
$$
y_{i} = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2} + ... + \beta_{p}x_{ip} + erro_{i}
$$
- Modelo para cada observa√ß√£o
$$y_{1} = \beta_{0} + \beta_{1}x_{11} + \beta_{2}x_{12} + ... + \beta_{p}x_{1p} + erro_{1}$$

$$y_{2} = \beta_{0} + \beta_{1}x_{21} + \beta_{2}x_{22} + ... + \beta_{p}x_{2p} + erro_{1}$$
$$...$$
$$y_{n} = \beta_{0} + \beta_{1}x_{n1} + \beta_{2}x_{n2} + ... + \beta_{p}x_{np} + erro_{n}$$

Regress√£o linear m√∫ltipla: especifica√ß√£o matricial
- Nota√ß√£o matricial
$$
\begin{bmatrix}
y_{1}\\
y_{2}\\
...\\
y_{n}\\
\end{bmatrix}
= 
\begin{bmatrix}
1 & x_{1}\\
1 & x_{2}\\
1 & ...\\
1 & x_{n}\\
\end{bmatrix}
x 
\begin{bmatrix}
\beta_{1}\\
\beta_{2}\\
...\\
\beta_{n}\\
\end{bmatrix}
+
\begin{bmatrix}
erro_{1}\\
erro_{2}\\
...\\
erro_{n}\\
\end{bmatrix}
$$

- Nota√ß√£o mais compacta
$$
y_{(n x  1)} = X_{(n x  p)} \beta_{(p x  1)} + erro_{(n x  1)}
$$
### Regress√£o linear m√∫ltipla: estima√ß√£o (treinamento)
- Objetivo: encontrar o vetor $\hat{\beta}$ , tal que $S Q (\beta ) = (y - X\beta )T(y - X\beta )$ seja a menor poss√≠vel.

### Regress√£o linear m√∫ltipla: estima√ß√£o
1. Passo 1: encontrar o vetor gradiente. Derivando em $\beta$ , temos

**[[ARRUMAR]]**

### Regress√£o linear m√∫ltipla: estima√ß√£o

2. Passo 2: resolver o sistema de equa√ß√µes lineares (esquece o "-2" primeiro)
$$ X^{T} (y - X\hat{\beta}) = 0$$
$$XTy - XTXÃÇ\beta  = 0$$
$$XTXÃÇ\beta  = XTyÃÇ$$
$$(XTX)^{-1}  XTXÃÇ\beta  = XTy (XTX)^{-1}$$
$$ I\beta  = (XTX)-1XTy $$

### Regress√£o linear m√∫ltipla: exemplo
- Conjunto de dados Boston dispon√≠vel no pacote MASS.
- Cinco primeiras covari√°veis dispon√≠veis:
  - crim: taxa de crimes per capita.
  - zn: propor√ß√£o de terrenos residenciais zoneados para lotes com mais de 25.000 p√©s quadrados.
  - indus: propor√ß√£o de acres de neg√≥cios n√£o varejistas por cidade.
  - chas: vari√°vel dummy de Charles River (1 se a √°rea limita o rio; 0 caso contr√°rio).
  - nox: concentra√ß√£o de √≥xido de nitrog√™nio (parte por 10 milh√µes).
- Vari√°vel resposta: medv valor mediano das casas ocupadas em $1000.

### Regress√£o linear m√∫ltipla: implementa√ß√£o computacional
- Carregando a base de dados 
```{r}
require(MASS)
## Carregando pacotes exigidos: MASS

data(Boston)
head(Boston[, c(1:5,14)])
## crim zn indus chas nox medv
## 1 0.00632 18 2.31 0 0.538 24.0
## 2 0.02731 0 7.07 0 0.469 21.6
## 3 0.02729 0 7.07 0 0.469 34.7
## 4 0.03237 0 2.18 0 0.458 33.4
## 5 0.06905 0 2.18 0 0.458 36.2
## 6 0.02985 0 2.18 0 0.458 28.7
```


- Matriz de delineamento (X).
```{r}
X <- model.matrix(~ crim + zn + indus +
chas + nox, data = Boston)
head(X)
## (Intercept) crim zn indus chas nox
## 1 1 0.00632 18 2.31 0 0.538
## 2 1 0.02731 0 7.07 0 0.469
## 3 1 0.02729 0 7.07 0 0.469
## 4 1 0.03237 0 2.18 0 0.458
## 5 1 0.06905 0 2.18 0 0.458
## 6 1 0.02985 0 2.18 0 0.458
```


- Vari√°vel resposta
```{r}
y <- Boston$medv
```

- Estimadores de m√≠nimos quadrados:
$$
\hat{\beta} = (X^{T}X)^{-1} X^{T}y
$$
- Computacionalmente: vers√£o ing√™nua (calcula inversa)
```{r}
round(solve(t(X)%*%X)%*%t(X)%*%y, 2)
## [,1]
## (Intercept) 29.49
## crim -0.22
## zn 0.06
## indus -0.38
## chas 7.03
## nox -5.42
```

- Computacionalmente: vers√£o eficiente (escalona?)
```{r}
round(solve(t(X)%*%X, t(X)%*%y), 2)
## [,1]
## (Intercept) 29.49
## crim -0.22
## zn 0.06
## indus -0.38
## chas 7.03
## nox -5.42
```

- Fun√ß√£o nativa do R
```{r}
t(round(coef(lm(medv ~ crim + zn + indus + chas + nox, data = Boston)), 2))
## (Intercept) crim zn indus chas nox
## [1,] 29.49 -0.22 0.06 -0.38 7.03 -5.42
```

### Matrizes esparsas (t√≥pico adicional)

- Matrizes aparecem em todos os tipos de aplica√ß√£o em ci√™ncia de dados.
- Modelos estat√≠sticos, machine learning, an√°lise de texto, an√°lise de cluster, etc.
- Muitas vezes as matrizes usadas t√™m uma grande quantidade de zeros.
- Quando uma matriz tem uma quantidade consider√°vel de zeros, dizemos que ela √©
esparsa, caso contr√°rio dizemos que a matriz √© densa.
- Todas as propriedades que vimos para matrizes em geral valem para matrizes esparsas.
- O R tem um conjunto de m√©todos altamente eficiente por meio do pacote Matrix.
- Saber que uma matriz √© esparsa √© √∫til pois permite:
- Planejar formas de armazenar a matriz em mem√≥ria.
- Economizar c√°lculos em algoritmos num√©ricos (multiplica√ß√£o, inversa, determinante,
decomposi√ß√µes, etc).

- Comparando a quantidade de mem√≥ria utilizada.
```{r}
library('Matrix')

m1 <- matrix(0, nrow = 1000, ncol = 1000)
object.size(m1)
## 8000216 bytes

m2 <- Matrix(0, nrow = 1000, ncol = 1000, sparse = TRUE)
object.size(m2)
## 9240 bytes
```


Comparando o tempo computacional


- Matriz densa
```{r}
y <- rnorm(1000)
X <- matrix(NA, ncol = 100, nrow = 1000)
for(i in 1:1000) {X[i,] <- rbinom(100, size = 1, p = 0.1)}
system.time(replicate(100, solve(t(X)%*%X, t(X)%*%y)))
## usu√°rio sistema decorrido
## 0.819 0.004 0.823
```


- Matriz esparsa
```{r}
y <- rnorm(1000)
X <- Matrix(NA, ncol = 100, nrow = 1000)
for(i in 1:1000) {X[i,] <- rbinom(100, size = 1, p = 0.1)}
X <- Matrix(X, sparse = TRUE)
system.time(replicate(100, solve(t(X)%*%X, t(X)%*%y)))
## usu√°rio sistema decorrido
## 0.223 0.000 0.224
```

### Diferentes formas de implementar as opera√ß√µes matriciais

- Criando a base de dados para a compara√ß√£o
```{r}
library(Matrix)
n <- 10000; p <- 500

#DENSA
x <- matrix(rbinom(n*p, 1, 0.01), nrow=n, ncol=p)
object.size(x)
## 20000216 bytes

#ESPARCA
X <- Matrix(x)
object.size(X)
## 600432 bytes
```

- Diferentes implementa√ß√µes
```{r}
y <- rnorm(n)

print("Matriz densa com %*%:")
system.time(solve(t(x)%*%x, t(x)%*%y))
## usu√°rio sistema decorrido
## 2.053 0.040 2.094

print("Matriz densa com crossprod")
system.time(solve(crossprod(x), crossprod(x, y)))
## usu√°rio sistema decorrido
## 1.731 0.016 1.748

print("Matriz espar√ßa com %*%")
system.time(solve(t(X)%*%X, t(X)%*%y))
## usu√°rio sistema decorrido
## 0.071 0.000 0.072

print("Matriz espar√ßa com crossprod")
system.time(solve(crossprod(X), crossprod(X,y)))
## usu√°rio sistema decorrido
## 0.029 0.000 0.050
```

- Implementa√ß√£o eficiente do modelo de regress√£o linear m√∫ltipla.
```{r}
library(glmnet)
## Loaded glmnet 4.1-6
system.time(b <- coef(lm(y~x)))
## usu√°rio sistema decorrido
## 2.389 0.044 2.434
system.time(g1 <-glmnet(x, y, nlambda=1, lambda=0, standardize=FALSE))
## usu√°rio sistema decorrido
## 0.065 0.020 0.086
system.time(g2 <- glmnet(X, y, nlambda=1, lambda=0, standardize=FALSE))
## usu√°rio sistema decorrido
## 0.006 0.000 0.006
```


# Proxima aula

### Sistemas lineares
- Sistema com duas equa√ß√µes:
$$ f 1(x 1,x 2) = 0$$
$$f 2(x 1,x 2) = 0$$
- Solu√ß√£o num√©rica consiste em encontrar\hat{} x 1 e\hat{} x 2 que satisfa√ßa o sistema de equa√ß√µes.
- Sistema com n equa√ß√µes
$$f 1(x 1, ‚Ä¶ , x n) = 0
‚ãÆ
f n(x 1, ‚Ä¶ , x n) = 0.
- Genericamente, tem-se
f(x ) = 0.$$

- Equa√ß√µes podem ser lineares ou n√£o-lineares.

Sistemas de equa√ß√µes lineares
- Cada equa√ß√£o √© linear na inc√≥gnita.
- Solu√ß√£o anal√≠tica em geral √© poss√≠vel.
- Exemplo:
$7x 1 + 3x 2 = 45$
$4x 1 + 5x 2 = 29$
- Solu√ß√£o anal√≠tica:$\hat{} x 1 = 6 e\hat{} x 2 = 1$
- Resolver (tedioso!!).

- Tr√™s poss√≠veis casos:
1. Uma √∫nica solu√ß√£o (sistema n√£o singular).
2. Infinitas solu√ß√µes (sistema singular).
3. Nenhuma solu√ß√£o (sistema imposs√≠vel).

Sistemas de equa√ß√µes lineares
- Representa√ß√£o matricial do sistema de equa√ß√µes lineares:
**[[ARRUMAR]]**

- De forma geral, tem-se
$Ax = b$

Opera√ß√µes com linhas
- Sem qualquer altera√ß√£o na rela√ß√£o linear, √© poss√≠vel
1. Trocar a posi√ß√£o de linhas:
$4x 1 + 5x 2 = 29$
$7x 1 + 3x 2 = 45$
2. Multiplicar qualquer linha por uma constante, aqui 4x 1 + 5x 2 por 1x4 , obtendo
**[[ARRUMAR]]**

Opera√ß√µes com linhas
3. Subtrair um m√∫ltiplo de uma linha de uma outra, aqui 7 * ùê∏ùëû.(1) menos Eq. (2), obtendo
**[[ARRUMAR]]**
- Fazendo as contas, tem-se
**[[ARRUMAR]]**

Solu√ß√£o de sistemas lineares
- Forma geral de um sistema com n equa√ß√µes lineares:
**[[ARRUMAR]]**

- Matricialmente, tem-se
**[[ARRUMAR]]**

- M√©todos diretos e m√©todos iterativos.

### M√©todos diretos

- O sistema de equa√ß√µes √© manipulado at√© se transformar em um sistema equivalente de
f√°cil resolu√ß√£o.
- Triangular superior:
**[[ARRUMAR]]**

- Substitui√ß√£o regressiva
**[[ARRUMAR]]**


M√©todos diretos
- Triangular inferior:
**[[ARRUMAR]]**

- Substitui√ß√£o progressiva
**[[ARRUMAR]]**



M√©todos diretos
- Diagonal:
**[[ARRUMAR]]**


Elimina√ß√£o de Gauss

M√©todos diretos: Elimina√ß√£o de Gauss
- M√©todo de Elimina√ß√£o de Gauss consiste em manipular o sistema original usando
opera√ß√µes de linha at√© obter um sistema triangular superior.
**[[ARRUMAR]]**

- Usar elimina√ß√£o regressiva no novo sistema para obter a solu√ß√£o.
- Resolva o seguinte sistema usando Elimina√ß√£o de Gauss.
**[[ARRUMAR]]**


M√©todos diretos: Elimina√ß√£o de Gauss
- Passo 1: encontrar o piv√¥ e eliminar os elementos abaixo dele usando opera√ß√µes de linha.
**[[ARRUMAR]]**

- Passo 2: encontrar o segundo piv√¥ e eliminar os elementos abaixo dele usando opera√ß√µes
de linha.
**[[ARRUMAR]]**

- Passo 3: substitui√ß√£o regressiva.

M√©todos diretos: Elimina√ß√£o de Gauss
- Usando a f√≥rmula de substitui√ß√£o regressiva temos:
**[[ARRUMAR]]**

- A extens√£o do procedimento para um sistema com n equa√ß√µes √© trivial.
1. Transforme o sistema em triangular superior usando opera√ß√µes linhas.
2. Resolva o novo sistema usando substitui√ß√£o regressiva.
- Potenciais problemas do m√©todo de elimina√ß√£o de Gauss:
- O elemento piv√¥ √© zero.
- O elemento piv√¥ √© pequeno em rela√ß√£o aos demais termos.

Elimina√ß√£o de Gauss com pivota√ß√£o

Elimina√ß√£o de Gauss com pivota√ß√£o
- Considere o sistema
$0x 1 + 2x 2 + 3x 2 = 46$
$4x 1 - 3x 2 + 2x 3 = 16$
$2x 1 + 4x 2 - 3x 3 = 12$
- Neste caso o piv√¥ √© zero e o procedimento n√£o pode come√ßar.
- Pivota√ß√£o - trocar a ordem das linhas.
1. Evitar piv√¥s zero.
2. Diminuir o n√∫mero de opera√ß√µes necess√°rias para triangular o sistema.
$4x 1 - 3x 2 + 2x 3 = 16$
$2x 1 + 4x 2 - 3x 3 = 12$
$0x 1 + 2x 2 + 3x 2 = 46$

Elimina√ß√£o de Gauss com pivota√ß√£o
- Se durante o procedimento uma equa√ß√£o piv√¥ tiver um elemento nulo e o sistema tiver
solu√ß√£o, uma equa√ß√£o com um elemento piv√¥ diferente de zero sempre existir√°.
- C√°lculos num√©ricos s√£o menos propensos a erros e apresentam menores erros de
arredondamento se o elemento piv√¥ for grande em valor absoluto.
- √â usual ordenar as linhas para que o maior valor seja o primeiro piv√¥.

Passo 1: obtendo uma matriz triangular superior.
```{r}
gauss <- function(A, b) {
Ae <- cbind(A, b) ## Sistema aumentado
rownames(Ae) <- paste0("x", 1:length(b))
n_row <- nrow(Ae)
n_col <- ncol(Ae)
SOL <- matrix(NA, n_row, n_col) ## Matriz para receber os resultados
SOL[1,] <- Ae[1,]
pivo <- matrix(0, n_col, n_row)
for(j in 1:c(n_row-1)) {
for(i in c(j+1):c(n_row)) {
pivo[i,j] <- Ae[i,j]/SOL[j,j]
SOL[i,] <- Ae[i,] - pivo[i,j]*SOL[j,]
Ae[i,] <- SOL[i,]
}
}
return(SOL)
}
```



Elimina√ß√£o de Gauss sem pivota√ß√£o
- Passo 2: substitui√ß√£o regressiva
```{r}
sub_reg <- function(SOL) {
n_row <- nrow(SOL)
n_col <- ncol(SOL)
A <- SOL[1:n_row,1:n_row]
b <- SOL[,n_col]
n <- length(b)
x <- c()
x[n] <- b[n]/A[n,n]
for(i in (n-1):1) {
x[i] <- (b[i] - sum(A[i,c(i+1):n]*x[c(i+1):n] ))/A[i,i]
}
return(x)
}
```

Elimina√ß√£o de Gauss sem pivota√ß√£o
- Resolva o sistema:
**[[ARRUMAR]]**

```{r}
A <- matrix(c(3,2,5,2,4,3,6,3,4),3,3)
b <- c(24,23,33)
S <- gauss(A, b) ## Passo 1: Triangulariza√ß√£o
sol = sub_reg(SOL = S) ## Passo 2: Substitui√ß√£o regressiva
sol
## [1] 4 3 1
A%*%sol ## Verificando a solu√ß√£o
## [,1]
## [1,] 24
## [2,] 23
## [3,] 33

```


Elimina√ß√£o de Gauss com pivota√ß√£o
- Resolva o seguinte sistema usando
Elimina√ß√£o de Gauss com pivota√ß√£o.
$0x 1 + 2x 2 + 3x 2 = 46$
$4x 1 - 3x 2 + 2x 3 = 16$
$2x 1 + 4x 2 - 3x 3 = 12$

```{r}
## Entrando com o sistema original
A <- matrix(c(0,4,2,2,-3,4,3,2,-3), 3,3)
b <- c(46,16,12)
## Pivoteamento
A_order <- A[order(A[,1], decreasing = TRUE),]
b_order <- b[order(A[,1], decreasing = TRUE)]
#### Triangula√ß√£o
S <- gauss(A_order, b_order)
S
## [,1] [,2] [,3] [,4]
## [1,] 4 -3.0 2.000000 16.00000
## [2,] 0 5.5 -4.000000 4.00000
## [3,] 0 0.0 4.454545 44.54545
#### Substitui√ß√£o regressiva
sol <- sub_reg(SOL = S)
sol
## [1] 5 8 10
#### Solu√ß√£o
A_order%*%sol
## [,1]
## [1,] 16
## [2,] 12
## [3,] 46
```


Elimina√ß√£o de Gauss-Jordan

M√©todos diretos: Elimina√ß√£o de Gauss-Jordan
- O sistema original √© manipulado at√© obter um sistema equivalente na forma diagonal.
**[[ARRUMAR]]**

- Algoritmo Gauss-Jordan
1. Normalize a equa√ß√£o piv√¥ com a divis√£o de todos os seus termos pelo coeficiente piv√¥.
2. Elimine os elementos fora da diagonal principal em TODAS as demais equa√ß√µes usando
opera√ß√µs de linha.
- O m√©todo de Gauss-Jordan pode ser combinado com pivota√ß√£o igual ao m√©todo de
elimina√ß√£o de Gauss.


M√©todos iterativos
- Nos m√©todos iterativos, as equa√ß√µes s√£o colocadas em uma forma expl√≠cita onde cada
inc√≥gnita √© escrita em termos das demais, i.e.
**[[ARRUMAR]]**

- Dado um valor inicial para as inc√≥gnitas estas ser√£o atualizadas at√© a converg√™ncia.
- Atualiza√ß√£o: M√©todo de Jacobi
**[[ARRUMAR]]**


- Atualiza√ß√£o: M√©todo de Gauss-Seidel
**[[ARRUMAR]]**

M√©todo iterativo de Jacobi
- Implementa√ß√£o computacional
```{r}
jacobi <- function(A, b, inicial, max_iter = 10, tol = 1e-04) {
n <- length(b)
x_temp <- matrix(NA, ncol = n, nrow = max_iter)
x_temp[1,] <- inicial
x <- x_temp[1,]
for(j in 2:max_iter) { #### Equa√ß√£o de atualiza√ß√£o
for(i in 1:n) {
x_temp[j,i] <- (b[i] - sum(A[i,1:n][-i]*x[-i]))/A[i,i]
}
x <- x_temp[j,]
if(sum(abs(x_temp[j,] - x_temp[c(j-1),])) < tol) break #### Crit√©rio de parada
}
return(list("Solucao" = x, "Iteracoes" = x_temp))
}

```


M√©todo iterativo de Jacobi
- Resolva o seguinte sistema de equa√ß√µes lineares usando o m√©todo de Jacobi.
$9x 1 - 2x 2 + 3x 3 + 2x 4 = 54.5$
$2x 1 + 8x 2 - 2x 3 + 3x 4 = -14$
$-3x 1 + 2x 2 + 11x 3 - 4x 4 = 12.5$
$-2x 1 + 3x 2 + 2x 3 - 10x 4 = -21$

- Computacionalmente
```{r}
A <- matrix(c(9,2,-3,-2,-2,8,2,
3,3,-2,11,2,2,3,-4,10),4,4)
b <- c(54.5, -14, 12.5, -21)
ss <- jacobi(A = A, b = b,
inicial = c(0,0,0,0),
max_iter = 15)

## Solu√ß√£o aproximada

ss$Solucao
## [1] 4.999502 -1.999771 2.500056 -1.000174
## Solu√ß√£o exata
solve(A, b)
## [1] 5.0 -2.0 2.5 -1.0
```

M√©todos iterativo de Jacobi e Gauss-Seidel
- Em R o pacote Rlinsolve fornece implementa√ß√µes eficientes dos m√©todos de Jacobi e Gauss-Seidel.
- Rlinsolve inclui suporte para matrizes esparsas via Matrix.
- Rlinsolve √© implementado em C++ usando o pacote Rcpp.
```{r}
A <- matrix(c(9,2,-3,-2,-2,8,2,3,3,-2,11,
2,2,3,-4,10),4,4)
b <- c(54.5, -14, 12.5, -21)
## pacote extra
require(Rlinsolve)
lsolve.jacobi(A, b)$x ## M√©todo de jacobi
## [,1]
## [1,] 4.9999708
## [2,] -2.0000631
## [3,] 2.5000163
## [4,] -0.9999483
lsolve.gs(A, b)$x ## M√©todo de Gauss-Seidell
## [,1]
## [1,] 4.999955
## [2,] -2.000071
## [3,] 2.500018
## [4,] -0.999968
```

### Decomposi√ß√£o LU
- Nos m√©todos de elimina√ß√£o de Gauss e Gauss-Jordan resolvemos sistemas do tipo
$$ Ax  = b .$$
- Sendo dois sistemas
$$Ax  = b_1, e \space Ax  = b_2$$
- C√°lculos do primeiro n√£o ajudam a resolver o segundo.
- IDEAL! - Opera√ß√µes realizadas em A fossem dissociadas das opera√ß√µes em $b$ .

Decomposi√ß√£o LU
- Suponha que precisamos resolver v√°rios sistemas do tipo $Ax  = b$
para diferentes $b$s.
- Op√ß√£o 1 - calcular a inversa $A_{-1}$, assim a solu√ß√£o $x  = A-1b$
- C√°lculo da inversa √© computacionalmente ineficiente.

Decomposi√ß√£o LU: algoritmo
- Decomponha (fatore) a matriz A em um produto de duas matrizes $A = LU$ onde L √© triangular inferior e U √© triangular superior.
- Baseado na decomposi√ß√£o o sistema tem a forma: $LUx  = b$ . (3)
- Defina $Ux  = y$ .
- Substituindo em 3 tem-se $Ly  = b$ . (4)
- Solu√ß√£o √© obtida em dois passos
- Resolva Eq.(4) para obter y  usando substitui√ß√£o progressiva.
- Resolva Eq.(3) para obter x  usando substitui√ß√£o regressiva.

Obtendo as matrizes L e U
- M√©todo de elimina√ß√£o de Gauss e m√©todo de Crout.
- Dentro do processo de elimina√ß√£o de Gauss as matrizes L e U s√£o obtidas como um subproduto, i.e.
**[[ARRUMAR]]**


- Os elementos m 'i j s s√£o os multiplicadores que multiplicam a equa√ß√£o piv√¥.

Obtendo as matrizes L e U
- Relembre o exemplo de elimina√ß√£o de Gauss.
**[[ARRUMAR]]**


- Neste caso, tem-se
**[[ARRUMAR]]**



Decomposi√ß√£o LU com pivota√ß√£o
- O m√©todo de elimina√ß√£o de Gauss foi realizado sem pivota√ß√£o.
- Como discutido a pivota√ß√£o pode ser necess√°ria.
- Quando realizada a pivota√ß√£o as mudan√ßas feitas devem ser armazenadas, tal que
PA = LU.
- P √© uma matriz de permuta√ß√£o.
- Se as matrizes LU forem usadas para resolver o sistema
Ax  = b ,
ent√£o a ordem das linhas de b  deve ser alterada de forma consistente com a pivota√ß√£o,
i.e. Pb .

Implementa√ß√£o: Decomposi√ß√£o LU
- Podemos facilmente modificar a fun√ß√£o gauss() para obter a decomposi√ß√£o LU.
```{r}
my_lu <- function(A) {
n_row <- nrow(A)
n_col <- ncol(A)
SOL <- matrix(NA, n_row, n_col) ## Matriz para receber os resultados
SOL[1,] <- A[1,]
pivo <- matrix(0, n_col, n_row)
for(j in 1:c(n_row-1)) {
for(i in c(j+1):c(n_row)) {
pivo[i,j] <- A[i,j]/SOL[j,j]
SOL[i,] <- A[i,] - pivo[i,j]*SOL[j,]
A[i,] <- SOL[i,]
}
}
diag(pivo) <- 1
return(list("L" = pivo, "U" = SOL)) }
```

Aplica√ß√£o: Decomposi√ß√£o LU
- Fazendo a decomposi√ß√£o.
```{r}
LU <- my_lu(A) ## Decomposi√ß√£o
LU
## $L
## [,1] [,2] [,3] [,4]
## [1,] 1.0000000 0.0000000 0.000000 0
## [2,] 0.2222222 1.0000000 0.000000 0
## [3,] -0.3333333 0.1578947 1.000000 0
## [4,] -0.2222222 0.3026316 0.279661 1
##
## $U
## [,1] [,2] [,3] [,4]
## [1,] 9 -2.000000e+00 3.000000 2.000000
## [2,] 0 8.444444e+00 -2.666667 2.555556
## [3,] 0 0.000000e+00 12.421053 -3.736842
## [4,] 0 -4.440892e-16 0.000000 10.716102
LU$L %*% LU$U ## Verificando a solu√ß√£o
## [,1] [,2] [,3] [,4]
## [1,] 9 -2 3 2
## [2,] 2 8 -2 3
## [3,] -3 2 11 -4
## [4,] -2 3 2 10

```


Aplica√ß√£o: Decomposi√ß√£o LU
- Resolvendo o sistema de equa√ß√µes.
```{r}
## Passo 1: Substitui√ß√£o progressiva
y = forwardsolve(LU$L, b)
## Passo 2: Substitui√ß√£o regressiva
x = backsolve(LU$U, y)
x
## [1] 5.0 -2.0 2.5 -1.0
A%*%x ## Verificando a solu√ß√£o
## [,1]
## [1,] 54.5
## [2,] -14.0
## [3,] 12.5
## [4,] -21.0
```

- Fun√ß√£o lu() do Matrix fornece a decomposi√ß√£o LU.
```{r}
require(Matrix)
## Calcula mas n√£o retorna
LU_M <- lu(A)
## Captura as matrizes L U e P
LU_M <- expand(LU_M)
## Substitui√ß√£o progressiva.
y <- forwardsolve(LU_M$L, LU_M$P%*%b)
## Substitui√ß√£o regressiva
x = backsolve(LU_M$U, y)
x
## [1] 5.0 -2.0 2.5 -1.0
```

## Obtendo a inversa

### Obtendo a inversa via decomposi√ß√£o LU
- O m√©todo LU √© especialmente adequado para o c√°lculo da inversa.
- Lembre-se que a inversa de A √© tal que AA-1 = I.
- O procedimento de c√°lculo da inversa √© essencialmente o mesmo da solu√ß√£o de um
sistema de equa√ß√µes lineares, por√©m com mais incognitas.
**[[ARRUMAR]]**


- Tr√™s sistemas de equa√ß√µes diferentes, em cada sistema, uma coluna da matriz X √© a incognita.

Implementa√ß√£o: inversa via decomposi√ß√£o LU
- Fun√ß√£o para resolver o sistema usando decomposi√ß√£o LU.
```{r}
solve_lu <- function(LU, b) {
  y <- forwardsolve(LU_M$L, LU_M$P%*%b)
  x = backsolve(LU_M$U, y)
  return(x)
}
```


- Resolvendo v√°rios sistemas
```{r}
my_solve <- function(LU, B) {
  n_col <- ncol(B)
  n_row <- nrow(B)
  inv <- matrix(NA, n_col, n_row)
  for(i in 1:n_col) {
    inv[,i] <- solve_lu(LU, B[,i])
  }
  return(inv)
}
```


Aplica√ß√£o: inversa via decomposi√ß√£o LU
- Calcule a inversa de
**[[ARRUMAR]]**

```{r}
A <- matrix(c(3,2,5,2,4,3,6,3,4),3,3)
I <- Diagonal(3, 1)
## Decomposi√ß√£o LU
LU <- my_lu(A)
## Obtendo a inversa
inv_A <- my_solve(LU = LU, B = I)
inv_A
## Verificando o resultado
A%*%inv_A

```


C√°lculo da inversa via m√©todo de Gauss-Jordan
- Procedimento Gauss-Jordan:
**[[ARRUMAR]]**

- Fun√ß√£o `solve()` usa a decomposi√ß√£o LU com pivota√ß√£o.
- R b√°sico √© constru√≠do sobre a biblioteca lapack escrita em C.
- Veja documenta√ß√£o em http://www.netlib.org/lapack/lug/node38.html.

Autovalores e autovetores
- Redu√ß√£o de dimensionalidade √© fundamental em ci√™ncia de dados.
- An√°lise de componentes principais (PCA)
- An√°lise fatorial (AF).
- Decompor grandes e complicados relacionamentos multivariados em simples
componentes n√£o relacionados.
- Vamos discutir apenas os aspectos matem√°ticos.

Intui√ß√£o
- Podemos decompor um vetor $\upsilon$  em duas informa√ß√µes separadas: dire√ß√£o $d$ e tamanho $\lambda $, i.e

$$\lambda  = ||\upsilon || = \sqrt \sum{}j ùúà2j  , e d = \upsilon \lambda $$
- √â mais f√°cil interpretar o tamanho de um vetor enquanto ignorando a sua dire√ß√£o e
vice-versa.
- Esta ideia pode ser estendida para matrizes.
- Uma matriz nada mais √© do que um conjunto de vetores.
- IDEIA - decompor a informa√ß√£o de uma matriz em outros componentes de mais f√°cil
interpreta√ß√£o/representa√ß√£o matem√°tica.

Autovalores e Autovetores
- Autovalores e autovetores s√£o definidos por uma simples igualdade $A\upsilon  = \lambda \upsilon $. (5)
- Os vetores $\upsilon$ ‚Äôs que satisfazem Eq. (5) s√£o os autovetores.
- Os valores $\lambda$ ‚Äôs que satisfazem Eq. (5) s√£o os autovalores.
- Vamos considerar o caso em que $A$ √© sim√©trica.
- A ideia pode ser estendida para matrizes n√£o sim√©tricas.


- Se A √© uma matriz sim√©trica $n \times n$, ent√£o existem exatamente $n$ pares ($\lambda j , \upsilon j $) que
satisfazem a equa√ß√£o:
A$\upsilon  = \lambda \upsilon$ .
- Se A tem autovalores $\lambda_1, ‚Ä¶ , \lambda_n$, ent√£o:
**[[ARRUMAR]]**

- A √© positiva definida, se e somente se todos $\lambda j  > 0$
- A √© semi-positiva definida, se e somente se todos $\lambda j  ‚â• 0$
- A ideia do PCA √© decompor/fatorar a matriz A em componentes mais simples de
interpretar.

Decomposi√ß√£o em autovalores e autovetores
- Teorema: qualquer matriz sim√©trica A pode ser fatorada em $A = Q\lambda QT$
onde $\lambda$ √© diagonal contendo os autovalores de A e as colunas de Q cont√™m os autovetores
ortonormais.
- Vetores ortonormais: s√£o mutuamente ortogonais e de comprimento unit√°rio.
- Teorema: se A tem autovetores Q e autovalores $\lambda j$ . Ent√£o A-1 tem autovetores Q e
autovalores $\lambda -1 j$  .
- Implica√ß√£o: se $A = Q\lambda QT$ ent√£o $A-1 = Q\lambda -1QT$

Diagonaliza√ß√£o
- Autovalores s√£o ut√©is porque eles permitem lidar com matrizes da mesma forma que lidamos com n√∫meros.
- Todos os c√°lculos s√£o feitos na matriz diagonal \lambda .
- Este processo √© chamado de diagonaliza√ß√£o.
- Um dos resultados mais poderosos em √Ålgebra Linear √© que qualquer matriz pode ser diagonalizada.
- O processo de diagonaliza√ß√£o √© chamado de Decomposi√ß√£o em valores singulares.

Decomposi√ß√£o em valores singulares (SVD)
- Teorema: qualquer matriz A pode ser decomposta em $A = UDVT$ onde D √© diagonal com entradas n√£o negativas e U e V s√£o ortogonais, i.e. UTU = VTV = I.
- Matrizes n√£o quadradas n√£o tem autovalores.
- Os elementos de D s√£o chamados de valores singulares.
- Os valores singulares s√£o os autovalores de ATA.

### Dimens√£o da SVD
- Se A √© n x  n, ent√£o U, D e V s√£o n x  n.
- Se A √© n x  p, sendo n > p, ent√£o U √© n x  p, D e V s√£o p x  p.
- Se A √© n x  p, sendo n < p, ent√£o VT √© n x  p, D e U s√£o n x  n.
- D ser√° sempre quadrada com dimens√£o igual ao m√≠nimo entre p e n.

### Decomposi√ß√£o em autovalores e autovetores em R
- Fun√ß√£o eigen() fornece a decomposi√ß√£o
```{r}
A <- matrix(c(1,0.8, 0.3, 0.8, 1,
0.2, 0.3, 0.2, 1),3,3)
isSymmetric.matrix(A)
## [1] TRUE
out <- eigen(A)
Q <- out$vectors ## Autovetores
D <- diag(out$values) ## Autovalores
Q
## [,1] [,2] [,3]
## [1,] -0.6712373 -0.1815663 0.71866142
## [2,] -0.6507744 -0.3198152 -0.68862977
## [3,] -0.3548708 0.9299204 -0.09651322
```

- Verificando a solu√ß√£o
```{r}
D
## [,1] [,2] [,3]
## [1,] 1.934216 0.0000000 0.0000000
## [2,] 0.000000 0.8726419 0.0000000
## [3,] 0.000000 0.0000000 0.1931419
Q%*%D%*%t(Q) ## Verificando
## [,1] [,2] [,3]
## [1,] 1.0 0.8 0.3
## [2,] 0.8 1.0 0.2
## [3,] 0.3 0.2 1.0
```


Decomposi√ß√£o em valores singulares em R
- Fun√ß√£o svd() fornece a decomposi√ß√£o
```{r}
svd(A)
## $d
## [1] 1.9342162 0.8726419 0.1931419
##
## $u
## [,1] [,2] [,3]
## [1,] -0.6712373 0.1815663 0.71866142
## [2,] -0.6507744 0.3198152 -0.68862977
## [3,] -0.3548708 -0.9299204 -0.09651322
##
## $v
## [,1] [,2] [,3]
## [1,] -0.6712373 0.1815663 0.71866142
## [2,] -0.6507744 0.3198152 -0.68862977
## [3,] -0.3548708 -0.9299204 -0.09651322
```


### Regress√£o ridge
- Relembrando: regress√£o linear m√∫ltipla
**[[ARRUMAR]]**

- Usando uma nota√ß√£o mais compacta,
**[[ARRUMAR]]**

- Minimiza a perda quadr√°tica:ÃÇ
**[[ARRUMAR]]**


### Regress√£o ridge
- Se p > n o sistema √© singular (m√∫ltiplas solu√ß√µes)!
- Como podemos ajustar o modelo?
- Introduzir uma penalidade pela complexidade.
- Soma de quadrados penalizada
**[[ARRUMAR]]**

- Matricialmente, tem-se
**[[ARRUMAR]]**

- IMPORTANTE !!
- y  centrado (m√©dia zero).
- X padronizada por coluna (m√©dia zero e vari√¢ncia um).

Regress√£o ridge
- Objetivo: minizar a soma de quadrados penalizada.
- Derivada
**[[ARRUMAR]]**


Aplica√ß√£o: regress√£o ridge
- Resolvendo o sistema linear, tem-se
**[[ARRUMAR]]**

- Solu√ß√£o depende de $\lambda$ .
- A inclus√£o de $\lambda$  faz o sistema ser n√£o singular.
- Na verdade quando fixamos $\lambda$  selecionamos uma solu√ß√£o em particular.

Aplica√ß√£o: regress√£o ridge
- Calcular $\hat{\beta}$  envolve a invers√£o de uma matriz p x  p potencialmente grande.
$$\hat{\beta}  = (XTX + \lambda I)-1 XTy$$
- Usando a decomposi√ß√£o SVD, tem-se
$$ X = UDVT$$
- √â poss√≠vel mostrar que,

$$ \hat{\beta} = Vdiag ( dj d2 j  + \lambda  ) UTy .$$

Implementa√ß√£o: regress√£o ridge
- Simulando o conjunto de dados (n = 100, p = 200).

```{r}
set.seed(123)
X <- matrix(NA, ncol = 200, nrow = 100)
X[,1] <- 1 ## Intercepto
for(i in 2:200) {
X[,i] <- rnorm(100, mean = 0, sd = 1)
X[,i] <- (X[,i] - mean(X[,i]))/var(X[,i])
}
## Par√¢metros
beta <- rbinom(200, size = 1, p = 0.1)*rnorm(200, mean = 10)
mu <- X%*%beta
## Observa√ß√µes
y <- rnorm(100, mean = mu, sd = 10)
```


Implementando o modelo.
- Modelo passo-a-passo
```{r}
y_c <- y - mean(y)
X_svd <- svd(X) ## Decomposi√ß√£o svd
lambda = 0.5 ## Penaliza√ß√£o
DD <- Diagonal(100, X_svd$d/(X_svd$d^2 + lambda))
DD[1] <- 0 ## N√£o penalizar o intercepto
beta_hat = as.numeric(X_svd$v%*%DD%*%t(X_svd$u)%*%y_c)
```

Resultados: regress√£o ridge
- Ajustados versus verdadeiros.
```{r}
plot(beta ~ beta_hat, xlab = expression(hat(beta)), ylab = expression(beta))
```

**[[ARRUMAR]]**

Resultados: regress√£o ridge
- Regress√£o com penaliza√ß√£o ridge, bem como, outras penaliza√ß√µes s√£o eficientemente
implementadas em R via pacote glmnet.

**IMPORTANTE!** A penaliza√ß√£o no glmnet √© ligeiramente diferente, por isso os $\hat{\beta}$‚Äôs n√£o
s√£o id√™nticos a nossa implementa√ß√£o naive.
- O glmnet oferece op√ß√µes para selecionar $\lambda$ via valida√ß√£o cruzada.

```{r}
require(glmnet)
beta_glm <- cv.glmnet(X[,-1], y_c, nlambda = 100)
```

Resultados: regress√£o ridge
- Valida√ß√£o cruzada.
```{r}
plot(beta_glm)
```


**[[ARRUMAR]]**


Resultados: regress√£o ridge
- Ajustados (glmnet) versus verdadeiros.
```{r}
plot(beta ~ as.numeric(coef(beta_glm)), xlab = expression(hat(beta)), ylab = expression(beta)
```
**[[ARRUMAR]]**

Coment√°rios
- Solu√ß√£o de sistemas lineares:
- M√©todos diretos: Elimina√ß√£o de Gauss e Gauss-Jordan.
- M√©todos iterativos: Jacobi e Gauss-Seidel.
- Inversa de matrizes.
- Decomposi√ß√£o ou fatoriza√ß√£o
- LU resolve sistema lineares pode ser usada para obter inversas.
- Autovalores e autovetores.
- Valores singulares.
- Existem muitas outras fatoriza√ß√µes: QR, Cholesky, Cholesky modificadas, etc.




```{r}

```

