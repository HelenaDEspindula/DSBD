---
title: "04_3-Caderno-InfEst-parte3"
author: "Helena R. S. D'Espindula"
output:
  html_document: 
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
      number_sections: true
  pdf_document:
date: "`r Sys.Date()`"
---

```{r setup, echo=TRUE, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, warning = TRUE, error = TRUE)
library(ggplot2)
library(glmnet)
library(Matrix)
library(bookdown)
```




# Álgebra Matricial

## Vetores e escalares
- Um vetor é uma lista de n números (escalares) escritos em linha ou coluna.
- Notação (primeiro a em negrito)

$$
a = (a_{i1} ... a_{in})
$$
ou

$$
a = \begin{bmatrix}
a_{i1}\\
.\\
.\\
.\\
a_{in}\\
\end{bmatrix}
$$

- Vetor linha e vetor coluna.
- Um elemento do vetor é chamado de ai , sendo i  a sua posição.
- O tamanho de um vetor é o seu número de elementos.
- O módulo de um vetor é o seu comprimento
**[[ARRUMAR]]**

- Vetor unitário é aquele que tem tamanho
$$ a = a |a|$$

- Dois vetores são iguais se tem o mesmo tamanho e os seus elementos em posições equivalentes são iguais.

### Operações com vetores

1. Soma $a + b = (ai  + b i ) = (a1 + b 1, … , an + b n)$.

$a = (1, 2, 3)$
$b = (3, 2, 1)$
$a+b = (4, 4, 4)$
$a-b = (-2, 0, 2)$

2. Subtração $a - b = (ai  - b i ) = (a1 - b 1, … , an - b n)$.

$$
a-b = (-2, 0, 2)
$$

3. Multiplicação por escalar $\alpha a = (\alpha a1, … , \alpha an)$.

$$
5 * a = (5*1, 5*2, 5*3)
$$

4. Transposta de um vetor:
**[[ARRUMAR]]**

5. Produto interno ou escalar entre dois vetores resulta em um escalar (mutiplica dois vetores e dá um número só como resultado)
$a * b = (a1b 1 + a2b 2 + … + anb n)$

- **Condições: os vetores devem ser do mesmo tipo e tamanho.**

### Vetores ortogonais
- Dois vetores são ortogonais entre si se o ângulo $\theta$ entre eles é de 90 graus.(= correlação de Pearson)
- Implicações: 
$$cos(\theta) = 0 e aTb = 0.$$
$$ cov (a,b) / raiz(variacia[a]) * raiz(variacia[b])$$

- O co-seno do ângulo $\theta$ entre os vetores é dado por:
$$cos(\theta) = aTb / \sqrt aTa\sqrt bTb .$$

### Operações com vetores em R
- Declarando vetores
```{r}
a <- c(4,5,6)
b <- c(1,2,3)
```

- Sendo a e b compatíveis
```{r}
#### Soma
a + b
## [1] 5 7 9
#### Substração
a - b
## [1] 3 3 3
```


- Multiplicação por escalar
```{r}
alpha = 10
alpha*a
## [1] 40 50 60
```
- Produto de Hadamard (não é produto interno)

```{r}
a*b
## [1] 4 10 18
```
- Produto vetorial (ou produto interno)

```{r}
a%*%b
##    [,1]
## [1,] 32
```
- Co-seno do ângulo entre dois vetores
```{r}
cos <- t(a)%*%b/(sqrt(t(a)%*%a)*sqrt(t(b)%*%b))
```
- Lei da reciclagem (não avalia se pode somar antes de somar)
```{r}
a <- c(4,5,6,5,6,7)
b <- c(1,2,3)
a + b
## [1] 5 7 9 6 8 10
```

## Matrizes

- Uma matriz é um arranjo retangular ou quadrado de números ou variáveis.
- A matriz costuma ser representada por uma letra maiuscula em negrito

- Uma matriz ($n \times m$) tem n linhas e m  colunas:

```{r latex}
# $$A = \begin{pmatrix}\
# a_{11} & a_{12} & ... & a_{1m}\\
# a_{21} & a_{22} & ... & a_{2m}\\
# ... & ... & ... & ... \\
# a_{n1} & a_{11} & ... & a_{nm}\\
# \end{pmatrix}$$
```


$$A = \begin{pmatrix}\
a_{11} & a_{12} & ... & a_{1m}\\
a_{21} & a_{22} & ... & a_{2m}\\
... & ... & ... & ... \\
a_{n1} & a_{11} & ... & a_{nm}\\
\end{pmatrix}$$

- O primeiro subscrito representa linha e o segundo representa coluna.
- A dimensão de uma matriz é o seu número de linhas e colunas.
- Duas matrizes são iguais se tem a mesma dimensão e se os elementos das correspondentes
posições são iguais.

### Matriz transposta
- A operação de transposição rearranja uma matriz de forma que suas linhas são transformadas em colunas e vice-versa.
**[[ARRUMAR]]**

- Note que (AT)T = A.
- Computacionalmente
- Declarando matrizes

```{r}
a <- c(1,2,3,4,5,6)
A <- matrix(a, nrow = 3, ncol = 2)
A
## [,1] [,2]
## [1,] 1 4
## [2,] 2 5
## [3,] 3 6
```

- O default preenche por colunas
- Transposta de uma matriz
```{r}
t(A)
## [,1] [,2] [,3]
## [1,] 1 2 3
## [2,] 4 5 6
```

### Operações com matrizes

- Multiplicação matriz por escalar.
$$\alpha * A = \begin{pmatrix}\
\alpha * a_{11} & \alpha * a_{12} & \alpha * ... & \alpha * a_{1m}\\
\alpha * a_{21} & \alpha * a_{22} & \alpha * ... & \alpha * a_{2m}\\
\alpha * ... & \alpha * ... & \alpha * ... & \alpha * ... \\
\alpha * a_{n1} & \alpha * a_{n2} & \alpha * ... & \alpha * a_{nm}\\
\end{pmatrix}$$

- Computacionalmente
```{r}
A <- matrix(c(1,2,3,4,5,6),
nrow = 3, ncol = 2)
alpha <- 10
alpha*A
## [,1] [,2]
## [1,] 10 40
## [2,] 20 50

```

- Duas matrizes podem ser somadas ou
subtraídas somente se tiverem o mesmo
tamanho.
1. Soma 
**[[ARRUMAR]]**
2. Subtração 
**[[ARRUMAR]]**

- Exemplo
$$A = \begin{pmatrix}\
1 & 2\\
3 & 4\\
5 & 6\\
\end{pmatrix}$$

$$B = \begin{pmatrix}\
10 & 20\\
30 & 40\\
50 & 60\\
\end{pmatrix}$$

$$A + B = \begin{pmatrix}\
11 & 22\\
33 & 44\\
55 & 66\\
\end{pmatrix}$$

- Soma de duas matrizes
```{r}
A <- matrix(c(1,2,3,4,5,6),
nrow = 3, ncol = 2)
B <- matrix(c(10,20,30,40,50,60),
nrow = 3, ncol = 2)
C = A + B
C
## [,1] [,2]
## [1,] 11 44
## [2,] 22 55
## [3,] 33 66
```

- Condição para multiplicar matrizes
$$
C_{m, n} = A_{m,q} B_{q,n}
$$
(q tem que ser igual)

**[[ARRUMAR]]**

- Computacionalmente.
- Matrizes compatíveis
```{r}
A <- matrix(c(2,8,6,-1,3,7),
nrow = 3, ncol = 2)
B <- matrix(c(4,-5,9,2,1,4,-3,6),
nrow = 2, ncol = 4)
C = A%*%B
C
## [,1] [,2] [,3] [,4]
## [1,] 13 16 -2 -12
## [2,] 17 78 20 -6
## [3,] -11 68 34 24
```

- Matrizes não compatíveis
```{r message=TRUE, warning=TRUE}
B %*% A
## Error in B %*% A: argumentos não compatíveis
```


Produto de Hadamard
- Produto simples ou de Hadamard

$$A \odot B = \begin{pmatrix}\
a_{11}*b_{11} & a_{12}*b_{12} & ... & a_{1m}*b_{1m}\\
a_{21}*b_{21} & a_{22}*b_{22} & ... & a_{2m}*b_{2m}\\
... & ... & ... & ... \\
a_{n1}*b_{n1} & a_{n2}*b_{n2} & ... & a_{nm}*b_{nm}\\
\end{pmatrix}$$


- Computacionalmente
```{r}
A <- matrix(c(1,2,3,4),
nrow = 2, ncol = 2)
B <- matrix(c(10,20,30,40),
nrow = 2, ncol = 2)
A*B
## [,1] [,2]
## [1,] 10 90
## [2,] 40 160
```


Propriedades envolvendo operações com matrizes
- Sendo A, B, C e D compatíveis temos,
1. $A + B = B + A$
2. $(A + B) + C = A + (B + C)$.
3. $\alpha (A + B) = \alpha A + \alpha B$.
4. $(\alpha  + \beta )A = \alpha A + \beta A$.
5. $\alpha (AB) = (\alpha A)B = A(\alpha B)$.
6. $A(B ± C) = AB ± AC$.
7. $(A ± B)C = AC ± BC$.
8. $(A-B)(C-D) = AC-BC-AD+BD$.

- Propriedades envolvendo transposta e
multiplicação
1. Se $A$ é $n \times m$ e $B$ é $m \times n$, então $(AB)^T = B^T A^T$.
2. Se $A$, $B$ e $C$ são compatíveis 
$$
(ABC)^{T}= C^{T}B^{T}A^{T}.
$$

### Matrizes de formas especiais
- Matriz quadrada ($m = n$)

Exemplo 4x4
```{r}
A <- matrix(c("a11","a21","a31","a41","a12","a22","a32","a42","a13","a23","a33","a43","a14","a24","a34","a44"), nrow = 4, ncol = 4)
A
```

- ai i  são os elementos da diagonal.
- ai j  para i  (diferente) j  → fora da diagonal.
- ai j  para j  > i  → acima da diagonal.
- ai j  para i  > j  → abaixo da diagonal.
- Matriz diagonal
$$D = \begin{pmatrix}\
a_{11} & 0 & 0 & 0\\
0 & a_{22} & 0 & 0\\
0 & 0 & a_{33} & 0\\
0 & 0 & 0 & a_{44}\\
\end{pmatrix}$$



- Matriz identidade
$$I = \begin{pmatrix}\
1 & 0 & 0 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1\\
\end{pmatrix}$$



### Matrizes de formas especiais
- Triangular superior
$$U = \begin{pmatrix}\
a_{11} & a_{12} & a_{13} & a_{14}\\
0 & a_{22} & a_{23} & a_{24}\\
0 & 0 & a_{33} & a_{34} \\
0 & 0 & 0 & a_{44}\\
\end{pmatrix}$$

- Triangular inferior
$$L = \begin{pmatrix}\
a_{11} & 0 & 0 & 0\\
a_{21} & a_{22} & 0 & 0\\
a_{31} & a_{32} & a_{33} & 0 \\
a_{41} & a_{42} & a_{43} & a_{44}\\
\end{pmatrix}$$

```{r}
# $$L = \begin{pmatrix}\
# a_{11} & a_{12} & a_{13} & a_{14}\\
# a_{22} & a_{22} & a_{23} & a_{24}\\
# a_{22} & a_{22} & a_{33} & a_{34} \\
# a_{22} & a_{22} & a_{22} & a_{44}\\
# \end{pmatrix}$$
```



- Matriz nula
$$0 = \begin{pmatrix}\
0 & 0 & 0 & 0\\
0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0\\
\end{pmatrix}$$


- Matriz quadrada simétrica
$$0 = \begin{pmatrix}\
1 & 0,8 & 0,6 & 0,4\\
0,8 & 1 & 0,2 & 0,4\\
0,6 & 0,2 & 1 & 0,1 \\
0,4 & 0,4 & 0,1 & 1,0\\
\end{pmatrix}$$


### Combinações lineares
- Um conjunto de vetores a1, a2, … , an é dito ser linearmente dependente se puderem ser
encontrados escalares c 1, c 2, … , c n e estes escalares não sejam todos iguais a 0 de tal forma
que
$$
c 1a1 + c 2a2 + … + c nan = 0.
$$

Exemplo:
```{r}
a1 <- matrix(c(1,0), nrow = 2, ncol = 1)
a1
a2 <- matrix(c(0,1), nrow = 2, ncol = 1)
a2

#O unico caso que esses c1*a1 + c2*a2 = (0, 0) é se c1 = 0 E c2 =0
#Ou seja Linearmente independente

a1 <- matrix(c(1,2), nrow = 2, ncol = 1)
a1
a2 <- matrix(c(-1,-2), nrow = 2, ncol = 1)
a2

#Existem casos fora os cs = 0 que fazem c1*a1 + c2*a2 = (0, 0)
#Ou seja Linearmente dependente

```


- Caso contrário é dito ser linearmente independente.
- Notação matricial
$$
Ac = 0.
$$
- As colunas de A são linearmente independentes se Ac = 0 implicar que c = 0.

### Rank ou posto de uma matriz
- O rank ou posto de qualquer matriz quadrada ou retangular A é definido como
rank(A) = número de colunas ou linhas linearmente independentes em A.
- Sendo A uma matriz retangular n x  m  o maior rank possível para A é o min(n,m ).
- O rank da matrix nula é 0.
- Se o rank da matriz é o min(n,m ) dizemos que a matriz tem rank completo.

### Matriz não singular e matriz inversa
- Uma matriz quadrada de posto completo é chamada de não singular.
- Sendo A quadrada de posto completo a matriz inversa de A é única tal que (só se a matriz for quadrada e de ranking completo)
$$
AA^{-1} = I.
$$
- Não quadrada (posto incompleto) → não terá inversa e é dita ser singular.
- Note que 
$$
A^{(-1^{-1})} =A
$$
A^{-1}^{-1} = A

## Matriz inversa
- Computacionalmente
```{r}
A <- matrix(c(4, 2, 7, 6), 2, 2)
A

A_inv <- solve(A)
A_inv

I = A %*% A_inv
I

```

- Verificando
```{r}
A%*%A_inv
## [,1] [,2]
## [1,] 1 0
## [2,] 0 1
```


- Propriedades envolvendo inversas
1. Se A é não singular, então AT é não singular e sua inversa é dada por $(AT)-1 = (A-1)T$
2. Se A e B são matrizes não singulares de mesmo tamanho, então o produto AB é
não singular e $(AB)-1 = B-1 A-1$

### Inversa generalizada
- A inversa generalizada de uma matriz A n x  p é qualquer matriz A- que satisfaça 
$$
AA^{-}A = A.
$$

- Não é única exceto quando A é não-singular (inversa usual).
- Exemplo

$$

$$
**[[ARRUMAR]]**


- a- = (1, 0, 0, 0)

- Verificando

```{r}
a <- matrix(c(1, 2, 3, 4), 4, 1)
a_invg <- matrix(c(1,0,0,0), 1, 4)
a%*%a_invg%*%a
## [,1]
## [1,] 1
## [2,] 2
## [3,] 3
## [4,] 4
```

- Moore-Penrose generalized inverse
```{r}
#### Matriz singular (col 3 = col 2 + col 1)
A <- matrix(c(2, 1, 3, 2, 0,
2, 3, 1, 4), 3, 3)
library(MASS)
A_ginv <- ginv(A)
A%*%A_ginv%*%A ## Verificando
```

## Matrizes positivas definidas

### Formas quadráticas
- Soma de quadrados são importantes em ciência de dados.
- Considere uma matriz A simétrica e y um vetor, o produto
$$
y^{T}Ay = 
\sum(a_{ij}y^{2}_{i}) + 
\sum_{i (diferente) j}(a_{ij}y_{i}y_{j})
$$
é chamado de forma quadrática.

$$
y^{T}Iy = \sum^{n}_{i=0}(y^{2}_{i})
$$


- Sendo y de dimensão n x  1, 
$$
yTIy = y 2
1 + y 2
2 + … , y 2
n
$$

- Consequentemente, yTy é a soma de quadrados dos elementos do vetor y.
- A raiz quadrada da soma de quadrados é o comprimento de y.

Matriz positiva definida
- Sendo A uma matriz simétrica com a propriedade yTAy > 0 para todos os possíveis y
exceto para quando y = 0, então a forma quadrática yTAy é chamada positiva definida,
e A é dita ser uma matriz positiva definida.
- Exemplo
**[[ARRUMAR]]**

A forma quadrática associada é dada por (ver abaixo) que é claramente positiva, desde que y 1 e y 2 sejam diferentes de zero.
$$
yTAy = (y 1 y 2) ( 2 -1
-1 3 ) (y 1
y 2
) = 2y 2
1 - 2y 1y 2 + 3y 2
2 ,
$$

### Propriedades de matrizes positivas definidas
1. Se A é positiva definida, então todos os valores da diagonal de A são positivos.
2. Se A é positiva semi-definida, então os elementos da diagonal de A são maiores ou iguais a zero.
3. Sendo P uma matriz não-singular e A uma matriz positiva definida, o produto PTAP é positiva definida.
4. Sendo P uma matriz não-singular e A uma matriz positiva semi-definida, o produto PTAP é positiva semi-definida.
5. Uma matriz positiva definida é não-singular.

### Determinante de uma matriz
- O determinante de uma matriz A é o escalar (= numero)
$$
|A| = \sum((-1)^k a_{1j_{1}} a_{2j_{2}} ... a_{nj_{n}})
$$
onde a soma é realizada para todas as n! permutações de grau n, e k  é o número de
mudanças necessárias para que os segundos subscritos sejam colocados na ordem
$1,2, … , n$

- Considere a matriz
**[[ARRUMAR]]**

Determinante de uma matriz
- Computacionalmente.
```{r}
A <- matrix(c(3,-2,-2,4),2,2)
determinant(A, logarithm = FALSE)$modulus
## [1] 8
## attr(,"logarithm")
## [1] FALSE
```

- Determinante em escala log.
```{r}
determinant(A, logarithm = TRUE)$modulus
## [1] 2.079442
## attr(,"logarithm")
## [1] TRUE
```

- Alguns aspectos interessantes sobre determinantes são:
1. Se A é singular, |A| = 0.
2. Se A é não singular, |A| (diferente) 0.
3. Se A é positiva definida, |A| > 0.
4. |AT| = |A|.
5. Se A é não singular, |A-1| = 1
|A| .

Traço de uma matriz
- O traço de uma matriz A n x  n é um escalar definido como a soma dos elementos da diagonal, 
**[[ARRUMAR]]**

- Propriedades
1. Se A e B são n x  n, então tr(A + B) = tr(A) + tr(B).
2. Se A é n x  p e B e p x  n, então tr(AB) = tr(BA).

- Computacionalmente
```{r}
A <- matrix(c(3,-2,-2,4),2,2)
sum(diag(A))
## [1] 7
```

## Cálculo vetorial e matricial

### Cálculo vetorial
- Seja $y = f(x)$ uma função das variáveis $x_{1}, x_{2}, x_{3}, ... , x_{p}$ e $\partial y$  as respectivas derivadas parciais.
**[[ARRUMAR]]**

Assim,
**[[ARRUMAR]]**

### Cálculo vetorial
- Sendo aT = (a1, a2, … , ap) um vetor de constantes e A uma matriz simétrica de constantes.
1. Seja y  = aTx = xTa. Então,
**[[ARRUMAR]]**

2. Seja y  = xTAx. Então,
**[[ARRUMAR]]**

### Cálculo Matricial
- Se y  = f (X) onde X é uma matriz p x  p. As derivadas parciais de y  em relação a cada x i j 
são organizadas em uma matriz.
**[[ARRUMAR]]**


- Algumas derivadas importantes envolvendo matrizes são apresentadas abaixo.
1. Seja y  = tr(XA) sendo X p x  p e definida positiva e A p x  p constantes. Então,
**[[ARRUMAR]]**

2. Sendo A não singular com derivadas $\partial A$
**[[ARRUMAR]]**

3. Sendo A n x  n positiva definida. Então,
**[[ARRUMAR]]**

## Regressão linear múltipla

### Regressão linear múltipla: especificação usual

- Regressão linear simples
$$
y_{i} = \beta_{0} +\beta_{1}x_{1} + erro_{i}
$$
- Regressão linear múltipla
$$
y_{i} = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2} + ... + \beta_{p}x_{ip} + erro_{i}
$$
- Modelo para cada observação
$$y_{1} = \beta_{0} + \beta_{1}x_{11} + \beta_{2}x_{12} + ... + \beta_{p}x_{1p} + erro_{1}$$

$$y_{2} = \beta_{0} + \beta_{1}x_{21} + \beta_{2}x_{22} + ... + \beta_{p}x_{2p} + erro_{1}$$
$$...$$
$$y_{n} = \beta_{0} + \beta_{1}x_{n1} + \beta_{2}x_{n2} + ... + \beta_{p}x_{np} + erro_{n}$$

Regressão linear múltipla: especificação matricial
- Notação matricial
$$
\begin{bmatrix}
y_{1}\\
y_{2}\\
...\\
y_{n}\\
\end{bmatrix}
= 
\begin{bmatrix}
1 & x_{1}\\
1 & x_{2}\\
1 & ...\\
1 & x_{n}\\
\end{bmatrix}
x 
\begin{bmatrix}
\beta_{1}\\
\beta_{2}\\
...\\
\beta_{n}\\
\end{bmatrix}
+
\begin{bmatrix}
erro_{1}\\
erro_{2}\\
...\\
erro_{n}\\
\end{bmatrix}
$$

- Notação mais compacta
$$
y_{(n x  1)} = X_{(n x  p)} \beta_{(p x  1)} + erro_{(n x  1)}
$$
### Regressão linear múltipla: estimação (treinamento)
- Objetivo: encontrar o vetor $\hat{\beta}$ , tal que $S Q (\beta ) = (y - X\beta )T(y - X\beta )$ seja a menor possível.

### Regressão linear múltipla: estimação
1. Passo 1: encontrar o vetor gradiente. Derivando em $\beta$ , temos

**[[ARRUMAR]]**

### Regressão linear múltipla: estimação

2. Passo 2: resolver o sistema de equações lineares (esquece o "-2" primeiro)
$$ X^{T} (y - X\hat{\beta}) = 0$$
$$XTy - XTX̂\beta  = 0$$
$$XTX̂\beta  = XTŷ$$
$$(XTX)^{-1}  XTX̂\beta  = XTy (XTX)^{-1}$$
$$ I\beta  = (XTX)-1XTy $$

### Regressão linear múltipla: exemplo
- Conjunto de dados Boston disponível no pacote MASS.
- Cinco primeiras covariáveis disponíveis:
  - crim: taxa de crimes per capita.
  - zn: proporção de terrenos residenciais zoneados para lotes com mais de 25.000 pés quadrados.
  - indus: proporção de acres de negócios não varejistas por cidade.
  - chas: variável dummy de Charles River (1 se a área limita o rio; 0 caso contrário).
  - nox: concentração de óxido de nitrogênio (parte por 10 milhões).
- Variável resposta: medv valor mediano das casas ocupadas em $1000.

### Regressão linear múltipla: implementação computacional
- Carregando a base de dados 
```{r}
require(MASS)
## Carregando pacotes exigidos: MASS

data(Boston)
head(Boston[, c(1:5,14)])
## crim zn indus chas nox medv
## 1 0.00632 18 2.31 0 0.538 24.0
## 2 0.02731 0 7.07 0 0.469 21.6
## 3 0.02729 0 7.07 0 0.469 34.7
## 4 0.03237 0 2.18 0 0.458 33.4
## 5 0.06905 0 2.18 0 0.458 36.2
## 6 0.02985 0 2.18 0 0.458 28.7
```


- Matriz de delineamento (X).
```{r}
X <- model.matrix(~ crim + zn + indus +
chas + nox, data = Boston)
head(X)
## (Intercept) crim zn indus chas nox
## 1 1 0.00632 18 2.31 0 0.538
## 2 1 0.02731 0 7.07 0 0.469
## 3 1 0.02729 0 7.07 0 0.469
## 4 1 0.03237 0 2.18 0 0.458
## 5 1 0.06905 0 2.18 0 0.458
## 6 1 0.02985 0 2.18 0 0.458
```


- Variável resposta
```{r}
y <- Boston$medv
```

- Estimadores de mínimos quadrados:
$$
\hat{\beta} = (X^{T}X)^{-1} X^{T}y
$$
- Computacionalmente: versão ingênua (calcula inversa)
```{r}
round(solve(t(X)%*%X)%*%t(X)%*%y, 2)
## [,1]
## (Intercept) 29.49
## crim -0.22
## zn 0.06
## indus -0.38
## chas 7.03
## nox -5.42
```

- Computacionalmente: versão eficiente (escalona?)
```{r}
round(solve(t(X)%*%X, t(X)%*%y), 2)
## [,1]
## (Intercept) 29.49
## crim -0.22
## zn 0.06
## indus -0.38
## chas 7.03
## nox -5.42
```

- Função nativa do R
```{r}
t(round(coef(lm(medv ~ crim + zn + indus + chas + nox, data = Boston)), 2))
## (Intercept) crim zn indus chas nox
## [1,] 29.49 -0.22 0.06 -0.38 7.03 -5.42
```

### Matrizes esparsas (tópico adicional)

- Matrizes aparecem em todos os tipos de aplicação em ciência de dados.
- Modelos estatísticos, machine learning, análise de texto, análise de cluster, etc.
- Muitas vezes as matrizes usadas têm uma grande quantidade de zeros.
- Quando uma matriz tem uma quantidade considerável de zeros, dizemos que ela é
esparsa, caso contrário dizemos que a matriz é densa.
- Todas as propriedades que vimos para matrizes em geral valem para matrizes esparsas.
- O R tem um conjunto de métodos altamente eficiente por meio do pacote Matrix.
- Saber que uma matriz é esparsa é útil pois permite:
- Planejar formas de armazenar a matriz em memória.
- Economizar cálculos em algoritmos numéricos (multiplicação, inversa, determinante,
decomposições, etc).

- Comparando a quantidade de memória utilizada.
```{r}
library('Matrix')

m1 <- matrix(0, nrow = 1000, ncol = 1000)
object.size(m1)
## 8000216 bytes

m2 <- Matrix(0, nrow = 1000, ncol = 1000, sparse = TRUE)
object.size(m2)
## 9240 bytes
```


Comparando o tempo computacional


- Matriz densa
```{r}
y <- rnorm(1000)
X <- matrix(NA, ncol = 100, nrow = 1000)
for(i in 1:1000) {X[i,] <- rbinom(100, size = 1, p = 0.1)}
system.time(replicate(100, solve(t(X)%*%X, t(X)%*%y)))
## usuário sistema decorrido
## 0.819 0.004 0.823
```


- Matriz esparsa
```{r}
y <- rnorm(1000)
X <- Matrix(NA, ncol = 100, nrow = 1000)
for(i in 1:1000) {X[i,] <- rbinom(100, size = 1, p = 0.1)}
X <- Matrix(X, sparse = TRUE)
system.time(replicate(100, solve(t(X)%*%X, t(X)%*%y)))
## usuário sistema decorrido
## 0.223 0.000 0.224
```

### Diferentes formas de implementar as operações matriciais

- Criando a base de dados para a comparação
```{r}
library(Matrix)
n <- 10000; p <- 500

#DENSA
x <- matrix(rbinom(n*p, 1, 0.01), nrow=n, ncol=p)
object.size(x)
## 20000216 bytes

#ESPARCA
X <- Matrix(x)
object.size(X)
## 600432 bytes
```

- Diferentes implementações
```{r}
y <- rnorm(n)

print("Matriz densa com %*%:")
system.time(solve(t(x)%*%x, t(x)%*%y))
## usuário sistema decorrido
## 2.053 0.040 2.094

print("Matriz densa com crossprod")
system.time(solve(crossprod(x), crossprod(x, y)))
## usuário sistema decorrido
## 1.731 0.016 1.748

print("Matriz esparça com %*%")
system.time(solve(t(X)%*%X, t(X)%*%y))
## usuário sistema decorrido
## 0.071 0.000 0.072

print("Matriz esparça com crossprod")
system.time(solve(crossprod(X), crossprod(X,y)))
## usuário sistema decorrido
## 0.029 0.000 0.050
```

- Implementação eficiente do modelo de regressão linear múltipla.
```{r}
library(glmnet)
## Loaded glmnet 4.1-6
system.time(b <- coef(lm(y~x)))
## usuário sistema decorrido
## 2.389 0.044 2.434
system.time(g1 <-glmnet(x, y, nlambda=1, lambda=0, standardize=FALSE))
## usuário sistema decorrido
## 0.065 0.020 0.086
system.time(g2 <- glmnet(X, y, nlambda=1, lambda=0, standardize=FALSE))
## usuário sistema decorrido
## 0.006 0.000 0.006
```
