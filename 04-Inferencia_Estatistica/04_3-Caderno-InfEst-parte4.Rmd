---
title: "04_3-Caderno-InfEst-parte4"
author: "Helena R. S. D'Espindula"
output:
  html_document: 
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
      number_sections: true
  pdf_document:
date: "`r Sys.Date()`"
---

```{r setup, echo=TRUE, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, warning = TRUE, error = TRUE)
library(ggplot2)
library(glmnet)
library(Matrix)
library(bookdown)
```

# 19.04.2024

### Sistemas lineares
- Sistema com duas equa√ß√µes:
$$ f 1(x 1,x 2) = 0$$
$$f 2(x 1,x 2) = 0$$
- Solu√ß√£o num√©rica consiste em encontrar $\hat{x}_1$ e $\hat{x}_2$ que satisfa√ßa o sistema de equa√ß√µes.

- Sistema com n equa√ß√µes
$$f 1(x 1, ‚Ä¶ , x n) = 0$$
‚ãÆ
$$f n(x 1, ‚Ä¶ , x n) = 0$$
- Genericamente, tem-se
$$f(x ) = 0$$

- Equa√ß√µes podem ser lineares ou n√£o-lineares.

Sistemas de equa√ß√µes lineares
- Cada equa√ß√£o √© linear na inc√≥gnita.
- Solu√ß√£o anal√≠tica em geral √© poss√≠vel.
- Exemplo:
$$7x 1 + 3x 2 = 45$$
$$4x 1 + 5x 2 = 29$$
- Solu√ß√£o anal√≠tica:$\hat{x_1} = 6 e\hat{x_2} = 1$
- Resolver (tedioso!!).

- Tr√™s poss√≠veis casos:
1. Uma √∫nica solu√ß√£o (sistema n√£o singular).
2. Infinitas solu√ß√µes (sistema singular).
3. Nenhuma solu√ß√£o (sistema imposs√≠vel).

#### Sistemas de equa√ß√µes lineares
- Representa√ß√£o matricial do sistema de equa√ß√µes lineares:
**[[ARRUMAR]]**

- De forma geral, tem-se
$$Ax = b$$
ou 
$$Ax -b = 0$$

#### Opera√ß√µes com linhas
- Sem qualquer altera√ß√£o na rela√ß√£o linear, √© poss√≠vel

1. Trocar a posi√ß√£o de linhas:
$$4x 1 + 5x 2 = 29$$
$$7x 1 + 3x 2 = 45$$

2. Multiplicar qualquer linha por uma constante, aqui 4x 1 + 5x 2 por 1x4 , obtendo
**[[ARRUMAR]]**


3. Subtrair um m√∫ltiplo de uma linha de uma outra, aqui 7 * ùê∏ùëû.(1) menos Eq. (2), obtendo
**[[ARRUMAR]]**
- Fazendo as contas, tem-se
**[[ARRUMAR]]**

Solu√ß√£o de sistemas lineares
- Forma geral de um sistema com n equa√ß√µes lineares:
**[[ARRUMAR]]**

- Matricialmente, tem-se
**[[ARRUMAR]]**

- M√©todos diretos e m√©todos iterativos.

### M√©todos diretos

- O sistema de equa√ß√µes √© manipulado at√© se transformar em um sistema equivalente de
f√°cil resolu√ß√£o.
- Triangular superior:
**[[ARRUMAR]]**

resolve x4, com ele resolve o x3 e por ai vai... (vai subindo)

- Substitui√ß√£o regressiva
**[[ARRUMAR]]**

M√©todos diretos
- Triangular inferior:
**[[ARRUMAR]]**

- Substitui√ß√£o progressiva
**[[ARRUMAR]]**



M√©todos diretos
- Diagonal: (mais caro em numero de opera√ß√µes, portanto pouco usado)
**[[ARRUMAR]]**


### Elimina√ß√£o de Gauss

#### M√©todos diretos: Elimina√ß√£o de Gauss
- M√©todo de Elimina√ß√£o de Gauss consiste em manipular o sistema original usando opera√ß√µes de linha at√© obter um sistema triangular superior.
**[[ARRUMAR]]**

- Usar elimina√ß√£o regressiva no novo sistema para obter a solu√ß√£o.
- Resolva o seguinte sistema usando Elimina√ß√£o de Gauss.
$$\begin{bmatrix}
3 & 2 & 6\\
2 & 4 & 3\\
5 & 3 & 4\\
\end{bmatrix} *  
\begin{bmatrix}
x_1\\
x_2\\
x_3\\
\end{bmatrix} = 
\begin{bmatrix}
24\\
23\\
33\\
\end{bmatrix}$$


M√©todos diretos: Elimina√ß√£o de Gauss
- Passo 1: encontrar o piv√¥ e eliminar os elementos abaixo dele usando opera√ß√µes de linha.
(pivo √© o cara que eu quero que odos abaixo dele sejam zero)
$$\begin{bmatrix}
[3] & 2 & 6\\
2-\frac{2}{3}3 & 4-\frac{5}{3}2 & 3 -\frac{2}{3}6\\
5-\frac{5}{3}3 & 3-\frac{5}{3}2 & 4-\frac{5}{3}6\\
\end{bmatrix} *  
\begin{bmatrix}
x_1\\
x_2\\
x_3\\
\end{bmatrix} = 
\begin{bmatrix}
24\\
23-\frac{2}{3}24\\
33-\frac{5}{3}24\\
\end{bmatrix}$$

$$\begin{bmatrix}
[3] & 2 & 6\\
0 & \frac{8}{3} & -1\\
0 & -\frac{1}{3} & -6\\
\end{bmatrix} *  
\begin{bmatrix}
x_1\\
x_2\\
x_3\\
\end{bmatrix} = 
\begin{bmatrix}
24\\
7\\
-7\\
\end{bmatrix}$$


- Passo 2: encontrar o segundo piv√¥ e eliminar os elementos abaixo dele usando opera√ß√µes de linha.

$$\begin{bmatrix}
3 & 2 & 6\\
0 & [\frac{8}{3}] & -1\\
0 & -\frac{1}{3} -(-\frac{3}{24})(\frac{8}{3}) & -6-(-\frac{3}{24})(-1)\\
\end{bmatrix} *  
\begin{bmatrix}
x_1\\
x_2\\
x_3\\
\end{bmatrix} = 
\begin{bmatrix}
24\\
7\\
-7-(-\frac{3}{24})(7)\\
\end{bmatrix}$$

$$\begin{bmatrix}
[3] & 2 & 6\\
0 & [\frac{8}{3}] & -1\\
0 & 0 & -\frac{147}{24}\\
\end{bmatrix} *  
\begin{bmatrix}
x_1\\
x_2\\
x_3\\
\end{bmatrix} = 
\begin{bmatrix}
24\\
7\\
-\frac{147}{24}\\
\end{bmatrix}$$

- Passo 3: substitui√ß√£o regressiva.

M√©todos diretos: Elimina√ß√£o de Gauss
- Usando a f√≥rmula de substitui√ß√£o regressiva temos:

$$x_3 = \frac{b_3}{a_{33}} = 1$$

$$x_2 = \frac{b_2 - a_{23}x_3}{a_{22}} = 3$$

$$x_1 = \frac{(b_1 - a_{12}x_2 + a_{13}x_3)}{a_{11}} = 4$$

- A extens√£o do procedimento para um sistema com n equa√ß√µes √© trivial.

1. Transforme o sistema em triangular superior usando opera√ß√µes linhas.

2. Resolva o novo sistema usando substitui√ß√£o regressiva.
- Potenciais problemas do m√©todo de elimina√ß√£o de Gauss:
- O elemento piv√¥ √© zero (muda ordem das linhas)
- O elemento piv√¥ √© pequeno em rela√ß√£o aos demais termos.

#### Elimina√ß√£o de Gauss com pivota√ß√£o

- Considere o sistema
$$0x 1 + 2x 2 + 3x 2 = 46$$
$$4x 1 - 3x 2 + 2x 3 = 16$$
$$2x 1 + 4x 2 - 3x 3 = 12$$
- Neste caso o piv√¥ √© zero e o procedimento n√£o pode come√ßar.
- Pivota√ß√£o - trocar a ordem das linhas.
1. Evitar piv√¥s zero.
2. Diminuir o n√∫mero de opera√ß√µes necess√°rias para triangular o sistema.
$$4x 1 - 3x 2 + 2x 3 = 16$$
$$2x 1 + 4x 2 - 3x 3 = 12$$
$$0x 1 + 2x 2 + 3x 2 = 46$$

Elimina√ß√£o de Gauss com pivota√ß√£o
- Se durante o procedimento uma equa√ß√£o piv√¥ tiver um elemento nulo e o sistema tiver
solu√ß√£o, uma equa√ß√£o com um elemento piv√¥ diferente de zero sempre existir√°.
- C√°lculos num√©ricos s√£o menos propensos a erros e apresentam menores erros de
arredondamento se o elemento piv√¥ for grande em valor absoluto.
- √â usual ordenar as linhas para que o maior valor seja o primeiro piv√¥.

Passo 1: obtendo uma matriz triangular superior.
```{r}
gauss <- function(A, b) {
Ae <- cbind(A, b) ## Sistema aumentado
rownames(Ae) <- paste0("x", 1:length(b))
n_row <- nrow(Ae)
n_col <- ncol(Ae)
SOL <- matrix(NA, n_row, n_col) ## Matriz para receber os resultados
SOL[1,] <- Ae[1,]
pivo <- matrix(0, n_col, n_row)
for(j in 1:c(n_row-1)) {
for(i in c(j+1):c(n_row)) {
pivo[i,j] <- Ae[i,j]/SOL[j,j]
SOL[i,] <- Ae[i,] - pivo[i,j]*SOL[j,]
Ae[i,] <- SOL[i,]
}
}
return(SOL)
}
```



Elimina√ß√£o de Gauss sem pivota√ß√£o
- Passo 2: substitui√ß√£o regressiva
```{r}
sub_reg <- function(SOL) {
n_row <- nrow(SOL)
n_col <- ncol(SOL)
A <- SOL[1:n_row,1:n_row]
b <- SOL[,n_col]
n <- length(b)
x <- c()
x[n] <- b[n]/A[n,n]
for(i in (n-1):1) {
x[i] <- (b[i] - sum(A[i,c(i+1):n]*x[c(i+1):n] ))/A[i,i]
}
return(x)
}
```

Elimina√ß√£o de Gauss sem pivota√ß√£o
- Resolva o sistema:
**[[ARRUMAR]]**

```{r}
A <- matrix(c(3,2,5,2,4,3,6,3,4),3,3)
b <- c(24,23,33)
S <- gauss(A, b) ## Passo 1: Triangulariza√ß√£o
sol = sub_reg(SOL = S) ## Passo 2: Substitui√ß√£o regressiva
sol
## [1] 4 3 1
A%*%sol ## Verificando a solu√ß√£o
## [,1]
## [1,] 24
## [2,] 23
## [3,] 33

```


Elimina√ß√£o de Gauss com pivota√ß√£o
- Resolva o seguinte sistema usando Elimina√ß√£o de Gauss com pivota√ß√£o.
$$0x 1 + 2x 2 + 3x 2 = 46$$
$$4x 1 - 3x 2 + 2x 3 = 16$$
$$2x 1 + 4x 2 - 3x 3 = 12$$

```{r}
## Entrando com o sistema original
A <- matrix(c(0,4,2,2,-3,4,3,2,-3), 3,3)
b <- c(46,16,12)
## Pivoteamento
A_order <- A[order(A[,1], decreasing = TRUE),]
b_order <- b[order(A[,1], decreasing = TRUE)]
#### Triangula√ß√£o
S <- gauss(A_order, b_order)
S
## [,1] [,2] [,3] [,4]
## [1,] 4 -3.0 2.000000 16.00000
## [2,] 0 5.5 -4.000000 4.00000
## [3,] 0 0.0 4.454545 44.54545
#### Substitui√ß√£o regressiva
sol <- sub_reg(SOL = S)
sol
## [1] 5 8 10
#### Solu√ß√£o
A_order%*%sol
## [,1]
## [1,] 16
## [2,] 12
## [3,] 46
```


#### Elimina√ß√£o de Gauss-Jordan

M√©todos diretos: Elimina√ß√£o de Gauss-Jordan
- O sistema original √© manipulado at√© obter um sistema equivalente na forma diagonal.
**[[ARRUMAR]]**

- Algoritmo Gauss-Jordan
1. Normalize a equa√ß√£o piv√¥ com a divis√£o de todos os seus termos pelo coeficiente piv√¥.
2. Elimine os elementos fora da diagonal principal em TODAS as demais equa√ß√µes usando
opera√ß√µs de linha.
- O m√©todo de Gauss-Jordan pode ser combinado com pivota√ß√£o igual ao m√©todo de
elimina√ß√£o de Gauss.


M√©todos iterativos

**ATEN√á√ÉO:** Metodos interativos tem que tem criterio de parada ou vai at√© o infinito.

- Nos m√©todos iterativos, as equa√ß√µes s√£o colocadas em uma forma expl√≠cita onde cada
inc√≥gnita √© escrita em termos das demais, i.e.
**[[ARRUMAR]]**

- Dado um valor inicial para as inc√≥gnitas estas ser√£o atualizadas at√© a converg√™ncia.
- Atualiza√ß√£o: M√©todo de Jacobi
**[[ARRUMAR]]**


- Atualiza√ß√£o: M√©todo de Gauss-Seidel
**[[ARRUMAR]]**

M√©todo iterativo de Jacobi
- Implementa√ß√£o computacional
```{r}
jacobi <- function(A, b, inicial, max_iter = 10, tol = 1e-04) {
n <- length(b)
x_temp <- matrix(NA, ncol = n, nrow = max_iter)
x_temp[1,] <- inicial
x <- x_temp[1,]
for(j in 2:max_iter) { #### Equa√ß√£o de atualiza√ß√£o
for(i in 1:n) {
x_temp[j,i] <- (b[i] - sum(A[i,1:n][-i]*x[-i]))/A[i,i]
}
x <- x_temp[j,]
if(sum(abs(x_temp[j,] - x_temp[c(j-1),])) < tol) break #### Crit√©rio de parada
}
return(list("Solucao" = x, "Iteracoes" = x_temp))
}

```


M√©todo iterativo de Jacobi
- Resolva o seguinte sistema de equa√ß√µes lineares usando o m√©todo de Jacobi.
$$9x 1 - 2x 2 + 3x 3 + 2x 4 = 54.5$$
$$2x 1 + 8x 2 - 2x 3 + 3x 4 = -14$$
$$-3x 1 + 2x 2 + 11x 3 - 4x 4 = 12.5$$
$$-2x 1 + 3x 2 + 2x 3 - 10x 4 = -21$$

- Computacionalmente
```{r}
A <- matrix(c(9,2,-3,-2,-2,8,2,
3,3,-2,11,2,2,3,-4,10),4,4)
b <- c(54.5, -14, 12.5, -21)
ss <- jacobi(A = A, b = b,
inicial = c(0,0,0,0),
max_iter = 15)

## Solu√ß√£o aproximada

ss$Solucao
## [1] 4.999502 -1.999771 2.500056 -1.000174
## Solu√ß√£o exata
solve(A, b)
## [1] 5.0 -2.0 2.5 -1.0
```

M√©todos iterativo de Jacobi e Gauss-Seidel
- Em R o pacote Rlinsolve fornece implementa√ß√µes eficientes dos m√©todos de Jacobi e Gauss-Seidel.
- Rlinsolve inclui suporte para matrizes esparsas via Matrix.
- Rlinsolve √© implementado em C++ usando o pacote Rcpp.
```{r}
A <- matrix(c(9,2,-3,-2,-2,8,2,3,3,-2,11,
2,2,3,-4,10),4,4)
b <- c(54.5, -14, 12.5, -21)
## pacote extra
require(Rlinsolve)
lsolve.jacobi(A, b)$x ## M√©todo de jacobi
## [,1]
## [1,] 4.9999708
## [2,] -2.0000631
## [3,] 2.5000163
## [4,] -0.9999483
lsolve.gs(A, b)$x ## M√©todo de Gauss-Seidell
## [,1]
## [1,] 4.999955
## [2,] -2.000071
## [3,] 2.500018
## [4,] -0.999968
```

### Decomposi√ß√£o LU
- Nos m√©todos de elimina√ß√£o de Gauss e Gauss-Jordan resolvemos sistemas do tipo
$$ Ax  = b .$$
- Sendo dois sistemas
$$Ax  = b_1, e \space Ax  = b_2$$
- C√°lculos do primeiro n√£o ajudam a resolver o segundo.
- IDEAL! - Opera√ß√µes realizadas em A fossem dissociadas das opera√ß√µes em $b$ .

Decomposi√ß√£o LU
- Suponha que precisamos resolver v√°rios sistemas do tipo 
$$Ax  = b$$
para diferentes $b$s.
- Op√ß√£o 1 - calcular a inversa $A_{-1}$, assim a solu√ß√£o 
$$x  = A^{-1}b$$
- C√°lculo da inversa √© computacionalmente ineficiente.

Decomposi√ß√£o LU: algoritmo
- Decomponha (fatore) a matriz A em um produto de duas matrizes 
$$A = LU$$
onde $L$ √© triangular inferior e $U$ √© triangular superior.

- Baseado na decomposi√ß√£o o sistema tem a forma: 
$$LUx  = b$$ (3)
- Defina 
$$Ux  = y$$
- Substituindo em 3 tem-se 
$$Ly  = b$$
- Solu√ß√£o √© obtida em dois passos
- Resolva Eq.(4) para obter $y$ usando substitui√ß√£o progressiva.
- Resolva Eq.(3) para obter $x$ usando substitui√ß√£o regressiva.


$$\begin{bmatrix}
a_{11} & a_{12} & a_{13}\\
a_{21} & a_{22} & a_{23}\\
a_{31} & a_{32} & a_{33}\\
\end{bmatrix} *  
\begin{bmatrix}
x_{11} & x_{12} & x_{13}\\
x_{21} & x_{22} & x_{23}\\
x_{31} & x_{32} & x_{33}\\
\end{bmatrix} = 
\begin{bmatrix}
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 1\\
\end{bmatrix}$$

,ou seja,


$$\begin{bmatrix}
a_{11} & a_{12} & a_{13}\\
a_{21} & a_{22} & a_{23}\\
a_{31} & a_{32} & a_{33}\\
\end{bmatrix} *  
\begin{bmatrix}
x_{11} \\
x_{21} \\
x_{31} \\
\end{bmatrix} = 
\begin{bmatrix}
1 \\
0 \\
0 \\
\end{bmatrix}$$

$$\begin{bmatrix}
a_{11} & a_{12} & a_{13}\\
a_{21} & a_{22} & a_{23}\\
a_{31} & a_{32} & a_{33}\\
\end{bmatrix} *  
\begin{bmatrix}
x_{12}\\
x_{22}\\
x_{32}\\
\end{bmatrix} = 
\begin{bmatrix}
0\\
1\\
0\\
\end{bmatrix}$$
$$\begin{bmatrix}
a_{11} & a_{12} & a_{13}\\
a_{21} & a_{22} & a_{23}\\
a_{31} & a_{32} & a_{33}\\
\end{bmatrix} *  
\begin{bmatrix}
x_{13}\\
x_{23}\\
x_{33}\\
\end{bmatrix} = 
\begin{bmatrix}
0\\
0\\
1\\
\end{bmatrix}$$

Obtendo as matrizes L e U
- M√©todo de elimina√ß√£o de Gauss e m√©todo de Crout.
- Dentro do processo de elimina√ß√£o de Gauss as matrizes L e U s√£o obtidas como um subproduto, i.e.
**[[ARRUMAR]]**


- Os elementos m 'i j s s√£o os multiplicadores que multiplicam a equa√ß√£o piv√¥.

Obtendo as matrizes L e U
- Relembre o exemplo de elimina√ß√£o de Gauss.
**[[ARRUMAR]]**


- Neste caso, tem-se
**[[ARRUMAR]]**



Decomposi√ß√£o LU com pivota√ß√£o
- O m√©todo de elimina√ß√£o de Gauss foi realizado sem pivota√ß√£o.
- Como discutido a pivota√ß√£o pode ser necess√°ria.
- Quando realizada a pivota√ß√£o as mudan√ßas feitas devem ser armazenadas, tal que
PA = LU.
- P √© uma matriz de permuta√ß√£o.
- Se as matrizes LU forem usadas para resolver o sistema
Ax  = b ,
ent√£o a ordem das linhas de b  deve ser alterada de forma consistente com a pivota√ß√£o,
i.e. Pb .

Implementa√ß√£o: Decomposi√ß√£o LU
- Podemos facilmente modificar a fun√ß√£o gauss() para obter a decomposi√ß√£o LU.
```{r}
my_lu <- function(A) {
n_row <- nrow(A)
n_col <- ncol(A)
SOL <- matrix(NA, n_row, n_col) ## Matriz para receber os resultados
SOL[1,] <- A[1,]
pivo <- matrix(0, n_col, n_row)
for(j in 1:c(n_row-1)) {
for(i in c(j+1):c(n_row)) {
pivo[i,j] <- A[i,j]/SOL[j,j]
SOL[i,] <- A[i,] - pivo[i,j]*SOL[j,]
A[i,] <- SOL[i,]
}
}
diag(pivo) <- 1
return(list("L" = pivo, "U" = SOL)) }
```

Aplica√ß√£o: Decomposi√ß√£o LU
- Fazendo a decomposi√ß√£o.
```{r}
LU <- my_lu(A) ## Decomposi√ß√£o
LU
## $L
## [,1] [,2] [,3] [,4]
## [1,] 1.0000000 0.0000000 0.000000 0
## [2,] 0.2222222 1.0000000 0.000000 0
## [3,] -0.3333333 0.1578947 1.000000 0
## [4,] -0.2222222 0.3026316 0.279661 1
##
## $U
## [,1] [,2] [,3] [,4]
## [1,] 9 -2.000000e+00 3.000000 2.000000
## [2,] 0 8.444444e+00 -2.666667 2.555556
## [3,] 0 0.000000e+00 12.421053 -3.736842
## [4,] 0 -4.440892e-16 0.000000 10.716102
LU$L %*% LU$U ## Verificando a solu√ß√£o
## [,1] [,2] [,3] [,4]
## [1,] 9 -2 3 2
## [2,] 2 8 -2 3
## [3,] -3 2 11 -4
## [4,] -2 3 2 10

```


Aplica√ß√£o: Decomposi√ß√£o LU
- Resolvendo o sistema de equa√ß√µes.
```{r}
## Passo 1: Substitui√ß√£o progressiva
y = forwardsolve(LU$L, b)
## Passo 2: Substitui√ß√£o regressiva
x = backsolve(LU$U, y)
x
## [1] 5.0 -2.0 2.5 -1.0
A%*%x ## Verificando a solu√ß√£o
## [,1]
## [1,] 54.5
## [2,] -14.0
## [3,] 12.5
## [4,] -21.0
```

- Fun√ß√£o lu() do Matrix fornece a decomposi√ß√£o LU.
```{r}
require(Matrix)
## Calcula mas n√£o retorna
LU_M <- lu(A)
## Captura as matrizes L U e P
LU_M <- expand(LU_M)
## Substitui√ß√£o progressiva.
y <- forwardsolve(LU_M$L, LU_M$P%*%b)
## Substitui√ß√£o regressiva
x = backsolve(LU_M$U, y)
x
## [1] 5.0 -2.0 2.5 -1.0
```

## Obtendo a inversa

### Obtendo a inversa via decomposi√ß√£o LU
- O m√©todo LU √© especialmente adequado para o c√°lculo da inversa.
- Lembre-se que a inversa de A √© tal que 
$$AA^{-1} = I$$
- O procedimento de c√°lculo da inversa √© essencialmente o mesmo da solu√ß√£o de um sistema de equa√ß√µes lineares, por√©m com mais incognitas.
$$\begin{bmatrix}
a_{11} & a_{12} & a_{13}\\
a_{21} & a_{22} & a_{23}\\
a_{31} & a_{32} & a_{33}\\
\end{bmatrix} *  
\begin{bmatrix}
x_{11} & x_{12} & x_{13}\\
x_{21} & x_{22} & x_{23}\\
x_{31} & x_{32} & x_{33}\\
\end{bmatrix} = 
\begin{bmatrix}
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 1\\
\end{bmatrix}$$


- Tr√™s sistemas de equa√ß√µes diferentes, em cada sistema, uma coluna da matriz X √© a incognita.

Implementa√ß√£o: inversa via decomposi√ß√£o LU
- Fun√ß√£o para resolver o sistema usando decomposi√ß√£o LU.
```{r}
solve_lu <- function(LU, b) {
  y <- forwardsolve(LU_M$L, LU_M$P%*%b)
  x = backsolve(LU_M$U, y)
  return(x)
}
```


- Resolvendo v√°rios sistemas
```{r}
my_solve <- function(LU, B) {
  n_col <- ncol(B)
  n_row <- nrow(B)
  inv <- matrix(NA, n_col, n_row)
  for(i in 1:n_col) {
    inv[,i] <- solve_lu(LU, B[,i])
  }
  return(inv)
}
```


Aplica√ß√£o: inversa via decomposi√ß√£o LU
- Calcule a inversa de
$$A = 
\begin{bmatrix}
3 & 2 & 6\\
2 & 4 & 3\\
5 & 3 & 4\\
\end{bmatrix}$$


```{r}
A <- matrix(c(3,2,5,2,4,3,6,3,4),3,3)
I <- Diagonal(3, 1)
## Decomposi√ß√£o LU
LU <- my_lu(A)
## Obtendo a inversa
inv_A <- my_solve(LU = LU, B = I)
inv_A
## Verificando o resultado
A%*%inv_A

```


C√°lculo da inversa via m√©todo de Gauss-Jordan
- Procedimento Gauss-Jordan:
**[[ARRUMAR]]**

- Fun√ß√£o `solve()` usa a decomposi√ß√£o LU com pivota√ß√£o.
- R b√°sico √© constru√≠do sobre a biblioteca lapack escrita em C.
- Veja documenta√ß√£o em <a href="http://www.netlib.org/lapack/lug/node38.html"> http://www.netlib.org/lapack/lug/node38.html</a>.

#### Autovalores e autovetores
- Redu√ß√£o de dimensionalidade √© fundamental em ci√™ncia de dados.
- An√°lise de componentes principais (PCA)
- An√°lise fatorial (AF).
- Decompor grandes e complicados relacionamentos multivariados em simples componentes n√£o relacionados.
- Vamos discutir apenas os aspectos matem√°ticos.

Intui√ß√£o
- Podemos decompor um vetor $\upsilon$  em duas informa√ß√µes separadas: dire√ß√£o $d$ e tamanho $\lambda $, i.e

$$\lambda  = ||\upsilon || = \sqrt{ \sum_j{v^2_j}} , 
\space \space \space  e  \space \space \space 
d = \frac{\upsilon}{\lambda} $$

- √â mais f√°cil interpretar o tamanho de um vetor enquanto ignorando a sua dire√ß√£o e
vice-versa.
- Esta ideia pode ser estendida para matrizes.
- Uma matriz nada mais √© do que um conjunto de vetores.
- IDEIA - decompor a informa√ß√£o de uma matriz em outros componentes de mais f√°cil
interpreta√ß√£o/representa√ß√£o matem√°tica.

Autovalores e Autovetores
- Autovalores e autovetores s√£o definidos por uma simples igualdade $A\upsilon  = \lambda \upsilon $. (5)
- Os vetores $\upsilon$ ‚Äôs que satisfazem Eq. (5) s√£o os autovetores.
- Os valores $\lambda$ ‚Äôs que satisfazem Eq. (5) s√£o os autovalores.
- Vamos considerar o caso em que $A$ √© sim√©trica.
- A ideia pode ser estendida para matrizes n√£o sim√©tricas.


- Se A √© uma matriz sim√©trica $n \times n$, ent√£o existem exatamente $n$ pares ($\lambda j , \upsilon j $) que
satisfazem a equa√ß√£o:
A$\upsilon  = \lambda \upsilon$ .
- Se A tem autovalores $\lambda_1, ‚Ä¶ , \lambda_n$, ent√£o:
**[[ARRUMAR]]**

- A √© positiva definida, se e somente se todos $\lambda j  > 0$
- A √© semi-positiva definida, se e somente se todos $\lambda j  ‚â• 0$
- A ideia do PCA √© decompor/fatorar a matriz A em componentes mais simples de
interpretar.

Decomposi√ß√£o em autovalores e autovetores
- Teorema: qualquer matriz sim√©trica A pode ser fatorada em 
$$A = Q \Lambda Q^T$$
onde $\lambda$ √© diagonal contendo os autovalores de A e as colunas de Q cont√™m os autovetores ortonormais.
- Vetores ortonormais: s√£o mutuamente ortogonais e de comprimento unit√°rio.
- Teorema: se $A$ tem autovetores $Q$ e autovalores $\lambda j$ . Ent√£o $A^{-1}$ tem autovetores $Q$ e
autovalores $\lambda^{-1}_j$  .
- Implica√ß√£o: se $A = Q\Lambda Q^T$ ent√£o $A-1 = Q\Lambda^{-1} Q^T$

**ATEN√á√ÉO:** Lambda √© uma matriz diagonal que voc√™ pode usar para fazer contas de n√∫meros (escalares) em matrizes.

#### Diagonaliza√ß√£o
- Autovalores s√£o ut√©is porque eles permitem lidar com matrizes da mesma forma que lidamos com n√∫meros.
- Todos os c√°lculos s√£o feitos na matriz diagonal \Lambda .
- Este processo √© chamado de diagonaliza√ß√£o.
- Um dos resultados mais poderosos em √Ålgebra Linear √© que qualquer matriz pode ser diagonalizada.
- O processo de diagonaliza√ß√£o √© chamado de Decomposi√ß√£o em valores singulares.

#### Decomposi√ß√£o em valores singulares (SVD)
- Teorema: qualquer matriz A pode ser decomposta em 
$$A = UDV^T$$
onde $D$ √© diagonal com entradas n√£o negativas e $U$ e $V$ s√£o ortogonais, i.e. $U^TU = V^TV = I$.
- Matrizes n√£o quadradas n√£o tem autovalores.
- Os elementos de D s√£o chamados de valores singulares.
- Os valores singulares s√£o os autovalores de $A^TA$.

### Dimens√£o da SVD
- Se $A$ √© $n \times n$, ent√£o $U$, $D$ e $V$ s√£o $n \times n$.
- Se $A$ √© $n \times p$, sendo $n > p$, ent√£o $U$ √© $n \times p$, $D$ e $V$ s√£o $p \times p$.
- Se $A$ √© $n \times p$, sendo $n < p$, ent√£o $V^T$ √© $n \times p$, $D$ e $U$ s√£o $n \times n$.
- $D$ ser√° sempre quadrada com dimens√£o igual ao m√≠nimo entre $ $ e $n$.

### Decomposi√ß√£o em autovalores e autovetores em R
- Fun√ß√£o eigen() fornece a decomposi√ß√£o
```{r}
A <- matrix(c(1,0.8, 0.3, 0.8, 1,
0.2, 0.3, 0.2, 1),3,3)
isSymmetric.matrix(A)
## [1] TRUE
out <- eigen(A)
Q <- out$vectors ## Autovetores
D <- diag(out$values) ## Autovalores
Q
## [,1] [,2] [,3]
## [1,] -0.6712373 -0.1815663 0.71866142
## [2,] -0.6507744 -0.3198152 -0.68862977
## [3,] -0.3548708 0.9299204 -0.09651322
```

- Verificando a solu√ß√£o
```{r}
D
## [,1] [,2] [,3]
## [1,] 1.934216 0.0000000 0.0000000
## [2,] 0.000000 0.8726419 0.0000000
## [3,] 0.000000 0.0000000 0.1931419
Q%*%D%*%t(Q) ## Verificando
## [,1] [,2] [,3]
## [1,] 1.0 0.8 0.3
## [2,] 0.8 1.0 0.2
## [3,] 0.3 0.2 1.0
```


Decomposi√ß√£o em valores singulares em R
- Fun√ß√£o svd() fornece a decomposi√ß√£o
```{r}
svd(A)
## $d
## [1] 1.9342162 0.8726419 0.1931419
##
## $u
## [,1] [,2] [,3]
## [1,] -0.6712373 0.1815663 0.71866142
## [2,] -0.6507744 0.3198152 -0.68862977
## [3,] -0.3548708 -0.9299204 -0.09651322
##
## $v
## [,1] [,2] [,3]
## [1,] -0.6712373 0.1815663 0.71866142
## [2,] -0.6507744 0.3198152 -0.68862977
## [3,] -0.3548708 -0.9299204 -0.09651322
```


### Regress√£o ridge
- Relembrando: regress√£o linear m√∫ltipla

$$\begin{bmatrix}
y_1 \\
y_2 \\
... \\
y_n \\
\end{bmatrix}_{n \times 1}=
\begin{bmatrix}
1 & x_{11} & ... & x_{p1} \\
1 & x_{12} & ... & x_{p2} \\
... & ... & ... & ... \\
1 & x_{1n} & ... & x_{pn} \\
\end{bmatrix}_{n \times p} *
\begin{bmatrix}
\beta_0 \\
\beta_1 \\
... \\
\beta_p \\
\end{bmatrix}_{p \times 1}+
\begin{bmatrix}
erro _0 \\
erro _1 \\
... \\
erro _p \\
\end{bmatrix}_{p \times 1}$$ 


- Usando uma nota√ß√£o mais compacta,
$$y_{n \times 1} = X_{n \times p} * \beta_{p \times 1} + erro_{p \times 1}$$

- Minimiza a perda quadr√°tica:
$$ \hat{\beta} = (X^T X)^{-1} X^T y $$



### Regress√£o ridge
- Se $p > n$ (mais colunas que linhas) o sistema √© singular (m√∫ltiplas solu√ß√µes, ex: pixels de uma imagem)!
- Como podemos ajustar o modelo?
- Introduzir uma penalidade pela complexidade.
- Soma de quadrados penalizada (para tirar os betas que s√£o irrelevantes)

$$
PSQ(\beta) = \sum_{i=1}^n{(y_i - x_i^T \beta)^2} + \sum_{j=1}^{p}{\beta^2_j}
$$

- Matricialmente, tem-se
$$ PSQ(\beta) = (y - X \beta)^T (y - X \beta) + \lambda \beta^T \beta$$

- IMPORTANTE !!
- $y$  centrado (m√©dia zero).
- $X$ padronizada por coluna (m√©dia zero e vari√¢ncia um).

Regress√£o ridge
- Objetivo: minizar a soma de quadrados penalizada.
- Derivada
$$ frac{\partial PQS (\beta)}{\partial \beta} = frac{\partial}{\partial \beta} [ (y - X \beta)^T (y - X \beta) + \lambda \beta^T \beta]$$
$$ = frac{\partial}{\partial \beta} [ (y - X \beta)^T (y - X \beta) + \lambda \beta^T \beta]$$

**[[ARRUMAR]]**


Aplica√ß√£o: regress√£o ridge
- Resolvendo o sistema linear, tem-se

**[[ARRUMAR]]**

- Solu√ß√£o depende de $\lambda$ .
- A inclus√£o de $\lambda$  faz o sistema ser n√£o singular.
- Na verdade quando fixamos $\lambda$  selecionamos uma solu√ß√£o em particular.

Aplica√ß√£o: regress√£o ridge
- Calcular $\hat{\beta}$  envolve a invers√£o de uma matriz p x  p potencialmente grande.
$$\hat{\beta}  = (X^T X + \lambda I)^{-1} X^Ty$$
- Usando a decomposi√ß√£o SVD, tem-se
$$ X = UDV^T$$
- √â poss√≠vel mostrar que,

$$ \hat{\beta} = V_{diag} ( \frac{d_j}{d^2_j  + \lambda}) U^T y$$

#### Implementa√ß√£o: regress√£o ridge
- Simulando o conjunto de dados (n = 100, p = 200).
- Conjunto treino e conjunto teste

```{r}
set.seed(123)
X <- matrix(NA, ncol = 200, nrow = 100)
X[,1] <- 1 ## Intercepto
for(i in 2:200) {
X[,i] <- rnorm(100, mean = 0, sd = 1)
X[,i] <- (X[,i] - mean(X[,i]))/var(X[,i])
}
## Par√¢metros
beta <- rbinom(200, size = 1, p = 0.1)*rnorm(200, mean = 10)
mu <- X%*%beta
## Observa√ß√µes
y <- rnorm(100, mean = mu, sd = 10)
```


#### Implementando o modelo
- Modelo passo-a-passo
```{r}
y_c <- y - mean(y)
X_svd <- svd(X) ## Decomposi√ß√£o svd
lambda = 0.5 ## Penaliza√ß√£o
DD <- Diagonal(100, X_svd$d/(X_svd$d^2 + lambda))
DD[1] <- 0 ## N√£o penalizar o intercepto
beta_hat = as.numeric(X_svd$v%*%DD%*%t(X_svd$u)%*%y_c)
```

Resultados
- Ajustados versus verdadeiros.
```{r}
plot(beta ~ beta_hat, xlab = expression(hat(beta)), ylab = expression(beta))
```
**[[ARRUMAR]]**

Resultados: regress√£o ridge
- Regress√£o com penaliza√ß√£o ridge, bem como, outras penaliza√ß√µes s√£o eficientemente
implementadas em R via pacote glmnet.

**IMPORTANTE!** A penaliza√ß√£o no glmnet √© ligeiramente diferente, por isso os $\hat{\beta}$‚Äôs n√£o
s√£o id√™nticos a nossa implementa√ß√£o naive.
- O glmnet oferece op√ß√µes para selecionar $\lambda$ via valida√ß√£o cruzada.

```{r}
require(glmnet)
beta_glm <- cv.glmnet(X[,-1], y_c, nlambda = 100)
```

Resultados: regress√£o ridge
- Valida√ß√£o cruzada.
```{r}
plot(beta_glm)
```
```{r}
beta_glm$lambda.min
#coef(beta_glm, s = "lambda.min")
```



**[[ARRUMAR]]**


Resultados: regress√£o ridge
- Ajustados (glmnet) versus verdadeiros.
```{r}
plot(beta ~ as.numeric(coef(beta_glm)), xlab = expression(hat(beta)), ylab = expression(beta)
```
**[[ARRUMAR]]**

Coment√°rios:

- Solu√ß√£o de sistemas lineares:
- M√©todos diretos: Elimina√ß√£o de Gauss e Gauss-Jordan.
- M√©todos iterativos: Jacobi e Gauss-Seidel.
- Inversa de matrizes.
- Decomposi√ß√£o ou fatoriza√ß√£o
- LU resolve sistema lineares pode ser usada para obter inversas.
- Autovalores e autovetores.
- Valores singulares.
- Existem muitas outras fatoriza√ß√µes: QR, Cholesky, Cholesky modificadas, etc.




```{r}

```

