


# Álgebra Matricial

## Vetores e escalares
- Um vetor é uma lista de n números (escalares) escritos em linha ou coluna.
- Notação (primeiro a em negrito)

$$
a = (a_{i1} ... a_{in})
$$
ou

$$
a = \begin{bmatrix}
a_{i1}\\
.\\
.\\
.\\
a_{in}\\
\end{bmatrix}
$$

- Vetor linha e vetor coluna.
- Um elemento do vetor é chamado de ai , sendo i  a sua posição.
- O tamanho de um vetor é o seu número de elementos.
- O módulo de um vetor é o seu comprimento
$$
|a| = \sqrt a2
1 + … + a2
n.
$$
- Vetor unitário é aquele que tem tamanho
$$
a = a
|a| .
$$
- Dois vetores são iguais se tem o mesmo
tamanho e os seus elementos em posições
equivalentes são iguais.

### Operações com vetores

1. Soma $a + b = (ai  + b i ) = (a1 + b 1, … , an + b n)$.

$a = (1, 2, 3)$
$b = (3, 2, 1)$
$a+b = (4, 4, 4)$
$a-b = (-2, 0, 2)$

2. Subtração $a − b = (ai  − b i ) = (a1 − b 1, … , an − b n)$.

$$
a-b = (-2, 0, 2)
$$

3. Multiplicação por escalar $𝛼a = (𝛼a1, … , 𝛼an)$.

$$
5 * a = (5*1, 5*2, 5*3)
$$

4. Transposta de um vetor:
[...]

5. Produto interno ou escalar entre dois vetores resulta em um escalar (mutiplica dois vetores e dá um número só como resultado)
a ⋅ b = (a1b 1 + a2b 2 + … + anb n).

- **Condições: os vetores devem ser do mesmo tipo e tamanho.**

### Vetores ortogonais
- Dois vetores são ortogonais entre si se o ângulo 𝜃 entre eles é de 90∘.(= correlação de Pearson)
- Implicações: 
$$cos(𝜃) = 0 e a⊤b = 0.$$
$$ cov (a,b) / raiz(variacia[a]) * raiz(variacia[b])$$

- O co-seno do ângulo 𝜃 entre os vetores é dado por:
$$cos(𝜃) = a⊤b / \sqrt a⊤a\sqrt b⊤b .$$

### Operações com vetores em R
- Declarando vetores
```{r}
a <- c(4,5,6)
b <- c(1,2,3)
```

- Sendo a e b compatíveis
```{r}
#### Soma
a + b
## [1] 5 7 9
#### Substração
a - b
## [1] 3 3 3
```


- Multiplicação por escalar
```{r}
alpha = 10
alpha*a
## [1] 40 50 60
```
- Produto de Hadamard (não é produto interno)

```{r}
a*b
## [1] 4 10 18
```
- Produto vetorial (ou produto interno)

```{r}
a%*%b
##    [,1]
## [1,] 32
```
- Co-seno do ângulo entre dois vetores
```{r}
cos <- t(a)%*%b/(sqrt(t(a)%*%a)*sqrt(t(b)%*%b))
```
- Lei da reciclagem (não avalia se pode somar antes de somar)
```{r}
a <- c(4,5,6,5,6,7)
b <- c(1,2,3)
a + b
## [1] 5 7 9 6 8 10
```

## Matrizes

- Uma matriz é um arranjo retangular ou quadrado de números ou variáveis.
- A matriz costuma ser representada por uma letra maiuscula em negrito

- Uma matriz (n x  𝑚) tem n linhas e 𝑚 colunas:

$$A = \begin{pmatrix}\
a_{11} & a_{12} & ... & a_{1m}\\
a_{21} & a_{22} & ... & a_{2m}\\
... & ... & ... & ... \\
a_{n1} & a_{11} & ... & a_{nm}\\
\end{pmatrix}$$

- O primeiro subscrito representa linha e o segundo representa coluna.
- A dimensão de uma matriz é o seu número de linhas e colunas.
- Duas matrizes são iguais se tem a mesma dimensão e se os elementos das correspondentes
posições são iguais.

### Matriz transposta
- A operação de transposição rearranja uma matriz de forma que suas linhas são transformadas em colunas e vice-versa.
⎛⎜
⎝
1 2
3 4
5 6
⎞⎟
⎠
⊤
= (1 3 5
2 4 6) .
- Note que (A⊤)⊤ = A.
- Computacionalmente
- Declarando matrizes
```{r}
a <- c(1,2,3,4,5,6)
A <- matrix(a, nrow = 3, ncol = 2)
A
## [,1] [,2]
## [1,] 1 4
## [2,] 2 5
## [3,] 3 6
```
- O default preenche por colunas.
- Transposta de uma matriz
```{r}
t(A)
## [,1] [,2] [,3]
## [1,] 1 2 3
## [2,] 4 5 6
```

### Operações com matrizes
- Multiplicação matriz por escalar.
$$\alpha * A = \begin{pmatrix}\
\alpha * a_{11} & \alpha * a_{12} & \alpha * ... & \alpha * a_{1m}\\
\alpha * a_{21} & \alpha * a_{22} & \alpha * ... & \alpha * a_{2m}\\
\alpha * ... & \alpha * ... & \alpha * ... & \alpha * ... \\
\alpha * a_{n1} & \alpha * a_{n2} & \alpha * ... & \alpha * a_{nm}\\
\end{pmatrix}$$

- Computacionalmente
```{r}
A <- matrix(c(1,2,3,4,5,6),
nrow = 3, ncol = 2)
alpha <- 10
alpha*A
## [,1] [,2]
## [1,] 10 40
## [2,] 20 50

```

- Duas matrizes podem ser somadas ou
subtraídas somente se tiverem o mesmo
tamanho.
1. Soma c i j  = ai j  + b i j .
2. Subtração c i j  = ai j  − b i j .
- Exemplo
$$A = \begin{pmatrix}\
1 & 2\\
3 & 4\\
5 & 6\\
\end{pmatrix}$$

$$B = \begin{pmatrix}\
10 & 20\\
30 & 40\\
50 & 60\\
\end{pmatrix}$$

$$A + B = \begin{pmatrix}\
11 & 22\\
33 & 44\\
55 & 66\\
\end{pmatrix}$$

- Soma de duas matrizes
```{r}
A <- matrix(c(1,2,3,4,5,6),
nrow = 3, ncol = 2)
B <- matrix(c(10,20,30,40,50,60),
nrow = 3, ncol = 2)
C = A + B
C
## [,1] [,2]
## [1,] 11 44
## [2,] 22 55
## [3,] 33 66
```

- Condição para multiplicar matrizes
$$
C_{m, n} = A_{m,q} B_{q,n}
$$
(q tem que ser igual)

C
𝑚x n = A
𝑚x 𝑞 B
𝑞x n.
- Cada elemento c i j  = \sum{}𝑞
𝑘=1 ai 𝑘b 𝑘j .
⎛⎜
⎝
2 −1
8 3
6 7
⎞⎟
⎠
( 4 9 1 −3
−5 2 4 6 ) =
⎛⎜
⎝
((2 ⋅ 4) + (−1 ⋅ −5)) ((2 ⋅ 9) + (−1 ⋅ 2)) ((2 ⋅ 1) + (−1 ⋅ 4)) ((2 ⋅ −3) + (−1 ⋅ 6))
((8 ⋅ 4) + (3 ⋅ −5)) ((8 ⋅ 9) + (3 ⋅ 2)) ((8 ⋅ 1) + (3 ⋅ 4)) ((8 ⋅ −3) + (3 ⋅ 6))
((6 ⋅ 4) + (7 ⋅ −5)) ((6 ⋅ 9) + (7 ⋅ 2)) ((6 ⋅ 1) + (7 ⋅ 4)) ((6 ⋅ −3) + (7 ⋅ 6))
⎞⎟
⎠
=
⎛⎜
⎝
13 16 −2 −12
17 78 20 −6
−11 68 34 24
⎞⎟
⎠
.

- Computacionalmente.
- Matrizes compatíveis
```{r}
A <- matrix(c(2,8,6,-1,3,7),
nrow = 3, ncol = 2)
B <- matrix(c(4,-5,9,2,1,4,-3,6),
nrow = 2, ncol = 4)
C = A%*%B
C
## [,1] [,2] [,3] [,4]
## [1,] 13 16 -2 -12
## [2,] 17 78 20 -6
## [3,] -11 68 34 24
```

- Matrizes não compatíveis
```{r message=TRUE, warning=TRUE}
B %*% A
## Error in B %*% A: argumentos não compatíveis
```


Produto de Hadamard
- Produto simples ou de Hadamard

$$A \odot B = \begin{pmatrix}\
a_{11}*b_{11} & a_{12}*b_{12} & ... & a_{1m}*b_{1m}\\
a_{21}*b_{21} & a_{22}*b_{22} & ... & a_{2m}*b_{2m}\\
... & ... & ... & ... \\
a_{n1}*b_{n1} & a_{n2}*b_{n2} & ... & a_{nm}*b_{nm}\\
\end{pmatrix}$$


- Computacionalmente
```{r}
A <- matrix(c(1,2,3,4),
nrow = 2, ncol = 2)
B <- matrix(c(10,20,30,40),
nrow = 2, ncol = 2)
A*B
## [,1] [,2]
## [1,] 10 90
## [2,] 40 160
```


Propriedades envolvendo operações com matrizes
- Sendo A, B, C e D compatíveis temos,
1. $A + B = B + A$
2. $(A + B) + C = A + (B + C)$.
3. $𝛼(A + B) = 𝛼A + 𝛼B$.
4. $(𝛼 + \beta )A = 𝛼A + \beta A$.
5. $𝛼(AB) = (𝛼A)B = A(𝛼B)$.
6. $A(B ± C) = AB ± AC$.
7. $(A ± B)C = AC ± BC$.
8. $(A−B)(C−D) = AC−BC−AD+BD$.

- Propriedades envolvendo transposta e
multiplicação
1. Se A é n x  𝑚 e B é 𝑚 x  n, então (AB)⊤ = B⊤A⊤.
2. Se A, B e C são compatíveis 
$$
(ABC)^{⊤}= C^{⊤}B^{⊤}A^{⊤}.
$$

### Matrizes de formas especiais
- Matriz quadrada (m = n)
Exemplo 4x4
```{r}
A <- matrix(c("a11","a21","a31","a41","a12","a22","a32","a42","a13","a23","a33","a43","a14","a24","a34","a44"), nrow = 4, ncol = 4)
A
```

- ai i  são os elementos da diagonal.
- ai j  para i  ≠ j  → fora da diagonal.
- ai j  para j  > i  → acima da diagonal.
- ai j  para i  > j  → abaixo da diagonal.
- Matriz diagonal
$$D = \begin{pmatrix}\
a_{11} & 0 & 0 & 0\\
0 & a_{22} & 0 & 0\\
0 & 0 & a_{33} & 0 \\
0 & 0 & 0 & a_{44}\\
\end{pmatrix}$$



- Matriz identidade
I = ⎛⎜⎜⎜⎜
⎝
1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 1
⎞⎟⎟⎟⎟
⎠

### Matrizes de formas especiais
- Triangular superior
U = ⎛⎜⎜⎜⎜
⎝
a11 a12 a13 a14
0 a22 a23 a24
0 0 a33 a34
0 0 0 a44
⎞⎟⎟⎟⎟
⎠
.
- Triangular inferior
L = ⎛⎜⎜⎜⎜
⎝
a11 0 0 0
a21 a22 0 0
a31 a32 a33 0
a41 a42 a43 a44
⎞⎟⎟⎟⎟
⎠
.
- Matriz nula
0 = ⎛⎜⎜⎜⎜
⎝
0 0 0 0
0 0 0 0
0 0 0 0
0 0 0 0
⎞⎟⎟⎟⎟
⎠
.
- Matriz quadrada simétrica
A = ⎛⎜⎜⎜⎜
⎝
1 0.8 0.6 0.4
0.8 1 0.2 0.4
0.6 0.2 1 0.1
0.4 0.4 0.1 1
⎞⎟⎟⎟⎟
⎠


### Combinações lineares
- Um conjunto de vetores a1, a2, … , an é dito ser linearmente dependente se puderem ser
encontrados escalares c 1, c 2, … , c n e estes escalares não sejam todos iguais a 0 de tal forma
que
$$
c 1a1 + c 2a2 + … + c nan = 0.
$$

Exemplo:
```{r}
a1 <- matrix(c(1,0), nrow = 2, ncol = 1)
a1
a2 <- matrix(c(0,1), nrow = 2, ncol = 1)
a2

#O unico caso que esses c1*a1 + c2*a2 = (0, 0) é se c1 = 0 E c2 =0
#Ou seja Linearmente independente

a1 <- matrix(c(1,2), nrow = 2, ncol = 1)
a1
a2 <- matrix(c(-1,-2), nrow = 2, ncol = 1)
a2

#Existem casos fora os cs = 0 que fazem c1*a1 + c2*a2 = (0, 0)
#Ou seja Linearmente dependente

```


- Caso contrário é dito ser linearmente independente.
- Notação matricial
$$
Ac = 0.
$$
- As colunas de A são linearmente independentes se Ac = 0 implicar que c = 0.

### Rank ou posto de uma matriz
- O rank ou posto de qualquer matriz quadrada ou retangular A é definido como
rank(A) = número de colunas ou linhas linearmente independentes em A.
- Sendo A uma matriz retangular n x  𝑚 o maior rank possível para A é o min(n,𝑚).
- O rank da matrix nula é 0.
- Se o rank da matriz é o min(n,𝑚) dizemos que a matriz tem rank completo.

### Matriz não singular e matriz inversa
- Uma matriz quadrada de posto completo é chamada de não singular.
- Sendo A quadrada de posto completo a matriz inversa de A é única tal que (só se a matriz for quadrada e de ranking completo)
$$
AA^{−1} = I.
$$
- Não quadrada (posto incompleto) → não terá inversa e é dita ser singular.
- Note que 
$$
A^{(-1^{-1})} =A
$$
A^{−1}^{−1} = A

## Matriz inversa
- Computacionalmente
```{r}
A <- matrix(c(4, 2, 7, 6), 2, 2)
A

A_inv <- solve(A)
A_inv

I = A %*% A_inv
I

```

- Verificando
```{r}
A%*%A_inv
## [,1] [,2]
## [1,] 1 0
## [2,] 0 1
```


- Propriedades envolvendo inversas
1. Se A é não singular, então A⊤ é não singular e sua inversa é dada por
(A⊤)−1 = (A−1)⊤.
2. Se A e B são matrizes não singulares de mesmo tamanho, então o produto AB é
não singular e
(AB)−1 = B−1A−1.

### Inversa generalizada
- A inversa generalizada de uma matriz A n x  p é qualquer matriz A− que satisfaça 
$$
AA^{−}A = A.
$$

- Não é única exceto quando A é não-singular (inversa usual).
- Exemplo

$$

$$

a = ⎛⎜⎜⎜⎜
⎝
1
2
3
4
⎞⎟⎟⎟⎟
⎠
.


- a− = (1, 0, 0, 0)

- Verificando

```{r}
a <- matrix(c(1, 2, 3, 4), 4, 1)
a_invg <- matrix(c(1,0,0,0), 1, 4)
a%*%a_invg%*%a
## [,1]
## [1,] 1
## [2,] 2
## [3,] 3
## [4,] 4
```
- Moore-Penrose generalized inverse
```{r}
#### Matriz singular (col 3 = col 2 + col 1)
A <- matrix(c(2, 1, 3, 2, 0,
2, 3, 1, 4), 3, 3)
library(MASS)
A_ginv <- ginv(A)
A%*%A_ginv%*%A ## Verificando
```

## Matrizes positivas definidas

### Formas quadráticas
- Soma de quadrados são importantes em ciência de dados.
- Considere uma matriz A simétrica e y um vetor, o produto
$$
y^{T}Ay = 
\sum(a_{ij}y^{2}_{i}) + 
\sum_{i \differ j}(a_{ij}y_{i}y_{j})
$$
é chamado de forma quadrática.

$$
y^{T}Iy = \sum^{n}_{i=0}(y^{2}_{i})
$$


- Sendo y de dimensão n x  1, 
$$
y⊤Iy = y 2
1 + y 2
2 + … , y 2
n
$$

- Consequentemente, y⊤y é a soma de quadrados dos elementos do vetor y.
- A raiz quadrada da soma de quadrados é o comprimento de y.

Matriz positiva definida
- Sendo A uma matriz simétrica com a propriedade y⊤Ay > 0 para todos os possíveis y
exceto para quando y = 0, então a forma quadrática y⊤Ay é chamada positiva definida,
e A é dita ser uma matriz positiva definida.
- Exemplo
A = ( 2 −1
−1 3 ) .
A forma quadrática associada é dada por (ver abaixo) que é claramente positiva, desde que y 1 e y 2 sejam diferentes de zero.
$$
y⊤Ay = (y 1 y 2) ( 2 −1
−1 3 ) (y 1
y 2
) = 2y 2
1 − 2y 1y 2 + 3y 2
2 ,
$$

### Propriedades de matrizes positivas definidas
1. Se A é positiva definida, então todos os valores da diagonal de A são positivos.
2. Se A é positiva semi-definida, então os elementos da diagonal de A são maiores ou iguais a zero.
3. Sendo P uma matriz não-singular e A uma matriz positiva definida, o produto P⊤AP é positiva definida.
4. Sendo P uma matriz não-singular e A uma matriz positiva semi-definida, o produto P⊤AP é positiva semi-definida.
5. Uma matriz positiva definida é não-singular.

### Determinante de uma matriz
- O determinante de uma matriz A é o escalar (= numero)
$$
|A| = \sum((-1)^k a_{1j_{1}} a_{2j_{2}} ... a_{nj_{n}})
$$
onde a soma é realizada para todas as n! permutações de grau n, e 𝑘 é o número de
mudanças necessárias para que os segundos subscritos sejam colocados na ordem
1,2, … , n.
- Considere a matriz
A = ( 3 −2
−2 4 ) .
|A| = (−1)0a11a22 + (−1)1a12a21 = 1 ⋅ (3 ⋅ 4) − (−2 ⋅ −2) = 12 − 4 = 8.

Determinante de uma matriz
- Computacionalmente.
A <- matrix(c(3,-2,-2,4),2,2)
determinant(A, logarithm = FALSE)$modulus
## [1] 8
## attr(,"logarithm")
## [1] FALSE
- Determinante em escala log.
determinant(A, logarithm = TRUE)$modulus
## [1] 2.079442
## attr(,"logarithm")
## [1] TRUE
- Alguns aspectos interessantes sobre
determinantes são:
1. Se A é singular, |A| = 0.
2. Se A é não singular, |A| ≠ 0.
3. Se A é positiva definida, |A| > 0.
4. |A⊤| = |A|.
5. Se A é não singular, |A−1| = 1
|A| .

Traço de uma matriz
- O traço de uma matriz A n x  n é um
escalar definido como a soma dos
elementos da diagonal, tr(A) = \sum{}n
i =1 ai i .
- Propriedades
1. Se A e B são n x  n, então
tr(A + B) = tr(A) + tr(B).
2. Se A é n x  p e B e p x  n, então
tr(AB) = tr(BA).
- Computacionalmente
```{r}
A <- matrix(c(3,-2,-2,4),2,2)
sum(diag(A))
## [1] 7
```

## Cálculo vetorial e matricial

### Cálculo vetorial
- Seja $y = f(x)$ uma função das variáveis $x_{1}, x_{2}, x_{3}, ... , x_{p}$ e \partial y  as respectivas derivadas parciais.

$$
\matrix_vertical de (a1x1, a2x2, .... apxp) 
$$

$$
\partial x 1
, \partial y 
\partial x 2
, … , \partial y 
\partial x p
$$

Assim,
$$
\deriv
$$


\partial y 
\partial x =
⎛⎜⎜⎜⎜⎜
⎝
\partial y 
\partial x 1
\partial y 
\partial x 2
⋮
\partial y 
\partial x p
⎞⎟⎟⎟⎟⎟
⎠
.

### Cálculo vetorial
- Sendo a⊤ = (a1, a2, … , ap) um vetor de constantes e A uma matriz simétrica de constantes.
1. Seja y  = a⊤x = x⊤a. Então,
$$
\partial y 
\partial x = \partial (x⊤a)
\partial x = a.
$$
2. Seja y  = x⊤Ax. Então,
$$
\partial y 
\partial x = \partial (x⊤Ax)
\partial x = 2Ax.
$$

### Cálculo Matricial
- Se y  = f (X) onde X é uma matriz p x  p. As derivadas parciais de y  em relação a cada x i j 
são organizadas em uma matriz.
$$
\partial y 
\partial X = ⎛⎜⎜
⎝
\partial y 
\partial x 11
… \partial y 
\partial x 1p
⋮ ⋱ ⋮
\partial y 
\partial x p1
… \partial y 
\partial x pp
⎞⎟⎟
$$


- Algumas derivadas importantes envolvendo matrizes são apresentadas abaixo.
1. Seja y  = tr(XA) sendo X p x  p e definida positiva e A p x  p constantes. Então,
$$
\partial y 
\partial X = \partial tr(XA)
\partial X = A + A⊤ − diag(A).
$$
2. Sendo A não singular com derivadas \partial A
$$
\partial x  . Então,
\partial A−1
\partial x  = −A−1 \partial A
\partial x  A−1.
$$
3. Sendo A n x  n positiva definida. Então,
$$
\partial  log |A|
\partial x  = tr (A−1 \partial A
\partial x  )
$$

## Regressão linear múltipla

### Regressão linear múltipla: especificação usual

- Regressão linear simples
$$
y_{i} = \beta_{0} +\beta_{1}x_{1} + erro_{i}
$$
- Regressão linear múltipla
$$
y_{i} = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2} + ... + \beta_{p}x_{ip} + erro_{i}
$$
- Modelo para cada observação
$$y_{1} = \beta_{0} + \beta_{1}x_{11} + \beta_{2}x_{12} + ... + \beta_{p}x_{1p} + erro_{1}$$

$$y_{2} = \beta_{0} + \beta_{1}x_{21} + \beta_{2}x_{22} + ... + \beta_{p}x_{2p} + erro_{1}$$
$$...$$
$$y_{n} = \beta_{0} + \beta_{1}x_{n1} + \beta_{2}x_{n2} + ... + \beta_{p}x_{np} + erro_{n}$$

Regressão linear múltipla: especificação matricial
- Notação matricial
$$
\begin{bmatrix}
y_{1}\\
y_{2}\\
...\\
y_{n}\\
\end{bmatrix}
= 
\begin{bmatrix}
1 & x_{1}\\
1 & x_{2}\\
1 & ...\\
1 & x_{n}\\
\end{bmatrix}
x 
\begin{bmatrix}
\beta_{1}\\
\beta_{2}\\
...\\
\beta_{n}\\
\end{bmatrix}
+
\begin{bmatrix}
erro_{1}\\
erro_{2}\\
...\\
erro_{n}\\
\end{bmatrix}
$$

- Notação mais compacta
$$
y_{(n x  1)} = X_{(n x  p)} \beta_{(p x  1)} + erro_{(n x  1)}
$$
### Regressão linear múltipla: estimação (treinamento)
- Objetivo: encontrar o vetor̂ \beta , tal que $𝑆𝑄(\beta ) = (y − X\beta )⊤(y − X\beta ) $ seja a menor possível.

### Regressão linear múltipla: estimação
1. Passo 1: encontrar o vetor gradiente. Derivando em \beta , temos
$$
\partial 𝑆𝑄(\beta )
\partial \beta  = \partial 
\partial \beta  (y − X\beta )⊤(y − X\beta )
= \partial 
\partial \beta  ((y − X\beta )⊤) (y − X\beta ) + (y − X\beta )⊤ \partial 
\partial \beta  (y − X\beta )
= −X⊤(y − X\beta ) + (y − X\beta )⊤(−X)
= −2X⊤(y − X\beta ).
$$

### Regressão linear múltipla: estimação

2. Passo 2: resolver o sistema de equações lineares (esquece o "-2" primeiro)
$$ X^{T} (y - X\hat{\beta}) = 0$$
$$X⊤y − X⊤X̂\beta  = 0$$
$$X⊤X̂\beta  = X⊤ŷ$$
$$(XTX)^{-1}  X⊤X̂\beta  = X⊤y (XTX)^{-1}$$
$$ I\beta  = (X⊤X)−1X⊤y $$

### Regressão linear múltipla: exemplo
- Conjunto de dados Boston disponível no pacote MASS.
- Cinco primeiras covariáveis disponíveis:
  - crim: taxa de crimes per capita.
  - zn: proporção de terrenos residenciais zoneados para lotes com mais de 25.000 pés quadrados.
  - indus: proporção de acres de negócios não varejistas por cidade.
  - chas: variável dummy de Charles River (1 se a área limita o rio; 0 caso contrário).
  - nox: concentração de óxido de nitrogênio (parte por 10 milhões).
- Variável resposta: medv valor mediano das casas ocupadas em $1000.

### Regressão linear múltipla: implementação computacional
- Carregando a base de dados 
```{r}
require(MASS)
## Carregando pacotes exigidos: MASS

data(Boston)
head(Boston[, c(1:5,14)])
## crim zn indus chas nox medv
## 1 0.00632 18 2.31 0 0.538 24.0
## 2 0.02731 0 7.07 0 0.469 21.6
## 3 0.02729 0 7.07 0 0.469 34.7
## 4 0.03237 0 2.18 0 0.458 33.4
## 5 0.06905 0 2.18 0 0.458 36.2
## 6 0.02985 0 2.18 0 0.458 28.7
```


- Matriz de delineamento (X).
```{r}
X <- model.matrix(~ crim + zn + indus +
chas + nox, data = Boston)
head(X)
## (Intercept) crim zn indus chas nox
## 1 1 0.00632 18 2.31 0 0.538
## 2 1 0.02731 0 7.07 0 0.469
## 3 1 0.02729 0 7.07 0 0.469
## 4 1 0.03237 0 2.18 0 0.458
## 5 1 0.06905 0 2.18 0 0.458
## 6 1 0.02985 0 2.18 0 0.458
```


- Variável resposta
```{r}
y <- Boston$medv
```

- Estimadores de mínimos quadrados:
$$
\hat{\beta} = (X^{T}X)^{-1} X^{T}y
$$
- Computacionalmente: versão ingênua (calcula inversa)
```{r}
round(solve(t(X)%*%X)%*%t(X)%*%y, 2)
## [,1]
## (Intercept) 29.49
## crim -0.22
## zn 0.06
## indus -0.38
## chas 7.03
## nox -5.42
```

- Computacionalmente: versão eficiente (escalona?)
```{r}
round(solve(t(X)%*%X, t(X)%*%y), 2)
## [,1]
## (Intercept) 29.49
## crim -0.22
## zn 0.06
## indus -0.38
## chas 7.03
## nox -5.42
```

- Função nativa do R
```{r}
t(round(coef(lm(medv ~ crim + zn + indus + chas + nox, data = Boston)), 2))
## (Intercept) crim zn indus chas nox
## [1,] 29.49 -0.22 0.06 -0.38 7.03 -5.42
```

### Matrizes esparsas (tópico adicional)

- Matrizes aparecem em todos os tipos de aplicação em ciência de dados.
- Modelos estatísticos, machine learning, análise de texto, análise de cluster, etc.
- Muitas vezes as matrizes usadas têm uma grande quantidade de zeros.
- Quando uma matriz tem uma quantidade considerável de zeros, dizemos que ela é
esparsa, caso contrário dizemos que a matriz é densa.
- Todas as propriedades que vimos para matrizes em geral valem para matrizes esparsas.
- O R tem um conjunto de métodos altamente eficiente por meio do pacote Matrix.
- Saber que uma matriz é esparsa é útil pois permite:
- Planejar formas de armazenar a matriz em memória.
- Economizar cálculos em algoritmos numéricos (multiplicação, inversa, determinante,
decomposições, etc).

- Comparando a quantidade de memória utilizada.
```{r}
library('Matrix')

m1 <- matrix(0, nrow = 1000, ncol = 1000)
object.size(m1)
## 8000216 bytes

m2 <- Matrix(0, nrow = 1000, ncol = 1000, sparse = TRUE)
object.size(m2)
## 9240 bytes
```


Comparando o tempo computacional


- Matriz densa
```{r}
y <- rnorm(1000)
X <- matrix(NA, ncol = 100, nrow = 1000)
for(i in 1:1000) {X[i,] <- rbinom(100, size = 1, p = 0.1)}
system.time(replicate(100, solve(t(X)%*%X, t(X)%*%y)))
## usuário sistema decorrido
## 0.819 0.004 0.823
```


- Matriz esparsa
```{r}
y <- rnorm(1000)
X <- Matrix(NA, ncol = 100, nrow = 1000)
for(i in 1:1000) {X[i,] <- rbinom(100, size = 1, p = 0.1)}
X <- Matrix(X, sparse = TRUE)
system.time(replicate(100, solve(t(X)%*%X, t(X)%*%y)))
## usuário sistema decorrido
## 0.223 0.000 0.224
```

### Diferentes formas de implementar as operações matriciais

- Criando a base de dados para a comparação
```{r}
library(Matrix)
n <- 10000; p <- 500

#DENSA
x <- matrix(rbinom(n*p, 1, 0.01), nrow=n, ncol=p)
object.size(x)
## 20000216 bytes

#ESPARCA
X <- Matrix(x)
object.size(X)
## 600432 bytes
```

- Diferentes implementações
```{r}
y <- rnorm(n)

print("Matriz densa com %*%:")
system.time(solve(t(x)%*%x, t(x)%*%y))
## usuário sistema decorrido
## 2.053 0.040 2.094

print("Matriz densa com crossprod")
system.time(solve(crossprod(x), crossprod(x, y)))
## usuário sistema decorrido
## 1.731 0.016 1.748

print("Matriz esparça com %*%")
system.time(solve(t(X)%*%X, t(X)%*%y))
## usuário sistema decorrido
## 0.071 0.000 0.072

print("Matriz esparça com crossprod")
system.time(solve(crossprod(X), crossprod(X,y)))
## usuário sistema decorrido
## 0.029 0.000 0.050
```

- Implementação eficiente do modelo de regressão linear múltipla.
```{r}
library(glmnet)
## Loaded glmnet 4.1-6
system.time(b <- coef(lm(y~x)))
## usuário sistema decorrido
## 2.389 0.044 2.434
system.time(g1 <-glmnet(x, y, nlambda=1, lambda=0, standardize=FALSE))
## usuário sistema decorrido
## 0.065 0.020 0.086
system.time(g2 <- glmnet(X, y, nlambda=1, lambda=0, standardize=FALSE))
## usuário sistema decorrido
## 0.006 0.000 0.006
```


# Proxima aula

### Sistemas lineares
- Sistema com duas equações:
$$ f 1(x 1,x 2) = 0$$
$$f 2(x 1,x 2) = 0$$
- Solução numérica consiste em encontrar̂ x 1 ê x 2 que satisfaça o sistema de equações.
- Sistema com n equações
$$f 1(x 1, … , x n) = 0
⋮
f n(x 1, … , x n) = 0.
- Genericamente, tem-se
f(x ) = 0.$$

- Equações podem ser lineares ou não-lineares.

Sistemas de equações lineares
- Cada equação é linear na incógnita.
- Solução analítica em geral é possível.
- Exemplo:
7x 1 + 3x 2 = 45
4x 1 + 5x 2 = 29.
- Solução analítica:̂ x 1 = 6 ê x 2 = 1.
- Resolver (tedioso!!).
- Três possíveis casos:
1. Uma única solução (sistema não singular).
2. Infinitas soluções (sistema singular).
3. Nenhuma solução (sistema impossível).

Sistemas de equações lineares
- Representação matricial do sistema de equações lineares:
A = [7 3
4 5] , x = [x 1
x 2
] e b = [45
29] .
- De forma geral, tem-se
Ax = b.

Operações com linhas
- Sem qualquer alteração na relação linear, é possível
1. Trocar a posição de linhas:
4x 1 + 5x 2 = 29
7x 1 + 3x 2 = 45.
2. Multiplicar qualquer linha por uma constante, aqui 4x 1 + 5x 2 por 1
4 , obtendo
x 1 + 5
4 x 2 = 29
4 (1)
7x 1 + 3x 2 = 45. (2)

Operações com linhas
3. Subtrair um múltiplo de uma linha de uma outra, aqui 7 ∗ 𝐸𝑞.(1) menos Eq. (2), obtendo
x 1 + 5
4 x 2 = 29
4
0x 1 + ( 35
4 − 3)x 2 = 203
4 − 45.
- Fazendo as contas, tem-se
0x 1 + 23
4 x 2 = 23
4 .

Solução de sistemas lineares
- Forma geral de um sistema com n equações lineares:
a11x 1 + a12x 2 + … + a1nx n = b 1
a21x 1 + a22x 2 + … + a2nx n = b 2
⋮
an1x 1 + an2x 2 + … + annx n = b n
- Matricialmente, tem-se
⎡
⎢
⎢
⎣
a11 a12 … a1n
a21 a22 … a2n
⋮ ⋮ ⋮ …
an1 an2 … ann
⎤
⎥
⎥
⎦
⎡
⎢
⎢
⎣
x 1
x 2
⋮
x n
⎤
⎥
⎥
⎦
= ⎡
⎢
⎢
⎣
b 1
b 2
⋮
b n
⎤
⎥
⎥
⎦
- Métodos diretos e métodos iterativos.

### Métodos diretos

- O sistema de equações é manipulado até se transformar em um sistema equivalente de
fácil resolução.
- Triangular superior:
⎡
⎢
⎢
⎣
a11 a12 a13 a14
0 a22 a23 a24
0 0 a33 a34
0 0 0 a44
⎤
⎥
⎥
⎦
⎡
⎢
⎢
⎣
x 1
x 2
x 3
x 4
⎤
⎥
⎥
⎦
= ⎡
⎢
⎢
⎣
b 1
b 2
b 3
b 4
⎤
⎥
⎥
⎦
.
- Substituição regressiva
x n = b n
ann
x i  = b i  − \sum{}j =n
j =i +1 ai j x j 
ai i 
, i  = n − 1, n − 2, … , 1.

Métodos diretos
- Triangular inferior:
⎡
⎢
⎢
⎣
a11 0 0 0
a21 a22 0 0
a31 a32 a33 0
a41 a42 a43 a44
⎤
⎥
⎥
⎦
⎡
⎢
⎢
⎣
x 1
x 2
x 3
x 4
⎤
⎥
⎥
⎦
= ⎡
⎢
⎢
⎣
b 1
b 2
b 3
b 4
⎤
⎥
⎥
⎦
.
- Substituição progressiva
x 1 = b 1
a11
x i  = b i  − \sum{}j =i −1
j =i  ai j x j 
ai i 
, i  = 2, 3, … , n.


Métodos diretos
- Diagonal:
⎡
⎢
⎢
⎣
a11 0 0 0
0 a22 0 0
0 0 a33 0
0 0 0 a44
⎤
⎥
⎥
⎦
⎡
⎢
⎢
⎣
x 1
x 2
x 3
x 4
⎤
⎥
⎥
⎦
= ⎡
⎢
⎢
⎣
b 1
b 2
b 3
b 4
⎤
⎥
⎥
⎦
.

Eliminação de Gauss

Métodos diretos: Eliminação de Gauss
- Método de Eliminação de Gauss consiste em manipular o sistema original usando
operações de linha até obter um sistema triangular superior.
[
a11 a12 a13 a14
a21 a22 a23 a24
a31 a23 a33 a34
a41 a24 a34 a44
] [
x 1
x 2
x 3
x 4
] = [
b 1
b 2
b 3
b 4
] → [
a11 a12 a13 a14
0 a′
22 a′
23 a′
24
0 0 a′
33 a′
34
0 0 0 a′
44
] [
x 1
x 2
x 3
x 4
] = [
b 1
b ′
2
b ′
3
b ′
4
]
- Usar eliminação regressiva no novo sistema para obter a solução.
- Resolva o seguinte sistema usando Eliminação de Gauss.
⎡
⎢
⎣
3 2 6
2 4 3
5 3 4
⎤
⎥
⎦
⎡
⎢
⎣
x 1
x 2
x 3
⎤
⎥
⎦
= ⎡
⎢
⎣
24
23
33
⎤
⎥
⎦

Métodos diretos: Eliminação de Gauss
- Passo 1: encontrar o pivô e eliminar os elementos abaixo dele usando operações de linha.
[ [3] 2 6
2 − 2
3 3 4 − 2
3 2 3 − 2
3 6
5 − 5
3 3 3 − 5
3 2 4 − 5
3 6
] [ 24
23 − 2
3 24
33 − 5
3 24
] → [[3] 2 6
0 8
3 −1
0 − 1
3 −6
] [24
7
−7]
- Passo 2: encontrar o segundo pivô e eliminar os elementos abaixo dele usando operações
de linha.
[3 2 6
0 [ 8
3 ] −1
0 − 1
3 − (− 3
24 ) ( 8
3 ) −6 − (− 3
24 )(−1)
] [ 24
7
−7 − (− 3
24 )(7)] → [3 2 6
0 [ 8
3 ] −1
0 0 − 147
24
] [ 24
7
− 147
24
]
- Passo 3: substituição regressiva.

Métodos diretos: Eliminação de Gauss
- Usando a fórmula de substituição regressiva temos:
- x 3 = b 3
a33
= 1.
- x 2 = b 2−a23x 3
a22
= 3.
- x 1 = (b 1−(a12x 2+a13x 3)
a11
= 4.
- A extensão do procedimento para um sistema com n equações é trivial.
1. Transforme o sistema em triangular superior usando operações linhas.
2. Resolva o novo sistema usando substituição regressiva.
- Potenciais problemas do método de eliminação de Gauss:
- O elemento pivô é zero.
- O elemento pivô é pequeno em relação aos demais termos.

Eliminação de Gauss com pivotação

Eliminação de Gauss com pivotação
- Considere o sistema
0x 1 + 2x 2 + 3x 2 = 46
4x 1 − 3x 2 + 2x 3 = 16
2x 1 + 4x 2 − 3x 3 = 12
- Neste caso o pivô é zero e o procedimento não pode começar.
- Pivotação - trocar a ordem das linhas.
1. Evitar pivôs zero.
2. Diminuir o número de operações necessárias para triangular o sistema.
4x 1 − 3x 2 + 2x 3 = 16
2x 1 + 4x 2 − 3x 3 = 12
0x 1 + 2x 2 + 3x 2 = 46

Eliminação de Gauss com pivotação
- Se durante o procedimento uma equação pivô tiver um elemento nulo e o sistema tiver
solução, uma equação com um elemento pivô diferente de zero sempre existirá.
- Cálculos numéricos são menos propensos a erros e apresentam menores erros de
arredondamento se o elemento pivô for grande em valor absoluto.
- É usual ordenar as linhas para que o maior valor seja o primeiro pivô.

Passo 1: obtendo uma matriz triangular superior.
gauss <- function(A, b) {
Ae <- cbind(A, b) ## Sistema aumentado
rownames(Ae) <- paste0("x", 1:length(b))
n_row <- nrow(Ae)
n_col <- ncol(Ae)
SOL <- matrix(NA, n_row, n_col) ## Matriz para receber os resultados
SOL[1,] <- Ae[1,]
pivo <- matrix(0, n_col, n_row)
for(j in 1:c(n_row-1)) {
for(i in c(j+1):c(n_row)) {
pivo[i,j] <- Ae[i,j]/SOL[j,j]
SOL[i,] <- Ae[i,] - pivo[i,j]*SOL[j,]
Ae[i,] <- SOL[i,]
}
}
return(SOL)
}

Eliminação de Gauss sem pivotação
- Passo 2: substituição regressiva
sub_reg <- function(SOL) {
n_row <- nrow(SOL)
n_col <- ncol(SOL)
A <- SOL[1:n_row,1:n_row]
b <- SOL[,n_col]
n <- length(b)
x <- c()
x[n] <- b[n]/A[n,n]
for(i in (n-1):1) {
x[i] <- (b[i] - sum(A[i,c(i+1):n]*x[c(i+1):n] ))/A[i,i]
}
return(x)
}

Eliminação de Gauss sem pivotação
- Resolva o sistema:
⎡
⎢
⎣
3 2 6
2 4 3
5 3 4
⎤
⎥
⎦
⎡
⎢
⎣
x 1
x 2
x 3
⎤
⎥
⎦
= ⎡
⎢
⎣
24
23
33
⎤
⎥
⎦
.
A <- matrix(c(3,2,5,2,4,3,6,3,4),3,3)
b <- c(24,23,33)
S <- gauss(A, b) ## Passo 1: Triangularização
sol = sub_reg(SOL = S) ## Passo 2: Substituição regressiva
sol
## [1] 4 3 1
A%*%sol ## Verificando a solução
## [,1]
## [1,] 24
## [2,] 23
## [3,] 33

Eliminação de Gauss com pivotação
- Resolva o seguinte sistema usando
Eliminação de Gauss com pivotação.
0x 1 + 2x 2 + 3x 2 = 46
4x 1 − 3x 2 + 2x 3 = 16
2x 1 + 4x 2 − 3x 3 = 12
## Entrando com o sistema original
A <- matrix(c(0,4,2,2,-3,4,3,2,-3), 3,3)
b <- c(46,16,12)
## Pivoteamento
A_order <- A[order(A[,1], decreasing = TRUE),]
b_order <- b[order(A[,1], decreasing = TRUE)]
#### Triangulação
S <- gauss(A_order, b_order)
S
## [,1] [,2] [,3] [,4]
## [1,] 4 -3.0 2.000000 16.00000
## [2,] 0 5.5 -4.000000 4.00000
## [3,] 0 0.0 4.454545 44.54545
#### Substituição regressiva
sol <- sub_reg(SOL = S)
sol
## [1] 5 8 10
#### Solução
A_order%*%sol
## [,1]
## [1,] 16
## [2,] 12
## [3,] 46

Eliminação de Gauss-Jordan

Métodos diretos: Eliminação de Gauss-Jordan
- O sistema original é manipulado até obter um sistema equivalente na forma diagonal.
[
a11 a12 a13 a14
a21 a22 a23 a24
a31 a23 a33 a34
a41 a24 a34 a44
] [
x 1
x 2
x 3
x 4
] = [
b 1
b 2
b 3
b 4
] → [
1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 1
] [
x 1
x 2
x 3
x 4
] = [
b ′
1
b ′
2
b ′
3
b ′
4
]
- Algoritmo Gauss-Jordan
1. Normalize a equação pivô com a divisão de todos os seus termos pelo coeficiente pivô.
2. Elimine os elementos fora da diagonal principal em TODAS as demais equações usando
operaçõs de linha.
- O método de Gauss-Jordan pode ser combinado com pivotação igual ao método de
eliminação de Gauss.


Métodos iterativos
- Nos métodos iterativos, as equações são colocadas em uma forma explícita onde cada
incógnita é escrita em termos das demais, i.e.
a11x 1 + a12x 2 + a13x 3 = b 1
a21x 1 + a22x 2 + a23x 3 = b 2
a31x 1 + a32x 2 + a33x 3 = b 3
→
x 1 = [b 1 − (a12x 2 + a13x 3)]/a11
x 2 = [b 2 − (a21x 1 + a23x 3)]/a22
x 3 = [b 3 − (a31x 1 + a32x 2)]/a33
.
- Dado um valor inicial para as incógnitas estas serão atualizadas até a convergência.
- Atualização: Método de Jacobi
x i  = 1
ai i 
[b i  − (
j =n
\sum{}
j =1;j ≠i 
ai j x j )] i  = 1, … , n.

- Atualização: Método de Gauss-Seidel
x 𝑘+1
1 = 1
a11
[b 1 −
j =n
\sum{}
j =2
a1j x (𝑘)
j  ] ,
x (𝑘+1)
i  = 1
ai i 
[b i  − (
j =i −1
\sum{}
j =1
ai j x (𝑘+1)
j  +
j =n
\sum{}
j =i +1
ai j x (𝑘)
j  )] i  = 2, 3, … , n − 1 e
x (𝑘+1)
n = 1
ann
[b n −
j =n−1
\sum{}
j =1
anj x (𝑘+1)
j  ] .

Método iterativo de Jacobi
- Implementação computacional
jacobi <- function(A, b, inicial, max_iter = 10, tol = 1e-04) {
n <- length(b)
x_temp <- matrix(NA, ncol = n, nrow = max_iter)
x_temp[1,] <- inicial
x <- x_temp[1,]
for(j in 2:max_iter) { #### Equação de atualização
for(i in 1:n) {
x_temp[j,i] <- (b[i] - sum(A[i,1:n][-i]*x[-i]))/A[i,i]
}
x <- x_temp[j,]
if(sum(abs(x_temp[j,] - x_temp[c(j-1),])) < tol) break #### Critério de parada
}
return(list("Solucao" = x, "Iteracoes" = x_temp))
}

Método iterativo de Jacobi
- Resolva o seguinte sistema de equações
lineares usando o método de Jacobi.
9x 1 − 2x 2 + 3x 3 + 2x 4 = 54.5
2x 1 + 8x 2 − 2x 3 + 3x 4 = −14
−3x 1 + 2x 2 + 11x 3 − 4x 4 = 12.5
−2x 1 + 3x 2 + 2x 3 − 10x 4 = −21
- Computacionalmente
A <- matrix(c(9,2,-3,-2,-2,8,2,
3,3,-2,11,2,2,3,-4,10),4,4)
b <- c(54.5, -14, 12.5, -21)
ss <- jacobi(A = A, b = b,
inicial = c(0,0,0,0),
max_iter = 15)
## Solução aproximada
ss$Solucao
## [1] 4.999502 -1.999771 2.500056 -1.000174
## Solução exata
solve(A, b)
## [1] 5.0 -2.0 2.5 -1.0

Métodos iterativo de Jacobi e Gauss-Seidel
- Em R o pacote Rlinsolve fornece
implementações eficientes dos métodos
de Jacobi e Gauss-Seidel.
- Rlinsolve inclui suporte para matrizes
esparsas via Matrix.
- Rlinsolve é implementado em C++
usando o pacote Rcpp.
A <- matrix(c(9,2,-3,-2,-2,8,2,3,3,-2,11,
2,2,3,-4,10),4,4)
b <- c(54.5, -14, 12.5, -21)
## pacote extra
require(Rlinsolve)
lsolve.jacobi(A, b)$x ## Método de jacobi
## [,1]
## [1,] 4.9999708
## [2,] -2.0000631
## [3,] 2.5000163
## [4,] -0.9999483
lsolve.gs(A, b)$x ## Método de Gauss-Seidell
## [,1]
## [1,] 4.999955
## [2,] -2.000071
## [3,] 2.500018
## [4,] -0.999968


### Decomposição LU
- Nos métodos de eliminação de Gauss e Gauss-Jordan resolvemos sistemas do tipo
$$ Ax  = b .$$
- Sendo dois sistemas
$$Ax  = b_1, e \space Ax  = b_2$$
- Cálculos do primeiro não ajudam a resolver o segundo.
- IDEAL! - Operações realizadas em A fossem dissociadas das operações em b .

Decomposição LU
- Suponha que precisamos resolver vários sistemas do tipo
Ax  = b .
para diferentes b ′𝑠.
- Opção 1 - calcular a inversa A−1, assim a solução
x  = A−1b .
- Cálculo da inversa é computacionalmente ineficiente.

Decomposição LU: algoritmo
- Decomponha (fatore) a matriz A em um produto de duas matrizes
A = LU,
onde L é triangular inferior e U é triangular superior.
- Baseado na decomposição o sistema tem a forma:
LUx  = b . (3)
- Defina Ux  = y .
- Substituindo em 3 tem-se
Ly  = b . (4)
- Solução é obtida em dois passos
- Resolva Eq.(4) para obter y  usando substituição progressiva.
- Resolva Eq.(3) para obter x  usando substituição regressiva.

Obtendo as matrizes L e U
- Método de eliminação de Gauss e método de Crout.
- Dentro do processo de eliminação de Gauss as matrizes L e U são obtidas como um
subproduto, i.e.
[
a11 a12 a13 a14
a21 a22 a23 a24
a31 a32 a33 a34
a41 a41 a43 a44
] = [
1
𝑚21 1
𝑚31 𝑚32 1
𝑚41 𝑚42 𝑚43 1
] [
a11 a12 a13 a14
0 a′
22 a′
23 a′
24
0 0 a′
33 a′
34
0 0 0 a′
44
] .
- Os elementos 𝑚′
i j 𝑠 são os multiplicadores que multiplicam a equação pivô.

Obtendo as matrizes L e U
- Relembre o exemplo de eliminação de Gauss.
$$
[ [3] 2 6
2 − 2
3 3 4 − 2
3 2 3 − 2
3 6
5 − 5
3 3 3 − 5
3 2 4 − 5
3 6
] [ 24
23 − 2
3 24
33 − 5
3 24
] → [[3] 2 6
0 8
3 −1
0 − 1
3 −6
] [24
7
−7]
[3 2 6
0 [ 8
3 ] −1
0 − 1
3 − (− 3
24 ) ( 8
3 ) −6 − (− 3
24 )(−1)
] [ 24
7
−7 − (− 3
24 )(7)] → [3 2 6
0 [ 8
3 ] −1
0 0 − 147
24
] [ 24
7
− 147
24
]
- Neste caso, tem-se
L = ⎡
⎢
⎣
1
2
3 1
5
3 − 3
24 1
⎤
⎥
⎦
e U = ⎡
⎢
⎣
3 2 6
0 8
3 −1
0 0 − 147
24
⎤
⎥
⎦
$$

Decomposição LU com pivotação
- O método de eliminação de Gauss foi realizado sem pivotação.
- Como discutido a pivotação pode ser necessária.
- Quando realizada a pivotação as mudanças feitas devem ser armazenadas, tal que
PA = LU.
- P é uma matriz de permutação.
- Se as matrizes LU forem usadas para resolver o sistema
Ax  = b ,
então a ordem das linhas de b  deve ser alterada de forma consistente com a pivotação,
i.e. Pb .

Implementação: Decomposição LU
- Podemos facilmente modificar a função gauss() para obter a decomposição LU.
my_lu <- function(A) {
n_row <- nrow(A)
n_col <- ncol(A)
SOL <- matrix(NA, n_row, n_col) ## Matriz para receber os resultados
SOL[1,] <- A[1,]
pivo <- matrix(0, n_col, n_row)
for(j in 1:c(n_row-1)) {
for(i in c(j+1):c(n_row)) {
pivo[i,j] <- A[i,j]/SOL[j,j]
SOL[i,] <- A[i,] - pivo[i,j]*SOL[j,]
A[i,] <- SOL[i,]
}
}
diag(pivo) <- 1
return(list("L" = pivo, "U" = SOL)) }

Aplicação: Decomposição LU
- Fazendo a decomposição.
LU <- my_lu(A) ## Decomposição
LU
## $L
## [,1] [,2] [,3] [,4]
## [1,] 1.0000000 0.0000000 0.000000 0
## [2,] 0.2222222 1.0000000 0.000000 0
## [3,] -0.3333333 0.1578947 1.000000 0
## [4,] -0.2222222 0.3026316 0.279661 1
##
## $U
## [,1] [,2] [,3] [,4]
## [1,] 9 -2.000000e+00 3.000000 2.000000
## [2,] 0 8.444444e+00 -2.666667 2.555556
## [3,] 0 0.000000e+00 12.421053 -3.736842
## [4,] 0 -4.440892e-16 0.000000 10.716102
LU$L %*% LU$U ## Verificando a solução
## [,1] [,2] [,3] [,4]
## [1,] 9 -2 3 2
## [2,] 2 8 -2 3
## [3,] -3 2 11 -4
## [4,] -2 3 2 10

Aplicação: Decomposição LU
- Resolvendo o sistema de equações.
## Passo 1: Substituição progressiva
y = forwardsolve(LU$L, b)
## Passo 2: Substituição regressiva
x = backsolve(LU$U, y)
x
## [1] 5.0 -2.0 2.5 -1.0
A%*%x ## Verificando a solução
## [,1]
## [1,] 54.5
## [2,] -14.0
## [3,] 12.5
## [4,] -21.0
- Função lu() do Matrix fornece a
decomposição LU.
require(Matrix)
## Calcula mas não retorna
LU_M <- lu(A)
## Captura as matrizes L U e P
LU_M <- expand(LU_M)
## Substituição progressiva.
y <- forwardsolve(LU_M$L, LU_M$P%*%b)
## Substituição regressiva
x = backsolve(LU_M$U, y)
x
## [1] 5.0 -2.0 2.5 -1.0

## Obtendo a inversa

### Obtendo a inversa via decomposição LU
- O método LU é especialmente adequado para o cálculo da inversa.
- Lembre-se que a inversa de A é tal que
AA−1 = I.
- O procedimento de cálculo da inversa é essencialmente o mesmo da solução de um
sistema de equações lineares, porém com mais incognitas.
⎡
⎢
⎣
a11 a12 a13
a21 a22 a23
a31 a32 a33
⎤
⎥
⎦
⎡
⎢
⎣
x 11 x 12 x 13
x 21 x 22 x 23
x 31 x 32 x 33
⎤
⎥
⎦
= ⎡
⎢
⎣
1 0 0
0 1 0
0 0 1
⎤
⎥
⎦
- Três sistemas de equações diferentes, em cada sistema, uma coluna da matriz X é a
incognita.

Implementação: inversa via decomposição LU
- Função para resolver o sistema usando
decomposição LU.
```{r}
solve_lu <- function(LU, b) {
  y <- forwardsolve(LU_M$L, LU_M$P%*%b)
  x = backsolve(LU_M$U, y)
  return(x)
}
```


- Resolvendo vários sistemas
```{r}
my_solve <- function(LU, B) {
  n_col <- ncol(B)
  n_row <- nrow(B)
  inv <- matrix(NA, n_col, n_row)
  for(i in 1:n_col) {
    inv[,i] <- solve_lu(LU, B[,i])
  }
  return(inv)
}
```


Aplicação: inversa via decomposição LU
- Calcule a inversa de
A = ⎡
⎢
⎣
3 2 6
2 4 3
5 3 4
⎤
⎥
⎦
A <- matrix(c(3,2,5,2,4,3,6,3,4),3,3)
I <- Diagonal(3, 1)
## Decomposição LU
LU <- my_lu(A)
## Obtendo a inversa
inv_A <- my_solve(LU = LU, B = I)
inv_A
## Verificando o resultado
A%*%inv_A

Cálculo da inversa via método de Gauss-Jordan
- Procedimento Gauss-Jordan:
⎡
⎢
⎣
a11 a21 a31 1 0 0
a21 a22 a32 0 1 0
a31 a32 a33 0 0 1
⎤
⎥
⎦
→ ⎡
⎢
⎣
1 0 0 a′
11 a′
21 a′
31
0 1 0 a′
21 a′
22 a′
32
0 0 1 a′
31 a′
32 a′
33
⎤
⎥
⎦

- Função solve() usa a decomposição LU com pivotação.
- R básico é construído sobre a biblioteca lapack escrita em C.
- Veja documentação em http://www.netlib.org/lapack/lug/node38.html.

Autovalores e autovetores
- Redução de dimensionalidade é fundamental em ciência de dados.
- Análise de componentes principais (PCA)
- Análise fatorial (AF).
- Decompor grandes e complicados relacionamentos multivariados em simples
componentes não relacionados.
- Vamos discutir apenas os aspectos matemáticos.

Intuição
- Podemos decompor um vetor $\upsilon$  em duas informações separadas: direção $d$ e tamanho $\lambda $, i.e

$$\lambda  = ||\upsilon || = \sqrt \sum{}j 𝜈2j  , e d = \upsilon \lambda $$
- É mais fácil interpretar o tamanho de um vetor enquanto ignorando a sua direção e
vice-versa.
- Esta ideia pode ser estendida para matrizes.
- Uma matriz nada mais é do que um conjunto de vetores.
- IDEIA - decompor a informação de uma matriz em outros componentes de mais fácil
interpretação/representação matemática.

Autovalores e Autovetores
- Autovalores e autovetores são definidos por uma simples igualdade
A\upsilon  = \lambda \upsilon . (5)
- Os vetores \upsilon ’s que satisfazem Eq. (5) são os autovetores.
- Os valores \lambda ’s que satisfazem Eq. (5) são os autovalores.
- Vamos considerar o caso em que A é simétrica.
- A ideia pode ser estendida para matrizes não simétricas.


- Se A é uma matriz simétrica n x  n, então existem exatamente n pares (\lambda j , \upsilon j ) que
satisfazem a equação:
A\upsilon  = \lambda \upsilon .
- Se A tem autovalores \lambda 1, … , \lambda n, então:
- tr(A) = \sum{}n
i =1 \lambda i .
- det(A) = ∏n
i =1 \lambda i .
- A é positiva definida, se e somente se todos \lambda j  > 0.
- A é semi-positiva definida, se e somente se todos \lambda j  ≥ 0.
- A ideia do PCA é decompor/fatorar a matriz A em componentes mais simples de
interpretar.

Decomposição em autovalores e autovetores
- Teorema: qualquer matriz simétrica A pode ser fatorada em
A = Q\lambda Q⊤,
onde \lambda  é diagonal contendo os autovalores de A e as colunas de Q contêm os autovetores
ortonormais.
- Vetores ortonormais: são mutuamente ortogonais e de comprimento unitário.
- Teorema: se A tem autovetores Q e autovalores \lambda j . Então A−1 tem autovetores Q e
autovalores \lambda −1
j  .
- Implicação: se A = Q\lambda Q⊤ então A−1 = Q\lambda −1Q⊤.

Diagonalização
- Autovalores são utéis porque eles permitem lidar com matrizes da mesma forma que
lidamos com números.
- Todos os cálculos são feitos na matriz diagonal \lambda .
- Este processo é chamado de diagonalização.
- Um dos resultados mais poderosos em Álgebra Linear é que qualquer matriz pode ser
diagonalizada.
- O processo de diagonalização é chamado de Decomposição em valores singulares.

Decomposição em valores singulares (SVD)
- Teorema: qualquer matriz A pode ser decomposta em,
A = UDV⊤,
onde D é diagonal com entradas não negativas e U e V são ortogonais,
i.e. U⊤U = V⊤V = I.
- Matrizes não quadradas não tem autovalores.
- Os elementos de D são chamados de valores singulares.
- Os valores singulares são os autovalores de A⊤A.

### Dimensão da SVD
- Se A é n x  n, então U, D e V são n x  n.
- Se A é n x  p, sendo n > p, então U é n x  p, D e V são p x  p.
- Se A é n x  p, sendo n < p, então V⊤ é n x  p, D e U são n x  n.
- D será sempre quadrada com dimensão igual ao mínimo entre p e n.

### Decomposição em autovalores e autovetores em R
- Função eigen() fornece a decomposição
```{r}
A <- matrix(c(1,0.8, 0.3, 0.8, 1,
0.2, 0.3, 0.2, 1),3,3)
isSymmetric.matrix(A)
## [1] TRUE
out <- eigen(A)
Q <- out$vectors ## Autovetores
D <- diag(out$values) ## Autovalores
Q
## [,1] [,2] [,3]
## [1,] -0.6712373 -0.1815663 0.71866142
## [2,] -0.6507744 -0.3198152 -0.68862977
## [3,] -0.3548708 0.9299204 -0.09651322
- Verificando a solução
D
## [,1] [,2] [,3]
## [1,] 1.934216 0.0000000 0.0000000
## [2,] 0.000000 0.8726419 0.0000000
## [3,] 0.000000 0.0000000 0.1931419
Q%*%D%*%t(Q) ## Verificando
## [,1] [,2] [,3]
## [1,] 1.0 0.8 0.3
## [2,] 0.8 1.0 0.2
## [3,] 0.3 0.2 1.0
```


Decomposição em valores singulares em R
- Função svd() fornece a decomposição
```{r}
svd(A)
## $d
## [1] 1.9342162 0.8726419 0.1931419
##
## $u
## [,1] [,2] [,3]
## [1,] -0.6712373 0.1815663 0.71866142
## [2,] -0.6507744 0.3198152 -0.68862977
## [3,] -0.3548708 -0.9299204 -0.09651322
##
## $v
## [,1] [,2] [,3]
## [1,] -0.6712373 0.1815663 0.71866142
## [2,] -0.6507744 0.3198152 -0.68862977
## [3,] -0.3548708 -0.9299204 -0.09651322
```


### Regressão ridge
- Relembrando: regressão linear múltipla
⎡
⎢
⎢
⎣
y 1
y 2
⋮
y n
⎤
⎥
⎥
⎦
nx 1
= ⎡
⎢
⎢
⎣
1 x 11 … x p1
1 x 12 … x p1
⋮ ⋮ ⋱ ⋮
1 x 1n … x pn
⎤
⎥
⎥
⎦
nx p
⎡
⎢
⎢
⎣
\beta 0
\beta 1
⋮
\beta p
⎤
⎥
⎥
⎦
px 1
- Usando uma notação mais compacta,
y
nx 1
= X
nx p \beta 
px 1
.
- Minimiza a perda quadrática:̂
\beta  = (X⊤X)−1X⊤y.

### Regressão ridge
- Se p > n o sistema é singular (múltiplas soluções)!
- Como podemos ajustar o modelo?
- Introduzir uma penalidade pela complexidade.
- Soma de quadrados penalizada
𝑃 𝑆𝑄(\beta ) =
n
\sum{}
i =1
(y i  − x ⊤
i  \beta )2 + \lambda 
p
\sum{}
j =1
\beta 2
j  .
- Matricialmente, tem-se
𝑃 𝑆𝑄(\beta ) = (y  − X\beta )⊤(y  − X\beta ) + \lambda \beta ⊤\beta .
- IMPORTANTE !!
- y  centrado (média zero).
- X padronizada por coluna (média zero e variância um).

Regressão ridge
- Objetivo: minizar a soma de quadrados penalizada.
- Derivada
\partial 𝑃 𝑄𝑆(\beta )
\partial \beta  = \partial 
\partial \beta  [(y  − X\beta )⊤(y  − X\beta ) + \lambda \beta ⊤\beta ]
= [ \partial 
\partial \beta  (y  − X\beta )⊤] (y  − X\beta ) + (y  − X\beta )⊤ [ \partial 
\partial \beta  (y  − X\beta )] +
\lambda  {[ \partial \beta ⊤
\partial \beta  ] \beta  + \beta ⊤ [ \partial \beta 
\partial \beta  ]}
= −2X⊤(y  − X\beta ) + 2\lambda \beta 
= −X⊤(y  − X\beta ) + \lambda \beta .

Aplicação: regressão ridge
- Resolvendo o sistema linear, tem-se
−X⊤(y  − X̂\beta ) + \lambda Î \beta  = 0
−X⊤y  + X⊤X̂\beta  + \lambda Î \beta  = 0
X⊤X̂\beta  + \lambda Î \beta  = X⊤y 
(X⊤X + \lambda I)̂ \beta  = X⊤y ̂
\beta  = (X⊤X + \lambda I)−1 X⊤y .
- Solução depende de \lambda .
- A inclusão de \lambda  faz o sistema ser não singular.
- Na verdade quando fixamos \lambda  selecionamos uma solução em particular.

Aplicação: regressão ridge
- Calcular̂ \hat{\beta}  envolve a inversão de uma matriz p x  p potencialmente grande.
$$\hat{\beta}  = (X⊤X + \lambda I)−1 X⊤y$$
- Usando a decomposição SVD, tem-se
$$ X = UDV⊤$$
- É possível mostrar que,

$$ \hat{\beta} = Vdiag ( dj d2 j  + \lambda  ) U⊤y .$$

Implementação: regressão ridge
- Simulando o conjunto de dados (n = 100, p = 200).

```{r}
set.seed(123)
X <- matrix(NA, ncol = 200, nrow = 100)
X[,1] <- 1 ## Intercepto
for(i in 2:200) {
X[,i] <- rnorm(100, mean = 0, sd = 1)
X[,i] <- (X[,i] - mean(X[,i]))/var(X[,i])
}
## Parâmetros
beta <- rbinom(200, size = 1, p = 0.1)*rnorm(200, mean = 10)
mu <- X%*%beta
## Observações
y <- rnorm(100, mean = mu, sd = 10)
```


Implementando o modelo.
- Modelo passo-a-passo
```{r}
y_c <- y - mean(y)
X_svd <- svd(X) ## Decomposição svd
lambda = 0.5 ## Penalização
DD <- Diagonal(100, X_svd$d/(X_svd$d^2 + lambda))
DD[1] <- 0 ## Não penalizar o intercepto
beta_hat = as.numeric(X_svd$v%*%DD%*%t(X_svd$u)%*%y_c)
```

Resultados: regressão ridge
- Ajustados versus verdadeiros.
```{r}
plot(beta ~ beta_hat, xlab = expression(hat(beta)), ylab = expression(beta))
```

−4 −2 0 2 4 6 8
0 2 4 6 8 10 12
\beta 
^
\beta 

Resultados: regressão ridge
- Regressão com penalização ridge, bem como, outras penalizações são eficientemente
implementadas em R via pacote glmnet.
**IMPORTANTE!** A penalização no glmnet é ligeiramente diferente, por isso os $\hat{\beta}$’s não
são idênticos a nossa implementação naive.
- O glmnet oferece opções para selecionar $\lambda$ via validação cruzada.

```{r}
require(glmnet)
beta_glm <- cv.glmnet(X[,-1], y_c, nlambda = 100)
```



Resultados: regressão ridge
- Validação cruzada.
```{r}
plot(beta_glm)
```


800 1000 1200 1400 1600 1800
Log(\lambda  )
Mean−Squared Error
103 96 94 90 88 82 80 76 74 67 60 52 43 38 32 27 22 15 9 3 0

Resultados: regressão ridge
- Ajustados (glmnet) versus verdadeiros.
plot(beta ~ as.numeric(coef(beta_glm)), xlab = expression(hat(beta)), ylab = expression(beta))−2 0 2 4 6 8
0 2 4 6 8 10 12
\beta 
^
\beta 

Comentários
- Solução de sistemas lineares:
- Métodos diretos: Eliminação de Gauss e Gauss-Jordan.
- Métodos iterativos: Jacobi e Gauss-Seidel.
- Inversa de matrizes.
- Decomposição ou fatorização
- LU resolve sistema lineares pode ser usada para obter inversas.
- Autovalores e autovetores.
- Valores singulares.
- Existem muitas outras fatorizações: QR, Cholesky, Cholesky modificadas, etc.




```{r}

```

