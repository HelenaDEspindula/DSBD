


# Ãlgebra Matricial

## Vetores e escalares
- Um vetor Ã© uma lista de n nÃºmeros (escalares) escritos em linha ou coluna.
- NotaÃ§Ã£o (primeiro a em negrito)

$$
a = (a_{i1} ... a_{in})
$$
ou

$$
a = \begin{bmatrix}
a_{i1}\\
.\\
.\\
.\\
a_{in}\\
\end{bmatrix}
$$

- Vetor linha e vetor coluna.
- Um elemento do vetor Ã© chamado de ai , sendo i  a sua posiÃ§Ã£o.
- O tamanho de um vetor Ã© o seu nÃºmero de elementos.
- O mÃ³dulo de um vetor Ã© o seu comprimento
$$
|a| = \sqrt a2
1 + â€¦ + a2
n.
$$
- Vetor unitÃ¡rio Ã© aquele que tem tamanho
$$
a = a
|a| .
$$
- Dois vetores sÃ£o iguais se tem o mesmo
tamanho e os seus elementos em posiÃ§Ãµes
equivalentes sÃ£o iguais.

### OperaÃ§Ãµes com vetores

1. Soma $a + b = (ai  + b i ) = (a1 + b 1, â€¦ , an + b n)$.

$a = (1, 2, 3)$
$b = (3, 2, 1)$
$a+b = (4, 4, 4)$
$a-b = (-2, 0, 2)$

2. SubtraÃ§Ã£o $a âˆ’ b = (ai  âˆ’ b i ) = (a1 âˆ’ b 1, â€¦ , an âˆ’ b n)$.

$$
a-b = (-2, 0, 2)
$$

3. MultiplicaÃ§Ã£o por escalar $ğ›¼a = (ğ›¼a1, â€¦ , ğ›¼an)$.

$$
5 * a = (5*1, 5*2, 5*3)
$$

4. Transposta de um vetor:
[...]

5. Produto interno ou escalar entre dois vetores resulta em um escalar (mutiplica dois vetores e dÃ¡ um nÃºmero sÃ³ como resultado)
a â‹… b = (a1b 1 + a2b 2 + â€¦ + anb n).

- **CondiÃ§Ãµes: os vetores devem ser do mesmo tipo e tamanho.**

### Vetores ortogonais
- Dois vetores sÃ£o ortogonais entre si se o Ã¢ngulo ğœƒ entre eles Ã© de 90âˆ˜.(= correlaÃ§Ã£o de Pearson)
- ImplicaÃ§Ãµes: 
$$cos(ğœƒ) = 0 e aâŠ¤b = 0.$$
$$ cov (a,b) / raiz(variacia[a]) * raiz(variacia[b])$$

- O co-seno do Ã¢ngulo ğœƒ entre os vetores Ã© dado por:
$$cos(ğœƒ) = aâŠ¤b / \sqrt aâŠ¤a\sqrt bâŠ¤b .$$

### OperaÃ§Ãµes com vetores em R
- Declarando vetores
```{r}
a <- c(4,5,6)
b <- c(1,2,3)
```

- Sendo a e b compatÃ­veis
```{r}
#### Soma
a + b
## [1] 5 7 9
#### SubstraÃ§Ã£o
a - b
## [1] 3 3 3
```


- MultiplicaÃ§Ã£o por escalar
```{r}
alpha = 10
alpha*a
## [1] 40 50 60
```
- Produto de Hadamard (nÃ£o Ã© produto interno)

```{r}
a*b
## [1] 4 10 18
```
- Produto vetorial (ou produto interno)

```{r}
a%*%b
##    [,1]
## [1,] 32
```
- Co-seno do Ã¢ngulo entre dois vetores
```{r}
cos <- t(a)%*%b/(sqrt(t(a)%*%a)*sqrt(t(b)%*%b))
```
- Lei da reciclagem (nÃ£o avalia se pode somar antes de somar)
```{r}
a <- c(4,5,6,5,6,7)
b <- c(1,2,3)
a + b
## [1] 5 7 9 6 8 10
```

## Matrizes

- Uma matriz Ã© um arranjo retangular ou quadrado de nÃºmeros ou variÃ¡veis.
- A matriz costuma ser representada por uma letra maiuscula em negrito

- Uma matriz (n x  ğ‘š) tem n linhas e ğ‘š colunas:

$$A = \begin{pmatrix}\
a_{11} & a_{12} & ... & a_{1m}\\
a_{21} & a_{22} & ... & a_{2m}\\
... & ... & ... & ... \\
a_{n1} & a_{11} & ... & a_{nm}\\
\end{pmatrix}$$

- O primeiro subscrito representa linha e o segundo representa coluna.
- A dimensÃ£o de uma matriz Ã© o seu nÃºmero de linhas e colunas.
- Duas matrizes sÃ£o iguais se tem a mesma dimensÃ£o e se os elementos das correspondentes
posiÃ§Ãµes sÃ£o iguais.

### Matriz transposta
- A operaÃ§Ã£o de transposiÃ§Ã£o rearranja uma matriz de forma que suas linhas sÃ£o transformadas em colunas e vice-versa.
â›âœ
â
1 2
3 4
5 6
ââŸ
â 
âŠ¤
= (1 3 5
2 4 6) .
- Note que (AâŠ¤)âŠ¤ = A.
- Computacionalmente
- Declarando matrizes
```{r}
a <- c(1,2,3,4,5,6)
A <- matrix(a, nrow = 3, ncol = 2)
A
## [,1] [,2]
## [1,] 1 4
## [2,] 2 5
## [3,] 3 6
```
- O default preenche por colunas.
- Transposta de uma matriz
```{r}
t(A)
## [,1] [,2] [,3]
## [1,] 1 2 3
## [2,] 4 5 6
```

### OperaÃ§Ãµes com matrizes
- MultiplicaÃ§Ã£o matriz por escalar.
$$\alpha * A = \begin{pmatrix}\
\alpha * a_{11} & \alpha * a_{12} & \alpha * ... & \alpha * a_{1m}\\
\alpha * a_{21} & \alpha * a_{22} & \alpha * ... & \alpha * a_{2m}\\
\alpha * ... & \alpha * ... & \alpha * ... & \alpha * ... \\
\alpha * a_{n1} & \alpha * a_{n2} & \alpha * ... & \alpha * a_{nm}\\
\end{pmatrix}$$

- Computacionalmente
```{r}
A <- matrix(c(1,2,3,4,5,6),
nrow = 3, ncol = 2)
alpha <- 10
alpha*A
## [,1] [,2]
## [1,] 10 40
## [2,] 20 50

```

- Duas matrizes podem ser somadas ou
subtraÃ­das somente se tiverem o mesmo
tamanho.
1. Soma c i j  = ai j  + b i j .
2. SubtraÃ§Ã£o c i j  = ai j  âˆ’ b i j .
- Exemplo
$$A = \begin{pmatrix}\
1 & 2\\
3 & 4\\
5 & 6\\
\end{pmatrix}$$

$$B = \begin{pmatrix}\
10 & 20\\
30 & 40\\
50 & 60\\
\end{pmatrix}$$

$$A + B = \begin{pmatrix}\
11 & 22\\
33 & 44\\
55 & 66\\
\end{pmatrix}$$

- Soma de duas matrizes
```{r}
A <- matrix(c(1,2,3,4,5,6),
nrow = 3, ncol = 2)
B <- matrix(c(10,20,30,40,50,60),
nrow = 3, ncol = 2)
C = A + B
C
## [,1] [,2]
## [1,] 11 44
## [2,] 22 55
## [3,] 33 66
```

- CondiÃ§Ã£o para multiplicar matrizes
$$
C_{m, n} = A_{m,q} B_{q,n}
$$
(q tem que ser igual)

C
ğ‘šx n = A
ğ‘šx ğ‘ B
ğ‘x n.
- Cada elemento c i j  = \sum{}ğ‘
ğ‘˜=1 ai ğ‘˜b ğ‘˜j .
â›âœ
â
2 âˆ’1
8 3
6 7
ââŸ
â 
( 4 9 1 âˆ’3
âˆ’5 2 4 6 ) =
â›âœ
â
((2 â‹… 4) + (âˆ’1 â‹… âˆ’5)) ((2 â‹… 9) + (âˆ’1 â‹… 2)) ((2 â‹… 1) + (âˆ’1 â‹… 4)) ((2 â‹… âˆ’3) + (âˆ’1 â‹… 6))
((8 â‹… 4) + (3 â‹… âˆ’5)) ((8 â‹… 9) + (3 â‹… 2)) ((8 â‹… 1) + (3 â‹… 4)) ((8 â‹… âˆ’3) + (3 â‹… 6))
((6 â‹… 4) + (7 â‹… âˆ’5)) ((6 â‹… 9) + (7 â‹… 2)) ((6 â‹… 1) + (7 â‹… 4)) ((6 â‹… âˆ’3) + (7 â‹… 6))
ââŸ
â 
=
â›âœ
â
13 16 âˆ’2 âˆ’12
17 78 20 âˆ’6
âˆ’11 68 34 24
ââŸ
â 
.

- Computacionalmente.
- Matrizes compatÃ­veis
```{r}
A <- matrix(c(2,8,6,-1,3,7),
nrow = 3, ncol = 2)
B <- matrix(c(4,-5,9,2,1,4,-3,6),
nrow = 2, ncol = 4)
C = A%*%B
C
## [,1] [,2] [,3] [,4]
## [1,] 13 16 -2 -12
## [2,] 17 78 20 -6
## [3,] -11 68 34 24
```

- Matrizes nÃ£o compatÃ­veis
```{r message=TRUE, warning=TRUE}
B %*% A
## Error in B %*% A: argumentos nÃ£o compatÃ­veis
```


Produto de Hadamard
- Produto simples ou de Hadamard

$$A \odot B = \begin{pmatrix}\
a_{11}*b_{11} & a_{12}*b_{12} & ... & a_{1m}*b_{1m}\\
a_{21}*b_{21} & a_{22}*b_{22} & ... & a_{2m}*b_{2m}\\
... & ... & ... & ... \\
a_{n1}*b_{n1} & a_{n2}*b_{n2} & ... & a_{nm}*b_{nm}\\
\end{pmatrix}$$


- Computacionalmente
```{r}
A <- matrix(c(1,2,3,4),
nrow = 2, ncol = 2)
B <- matrix(c(10,20,30,40),
nrow = 2, ncol = 2)
A*B
## [,1] [,2]
## [1,] 10 90
## [2,] 40 160
```


Propriedades envolvendo operaÃ§Ãµes com matrizes
- Sendo A, B, C e D compatÃ­veis temos,
1. $A + B = B + A$
2. $(A + B) + C = A + (B + C)$.
3. $ğ›¼(A + B) = ğ›¼A + ğ›¼B$.
4. $(ğ›¼ + \beta )A = ğ›¼A + \beta A$.
5. $ğ›¼(AB) = (ğ›¼A)B = A(ğ›¼B)$.
6. $A(B Â± C) = AB Â± AC$.
7. $(A Â± B)C = AC Â± BC$.
8. $(Aâˆ’B)(Câˆ’D) = ACâˆ’BCâˆ’AD+BD$.

- Propriedades envolvendo transposta e
multiplicaÃ§Ã£o
1. Se A Ã© n x  ğ‘š e B Ã© ğ‘š x  n, entÃ£o (AB)âŠ¤ = BâŠ¤AâŠ¤.
2. Se A, B e C sÃ£o compatÃ­veis 
$$
(ABC)^{âŠ¤}= C^{âŠ¤}B^{âŠ¤}A^{âŠ¤}.
$$

### Matrizes de formas especiais
- Matriz quadrada (m = n)
Exemplo 4x4
```{r}
A <- matrix(c("a11","a21","a31","a41","a12","a22","a32","a42","a13","a23","a33","a43","a14","a24","a34","a44"), nrow = 4, ncol = 4)
A
```

- ai i  sÃ£o os elementos da diagonal.
- ai j  para i  â‰  j  â†’ fora da diagonal.
- ai j  para j  > i  â†’ acima da diagonal.
- ai j  para i  > j  â†’ abaixo da diagonal.
- Matriz diagonal
$$D = \begin{pmatrix}\
a_{11} & 0 & 0 & 0\\
0 & a_{22} & 0 & 0\\
0 & 0 & a_{33} & 0 \\
0 & 0 & 0 & a_{44}\\
\end{pmatrix}$$



- Matriz identidade
I = â›âœâœâœâœ
â
1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 1
ââŸâŸâŸâŸ
â 

### Matrizes de formas especiais
- Triangular superior
U = â›âœâœâœâœ
â
a11 a12 a13 a14
0 a22 a23 a24
0 0 a33 a34
0 0 0 a44
ââŸâŸâŸâŸ
â 
.
- Triangular inferior
L = â›âœâœâœâœ
â
a11 0 0 0
a21 a22 0 0
a31 a32 a33 0
a41 a42 a43 a44
ââŸâŸâŸâŸ
â 
.
- Matriz nula
0 = â›âœâœâœâœ
â
0 0 0 0
0 0 0 0
0 0 0 0
0 0 0 0
ââŸâŸâŸâŸ
â 
.
- Matriz quadrada simÃ©trica
A = â›âœâœâœâœ
â
1 0.8 0.6 0.4
0.8 1 0.2 0.4
0.6 0.2 1 0.1
0.4 0.4 0.1 1
ââŸâŸâŸâŸ
â 


### CombinaÃ§Ãµes lineares
- Um conjunto de vetores a1, a2, â€¦ , an Ã© dito ser linearmente dependente se puderem ser
encontrados escalares c 1, c 2, â€¦ , c n e estes escalares nÃ£o sejam todos iguais a 0 de tal forma
que
$$
c 1a1 + c 2a2 + â€¦ + c nan = 0.
$$

Exemplo:
```{r}
a1 <- matrix(c(1,0), nrow = 2, ncol = 1)
a1
a2 <- matrix(c(0,1), nrow = 2, ncol = 1)
a2

#O unico caso que esses c1*a1 + c2*a2 = (0, 0) Ã© se c1 = 0 E c2 =0
#Ou seja Linearmente independente

a1 <- matrix(c(1,2), nrow = 2, ncol = 1)
a1
a2 <- matrix(c(-1,-2), nrow = 2, ncol = 1)
a2

#Existem casos fora os cs = 0 que fazem c1*a1 + c2*a2 = (0, 0)
#Ou seja Linearmente dependente

```


- Caso contrÃ¡rio Ã© dito ser linearmente independente.
- NotaÃ§Ã£o matricial
$$
Ac = 0.
$$
- As colunas de A sÃ£o linearmente independentes se Ac = 0 implicar que c = 0.

### Rank ou posto de uma matriz
- O rank ou posto de qualquer matriz quadrada ou retangular A Ã© definido como
rank(A) = nÃºmero de colunas ou linhas linearmente independentes em A.
- Sendo A uma matriz retangular n x  ğ‘š o maior rank possÃ­vel para A Ã© o min(n,ğ‘š).
- O rank da matrix nula Ã© 0.
- Se o rank da matriz Ã© o min(n,ğ‘š) dizemos que a matriz tem rank completo.

### Matriz nÃ£o singular e matriz inversa
- Uma matriz quadrada de posto completo Ã© chamada de nÃ£o singular.
- Sendo A quadrada de posto completo a matriz inversa de A Ã© Ãºnica tal que (sÃ³ se a matriz for quadrada e de ranking completo)
$$
AA^{âˆ’1} = I.
$$
- NÃ£o quadrada (posto incompleto) â†’ nÃ£o terÃ¡ inversa e Ã© dita ser singular.
- Note que 
$$
A^{(-1^{-1})} =A
$$
A^{âˆ’1}^{âˆ’1} = A

## Matriz inversa
- Computacionalmente
```{r}
A <- matrix(c(4, 2, 7, 6), 2, 2)
A

A_inv <- solve(A)
A_inv

I = A %*% A_inv
I

```

- Verificando
```{r}
A%*%A_inv
## [,1] [,2]
## [1,] 1 0
## [2,] 0 1
```


- Propriedades envolvendo inversas
1. Se A Ã© nÃ£o singular, entÃ£o AâŠ¤ Ã© nÃ£o singular e sua inversa Ã© dada por
(AâŠ¤)âˆ’1 = (Aâˆ’1)âŠ¤.
2. Se A e B sÃ£o matrizes nÃ£o singulares de mesmo tamanho, entÃ£o o produto AB Ã©
nÃ£o singular e
(AB)âˆ’1 = Bâˆ’1Aâˆ’1.

### Inversa generalizada
- A inversa generalizada de uma matriz A n x  p Ã© qualquer matriz Aâˆ’ que satisfaÃ§a 
$$
AA^{âˆ’}A = A.
$$

- NÃ£o Ã© Ãºnica exceto quando A Ã© nÃ£o-singular (inversa usual).
- Exemplo

$$

$$

a = â›âœâœâœâœ
â
1
2
3
4
ââŸâŸâŸâŸ
â 
.


- aâˆ’ = (1, 0, 0, 0)

- Verificando

```{r}
a <- matrix(c(1, 2, 3, 4), 4, 1)
a_invg <- matrix(c(1,0,0,0), 1, 4)
a%*%a_invg%*%a
## [,1]
## [1,] 1
## [2,] 2
## [3,] 3
## [4,] 4
```
- Moore-Penrose generalized inverse
```{r}
#### Matriz singular (col 3 = col 2 + col 1)
A <- matrix(c(2, 1, 3, 2, 0,
2, 3, 1, 4), 3, 3)
library(MASS)
A_ginv <- ginv(A)
A%*%A_ginv%*%A ## Verificando
```

## Matrizes positivas definidas

### Formas quadrÃ¡ticas
- Soma de quadrados sÃ£o importantes em ciÃªncia de dados.
- Considere uma matriz A simÃ©trica e y um vetor, o produto
$$
y^{T}Ay = 
\sum(a_{ij}y^{2}_{i}) + 
\sum_{i \differ j}(a_{ij}y_{i}y_{j})
$$
Ã© chamado de forma quadrÃ¡tica.

$$
y^{T}Iy = \sum^{n}_{i=0}(y^{2}_{i})
$$


- Sendo y de dimensÃ£o n x  1, 
$$
yâŠ¤Iy = y 2
1 + y 2
2 + â€¦ , y 2
n
$$

- Consequentemente, yâŠ¤y Ã© a soma de quadrados dos elementos do vetor y.
- A raiz quadrada da soma de quadrados Ã© o comprimento de y.

Matriz positiva definida
- Sendo A uma matriz simÃ©trica com a propriedade yâŠ¤Ay > 0 para todos os possÃ­veis y
exceto para quando y = 0, entÃ£o a forma quadrÃ¡tica yâŠ¤Ay Ã© chamada positiva definida,
e A Ã© dita ser uma matriz positiva definida.
- Exemplo
A = ( 2 âˆ’1
âˆ’1 3 ) .
A forma quadrÃ¡tica associada Ã© dada por (ver abaixo) que Ã© claramente positiva, desde que y 1 e y 2 sejam diferentes de zero.
$$
yâŠ¤Ay = (y 1 y 2) ( 2 âˆ’1
âˆ’1 3 ) (y 1
y 2
) = 2y 2
1 âˆ’ 2y 1y 2 + 3y 2
2 ,
$$

### Propriedades de matrizes positivas definidas
1. Se A Ã© positiva definida, entÃ£o todos os valores da diagonal de A sÃ£o positivos.
2. Se A Ã© positiva semi-definida, entÃ£o os elementos da diagonal de A sÃ£o maiores ou iguais a zero.
3. Sendo P uma matriz nÃ£o-singular e A uma matriz positiva definida, o produto PâŠ¤AP Ã© positiva definida.
4. Sendo P uma matriz nÃ£o-singular e A uma matriz positiva semi-definida, o produto PâŠ¤AP Ã© positiva semi-definida.
5. Uma matriz positiva definida Ã© nÃ£o-singular.

### Determinante de uma matriz
- O determinante de uma matriz A Ã© o escalar (= numero)
$$
|A| = \sum((-1)^k a_{1j_{1}} a_{2j_{2}} ... a_{nj_{n}})
$$
onde a soma Ã© realizada para todas as n! permutaÃ§Ãµes de grau n, e ğ‘˜ Ã© o nÃºmero de
mudanÃ§as necessÃ¡rias para que os segundos subscritos sejam colocados na ordem
1,2, â€¦ , n.
- Considere a matriz
A = ( 3 âˆ’2
âˆ’2 4 ) .
|A| = (âˆ’1)0a11a22 + (âˆ’1)1a12a21 = 1 â‹… (3 â‹… 4) âˆ’ (âˆ’2 â‹… âˆ’2) = 12 âˆ’ 4 = 8.

Determinante de uma matriz
- Computacionalmente.
A <- matrix(c(3,-2,-2,4),2,2)
determinant(A, logarithm = FALSE)$modulus
## [1] 8
## attr(,"logarithm")
## [1] FALSE
- Determinante em escala log.
determinant(A, logarithm = TRUE)$modulus
## [1] 2.079442
## attr(,"logarithm")
## [1] TRUE
- Alguns aspectos interessantes sobre
determinantes sÃ£o:
1. Se A Ã© singular, |A| = 0.
2. Se A Ã© nÃ£o singular, |A| â‰  0.
3. Se A Ã© positiva definida, |A| > 0.
4. |AâŠ¤| = |A|.
5. Se A Ã© nÃ£o singular, |Aâˆ’1| = 1
|A| .

TraÃ§o de uma matriz
- O traÃ§o de uma matriz A n x  n Ã© um
escalar definido como a soma dos
elementos da diagonal, tr(A) = \sum{}n
i =1 ai i .
- Propriedades
1. Se A e B sÃ£o n x  n, entÃ£o
tr(A + B) = tr(A) + tr(B).
2. Se A Ã© n x  p e B e p x  n, entÃ£o
tr(AB) = tr(BA).
- Computacionalmente
```{r}
A <- matrix(c(3,-2,-2,4),2,2)
sum(diag(A))
## [1] 7
```

## CÃ¡lculo vetorial e matricial

### CÃ¡lculo vetorial
- Seja $y = f(x)$ uma funÃ§Ã£o das variÃ¡veis $x_{1}, x_{2}, x_{3}, ... , x_{p}$ e \partial y  as respectivas derivadas parciais.

$$
\matrix_vertical de (a1x1, a2x2, .... apxp) 
$$

$$
\partial x 1
, \partial y 
\partial x 2
, â€¦ , \partial y 
\partial x p
$$

Assim,
$$
\deriv
$$


\partial y 
\partial x =
â›âœâœâœâœâœ
â
\partial y 
\partial x 1
\partial y 
\partial x 2
â‹®
\partial y 
\partial x p
ââŸâŸâŸâŸâŸ
â 
.

### CÃ¡lculo vetorial
- Sendo aâŠ¤ = (a1, a2, â€¦ , ap) um vetor de constantes e A uma matriz simÃ©trica de constantes.
1. Seja y  = aâŠ¤x = xâŠ¤a. EntÃ£o,
$$
\partial y 
\partial x = \partial (xâŠ¤a)
\partial x = a.
$$
2. Seja y  = xâŠ¤Ax. EntÃ£o,
$$
\partial y 
\partial x = \partial (xâŠ¤Ax)
\partial x = 2Ax.
$$

### CÃ¡lculo Matricial
- Se y  = f (X) onde X Ã© uma matriz p x  p. As derivadas parciais de y  em relaÃ§Ã£o a cada x i j 
sÃ£o organizadas em uma matriz.
$$
\partial y 
\partial X = â›âœâœ
â
\partial y 
\partial x 11
â€¦ \partial y 
\partial x 1p
â‹® â‹± â‹®
\partial y 
\partial x p1
â€¦ \partial y 
\partial x pp
ââŸâŸ
$$


- Algumas derivadas importantes envolvendo matrizes sÃ£o apresentadas abaixo.
1. Seja y  = tr(XA) sendo X p x  p e definida positiva e A p x  p constantes. EntÃ£o,
$$
\partial y 
\partial X = \partial tr(XA)
\partial X = A + AâŠ¤ âˆ’ diag(A).
$$
2. Sendo A nÃ£o singular com derivadas \partial A
$$
\partial x  . EntÃ£o,
\partial Aâˆ’1
\partial x  = âˆ’Aâˆ’1 \partial A
\partial x  Aâˆ’1.
$$
3. Sendo A n x  n positiva definida. EntÃ£o,
$$
\partial  log |A|
\partial x  = tr (Aâˆ’1 \partial A
\partial x  )
$$

## RegressÃ£o linear mÃºltipla

### RegressÃ£o linear mÃºltipla: especificaÃ§Ã£o usual

- RegressÃ£o linear simples
$$
y_{i} = \beta_{0} +\beta_{1}x_{1} + erro_{i}
$$
- RegressÃ£o linear mÃºltipla
$$
y_{i} = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2} + ... + \beta_{p}x_{ip} + erro_{i}
$$
- Modelo para cada observaÃ§Ã£o
$$y_{1} = \beta_{0} + \beta_{1}x_{11} + \beta_{2}x_{12} + ... + \beta_{p}x_{1p} + erro_{1}$$

$$y_{2} = \beta_{0} + \beta_{1}x_{21} + \beta_{2}x_{22} + ... + \beta_{p}x_{2p} + erro_{1}$$
$$...$$
$$y_{n} = \beta_{0} + \beta_{1}x_{n1} + \beta_{2}x_{n2} + ... + \beta_{p}x_{np} + erro_{n}$$

RegressÃ£o linear mÃºltipla: especificaÃ§Ã£o matricial
- NotaÃ§Ã£o matricial
$$
\begin{bmatrix}
y_{1}\\
y_{2}\\
...\\
y_{n}\\
\end{bmatrix}
= 
\begin{bmatrix}
1 & x_{1}\\
1 & x_{2}\\
1 & ...\\
1 & x_{n}\\
\end{bmatrix}
x 
\begin{bmatrix}
\beta_{1}\\
\beta_{2}\\
...\\
\beta_{n}\\
\end{bmatrix}
+
\begin{bmatrix}
erro_{1}\\
erro_{2}\\
...\\
erro_{n}\\
\end{bmatrix}
$$

- NotaÃ§Ã£o mais compacta
$$
y_{(n x  1)} = X_{(n x  p)} \beta_{(p x  1)} + erro_{(n x  1)}
$$
### RegressÃ£o linear mÃºltipla: estimaÃ§Ã£o (treinamento)
- Objetivo: encontrar o vetorÌ‚ \beta , tal que $ğ‘†ğ‘„(\beta ) = (y âˆ’ X\beta )âŠ¤(y âˆ’ X\beta ) $ seja a menor possÃ­vel.

### RegressÃ£o linear mÃºltipla: estimaÃ§Ã£o
1. Passo 1: encontrar o vetor gradiente. Derivando em \beta , temos
$$
\partial ğ‘†ğ‘„(\beta )
\partial \beta  = \partial 
\partial \beta  (y âˆ’ X\beta )âŠ¤(y âˆ’ X\beta )
= \partial 
\partial \beta  ((y âˆ’ X\beta )âŠ¤) (y âˆ’ X\beta ) + (y âˆ’ X\beta )âŠ¤ \partial 
\partial \beta  (y âˆ’ X\beta )
= âˆ’XâŠ¤(y âˆ’ X\beta ) + (y âˆ’ X\beta )âŠ¤(âˆ’X)
= âˆ’2XâŠ¤(y âˆ’ X\beta ).
$$

### RegressÃ£o linear mÃºltipla: estimaÃ§Ã£o

2. Passo 2: resolver o sistema de equaÃ§Ãµes lineares (esquece o "-2" primeiro)
$$ X^{T} (y - X\hat{\beta}) = 0$$
$$XâŠ¤y âˆ’ XâŠ¤XÌ‚\beta  = 0$$
$$XâŠ¤XÌ‚\beta  = XâŠ¤yÌ‚$$
$$(XTX)^{-1}  XâŠ¤XÌ‚\beta  = XâŠ¤y (XTX)^{-1}$$
$$ I\beta  = (XâŠ¤X)âˆ’1XâŠ¤y $$

### RegressÃ£o linear mÃºltipla: exemplo
- Conjunto de dados Boston disponÃ­vel no pacote MASS.
- Cinco primeiras covariÃ¡veis disponÃ­veis:
  - crim: taxa de crimes per capita.
  - zn: proporÃ§Ã£o de terrenos residenciais zoneados para lotes com mais de 25.000 pÃ©s quadrados.
  - indus: proporÃ§Ã£o de acres de negÃ³cios nÃ£o varejistas por cidade.
  - chas: variÃ¡vel dummy de Charles River (1 se a Ã¡rea limita o rio; 0 caso contrÃ¡rio).
  - nox: concentraÃ§Ã£o de Ã³xido de nitrogÃªnio (parte por 10 milhÃµes).
- VariÃ¡vel resposta: medv valor mediano das casas ocupadas em $1000.

### RegressÃ£o linear mÃºltipla: implementaÃ§Ã£o computacional
- Carregando a base de dados 
```{r}
require(MASS)
## Carregando pacotes exigidos: MASS

data(Boston)
head(Boston[, c(1:5,14)])
## crim zn indus chas nox medv
## 1 0.00632 18 2.31 0 0.538 24.0
## 2 0.02731 0 7.07 0 0.469 21.6
## 3 0.02729 0 7.07 0 0.469 34.7
## 4 0.03237 0 2.18 0 0.458 33.4
## 5 0.06905 0 2.18 0 0.458 36.2
## 6 0.02985 0 2.18 0 0.458 28.7
```


- Matriz de delineamento (X).
```{r}
X <- model.matrix(~ crim + zn + indus +
chas + nox, data = Boston)
head(X)
## (Intercept) crim zn indus chas nox
## 1 1 0.00632 18 2.31 0 0.538
## 2 1 0.02731 0 7.07 0 0.469
## 3 1 0.02729 0 7.07 0 0.469
## 4 1 0.03237 0 2.18 0 0.458
## 5 1 0.06905 0 2.18 0 0.458
## 6 1 0.02985 0 2.18 0 0.458
```


- VariÃ¡vel resposta
```{r}
y <- Boston$medv
```

- Estimadores de mÃ­nimos quadrados:
$$
\hat{\beta} = (X^{T}X)^{-1} X^{T}y
$$
- Computacionalmente: versÃ£o ingÃªnua (calcula inversa)
```{r}
round(solve(t(X)%*%X)%*%t(X)%*%y, 2)
## [,1]
## (Intercept) 29.49
## crim -0.22
## zn 0.06
## indus -0.38
## chas 7.03
## nox -5.42
```

- Computacionalmente: versÃ£o eficiente (escalona?)
```{r}
round(solve(t(X)%*%X, t(X)%*%y), 2)
## [,1]
## (Intercept) 29.49
## crim -0.22
## zn 0.06
## indus -0.38
## chas 7.03
## nox -5.42
```

- FunÃ§Ã£o nativa do R
```{r}
t(round(coef(lm(medv ~ crim + zn + indus + chas + nox, data = Boston)), 2))
## (Intercept) crim zn indus chas nox
## [1,] 29.49 -0.22 0.06 -0.38 7.03 -5.42
```

### Matrizes esparsas (tÃ³pico adicional)

- Matrizes aparecem em todos os tipos de aplicaÃ§Ã£o em ciÃªncia de dados.
- Modelos estatÃ­sticos, machine learning, anÃ¡lise de texto, anÃ¡lise de cluster, etc.
- Muitas vezes as matrizes usadas tÃªm uma grande quantidade de zeros.
- Quando uma matriz tem uma quantidade considerÃ¡vel de zeros, dizemos que ela Ã©
esparsa, caso contrÃ¡rio dizemos que a matriz Ã© densa.
- Todas as propriedades que vimos para matrizes em geral valem para matrizes esparsas.
- O R tem um conjunto de mÃ©todos altamente eficiente por meio do pacote Matrix.
- Saber que uma matriz Ã© esparsa Ã© Ãºtil pois permite:
- Planejar formas de armazenar a matriz em memÃ³ria.
- Economizar cÃ¡lculos em algoritmos numÃ©ricos (multiplicaÃ§Ã£o, inversa, determinante,
decomposiÃ§Ãµes, etc).

- Comparando a quantidade de memÃ³ria utilizada.
```{r}
library('Matrix')

m1 <- matrix(0, nrow = 1000, ncol = 1000)
object.size(m1)
## 8000216 bytes

m2 <- Matrix(0, nrow = 1000, ncol = 1000, sparse = TRUE)
object.size(m2)
## 9240 bytes
```


Comparando o tempo computacional


- Matriz densa
```{r}
y <- rnorm(1000)
X <- matrix(NA, ncol = 100, nrow = 1000)
for(i in 1:1000) {X[i,] <- rbinom(100, size = 1, p = 0.1)}
system.time(replicate(100, solve(t(X)%*%X, t(X)%*%y)))
## usuÃ¡rio sistema decorrido
## 0.819 0.004 0.823
```


- Matriz esparsa
```{r}
y <- rnorm(1000)
X <- Matrix(NA, ncol = 100, nrow = 1000)
for(i in 1:1000) {X[i,] <- rbinom(100, size = 1, p = 0.1)}
X <- Matrix(X, sparse = TRUE)
system.time(replicate(100, solve(t(X)%*%X, t(X)%*%y)))
## usuÃ¡rio sistema decorrido
## 0.223 0.000 0.224
```

### Diferentes formas de implementar as operaÃ§Ãµes matriciais

- Criando a base de dados para a comparaÃ§Ã£o
```{r}
library(Matrix)
n <- 10000; p <- 500

#DENSA
x <- matrix(rbinom(n*p, 1, 0.01), nrow=n, ncol=p)
object.size(x)
## 20000216 bytes

#ESPARCA
X <- Matrix(x)
object.size(X)
## 600432 bytes
```

- Diferentes implementaÃ§Ãµes
```{r}
y <- rnorm(n)

print("Matriz densa com %*%:")
system.time(solve(t(x)%*%x, t(x)%*%y))
## usuÃ¡rio sistema decorrido
## 2.053 0.040 2.094

print("Matriz densa com crossprod")
system.time(solve(crossprod(x), crossprod(x, y)))
## usuÃ¡rio sistema decorrido
## 1.731 0.016 1.748

print("Matriz esparÃ§a com %*%")
system.time(solve(t(X)%*%X, t(X)%*%y))
## usuÃ¡rio sistema decorrido
## 0.071 0.000 0.072

print("Matriz esparÃ§a com crossprod")
system.time(solve(crossprod(X), crossprod(X,y)))
## usuÃ¡rio sistema decorrido
## 0.029 0.000 0.050
```

- ImplementaÃ§Ã£o eficiente do modelo de regressÃ£o linear mÃºltipla.
```{r}
library(glmnet)
## Loaded glmnet 4.1-6
system.time(b <- coef(lm(y~x)))
## usuÃ¡rio sistema decorrido
## 2.389 0.044 2.434
system.time(g1 <-glmnet(x, y, nlambda=1, lambda=0, standardize=FALSE))
## usuÃ¡rio sistema decorrido
## 0.065 0.020 0.086
system.time(g2 <- glmnet(X, y, nlambda=1, lambda=0, standardize=FALSE))
## usuÃ¡rio sistema decorrido
## 0.006 0.000 0.006
```


# Proxima aula

### Sistemas lineares
- Sistema com duas equaÃ§Ãµes:
$$ f 1(x 1,x 2) = 0$$
$$f 2(x 1,x 2) = 0$$
- SoluÃ§Ã£o numÃ©rica consiste em encontrarÌ‚ x 1 eÌ‚ x 2 que satisfaÃ§a o sistema de equaÃ§Ãµes.
- Sistema com n equaÃ§Ãµes
$$f 1(x 1, â€¦ , x n) = 0
â‹®
f n(x 1, â€¦ , x n) = 0.
- Genericamente, tem-se
f(x ) = 0.$$

- EquaÃ§Ãµes podem ser lineares ou nÃ£o-lineares.

Sistemas de equaÃ§Ãµes lineares
- Cada equaÃ§Ã£o Ã© linear na incÃ³gnita.
- SoluÃ§Ã£o analÃ­tica em geral Ã© possÃ­vel.
- Exemplo:
7x 1 + 3x 2 = 45
4x 1 + 5x 2 = 29.
- SoluÃ§Ã£o analÃ­tica:Ì‚ x 1 = 6 eÌ‚ x 2 = 1.
- Resolver (tedioso!!).
- TrÃªs possÃ­veis casos:
1. Uma Ãºnica soluÃ§Ã£o (sistema nÃ£o singular).
2. Infinitas soluÃ§Ãµes (sistema singular).
3. Nenhuma soluÃ§Ã£o (sistema impossÃ­vel).

Sistemas de equaÃ§Ãµes lineares
- RepresentaÃ§Ã£o matricial do sistema de equaÃ§Ãµes lineares:
A = [7 3
4 5] , x = [x 1
x 2
] e b = [45
29] .
- De forma geral, tem-se
Ax = b.

OperaÃ§Ãµes com linhas
- Sem qualquer alteraÃ§Ã£o na relaÃ§Ã£o linear, Ã© possÃ­vel
1. Trocar a posiÃ§Ã£o de linhas:
4x 1 + 5x 2 = 29
7x 1 + 3x 2 = 45.
2. Multiplicar qualquer linha por uma constante, aqui 4x 1 + 5x 2 por 1
4 , obtendo
x 1 + 5
4 x 2 = 29
4 (1)
7x 1 + 3x 2 = 45. (2)

OperaÃ§Ãµes com linhas
3. Subtrair um mÃºltiplo de uma linha de uma outra, aqui 7 âˆ— ğ¸ğ‘.(1) menos Eq. (2), obtendo
x 1 + 5
4 x 2 = 29
4
0x 1 + ( 35
4 âˆ’ 3)x 2 = 203
4 âˆ’ 45.
- Fazendo as contas, tem-se
0x 1 + 23
4 x 2 = 23
4 .

SoluÃ§Ã£o de sistemas lineares
- Forma geral de um sistema com n equaÃ§Ãµes lineares:
a11x 1 + a12x 2 + â€¦ + a1nx n = b 1
a21x 1 + a22x 2 + â€¦ + a2nx n = b 2
â‹®
an1x 1 + an2x 2 + â€¦ + annx n = b n
- Matricialmente, tem-se
â¡
â¢
â¢
â£
a11 a12 â€¦ a1n
a21 a22 â€¦ a2n
â‹® â‹® â‹® â€¦
an1 an2 â€¦ ann
â¤
â¥
â¥
â¦
â¡
â¢
â¢
â£
x 1
x 2
â‹®
x n
â¤
â¥
â¥
â¦
= â¡
â¢
â¢
â£
b 1
b 2
â‹®
b n
â¤
â¥
â¥
â¦
- MÃ©todos diretos e mÃ©todos iterativos.

### MÃ©todos diretos

- O sistema de equaÃ§Ãµes Ã© manipulado atÃ© se transformar em um sistema equivalente de
fÃ¡cil resoluÃ§Ã£o.
- Triangular superior:
â¡
â¢
â¢
â£
a11 a12 a13 a14
0 a22 a23 a24
0 0 a33 a34
0 0 0 a44
â¤
â¥
â¥
â¦
â¡
â¢
â¢
â£
x 1
x 2
x 3
x 4
â¤
â¥
â¥
â¦
= â¡
â¢
â¢
â£
b 1
b 2
b 3
b 4
â¤
â¥
â¥
â¦
.
- SubstituiÃ§Ã£o regressiva
x n = b n
ann
x i  = b i  âˆ’ \sum{}j =n
j =i +1 ai j x j 
ai i 
, i  = n âˆ’ 1, n âˆ’ 2, â€¦ , 1.

MÃ©todos diretos
- Triangular inferior:
â¡
â¢
â¢
â£
a11 0 0 0
a21 a22 0 0
a31 a32 a33 0
a41 a42 a43 a44
â¤
â¥
â¥
â¦
â¡
â¢
â¢
â£
x 1
x 2
x 3
x 4
â¤
â¥
â¥
â¦
= â¡
â¢
â¢
â£
b 1
b 2
b 3
b 4
â¤
â¥
â¥
â¦
.
- SubstituiÃ§Ã£o progressiva
x 1 = b 1
a11
x i  = b i  âˆ’ \sum{}j =i âˆ’1
j =i  ai j x j 
ai i 
, i  = 2, 3, â€¦ , n.


MÃ©todos diretos
- Diagonal:
â¡
â¢
â¢
â£
a11 0 0 0
0 a22 0 0
0 0 a33 0
0 0 0 a44
â¤
â¥
â¥
â¦
â¡
â¢
â¢
â£
x 1
x 2
x 3
x 4
â¤
â¥
â¥
â¦
= â¡
â¢
â¢
â£
b 1
b 2
b 3
b 4
â¤
â¥
â¥
â¦
.

EliminaÃ§Ã£o de Gauss

MÃ©todos diretos: EliminaÃ§Ã£o de Gauss
- MÃ©todo de EliminaÃ§Ã£o de Gauss consiste em manipular o sistema original usando
operaÃ§Ãµes de linha atÃ© obter um sistema triangular superior.
[
a11 a12 a13 a14
a21 a22 a23 a24
a31 a23 a33 a34
a41 a24 a34 a44
] [
x 1
x 2
x 3
x 4
] = [
b 1
b 2
b 3
b 4
] â†’ [
a11 a12 a13 a14
0 aâ€²
22 aâ€²
23 aâ€²
24
0 0 aâ€²
33 aâ€²
34
0 0 0 aâ€²
44
] [
x 1
x 2
x 3
x 4
] = [
b 1
b â€²
2
b â€²
3
b â€²
4
]
- Usar eliminaÃ§Ã£o regressiva no novo sistema para obter a soluÃ§Ã£o.
- Resolva o seguinte sistema usando EliminaÃ§Ã£o de Gauss.
â¡
â¢
â£
3 2 6
2 4 3
5 3 4
â¤
â¥
â¦
â¡
â¢
â£
x 1
x 2
x 3
â¤
â¥
â¦
= â¡
â¢
â£
24
23
33
â¤
â¥
â¦

MÃ©todos diretos: EliminaÃ§Ã£o de Gauss
- Passo 1: encontrar o pivÃ´ e eliminar os elementos abaixo dele usando operaÃ§Ãµes de linha.
[ [3] 2 6
2 âˆ’ 2
3 3 4 âˆ’ 2
3 2 3 âˆ’ 2
3 6
5 âˆ’ 5
3 3 3 âˆ’ 5
3 2 4 âˆ’ 5
3 6
] [ 24
23 âˆ’ 2
3 24
33 âˆ’ 5
3 24
] â†’ [[3] 2 6
0 8
3 âˆ’1
0 âˆ’ 1
3 âˆ’6
] [24
7
âˆ’7]
- Passo 2: encontrar o segundo pivÃ´ e eliminar os elementos abaixo dele usando operaÃ§Ãµes
de linha.
[3 2 6
0 [ 8
3 ] âˆ’1
0 âˆ’ 1
3 âˆ’ (âˆ’ 3
24 ) ( 8
3 ) âˆ’6 âˆ’ (âˆ’ 3
24 )(âˆ’1)
] [ 24
7
âˆ’7 âˆ’ (âˆ’ 3
24 )(7)] â†’ [3 2 6
0 [ 8
3 ] âˆ’1
0 0 âˆ’ 147
24
] [ 24
7
âˆ’ 147
24
]
- Passo 3: substituiÃ§Ã£o regressiva.

MÃ©todos diretos: EliminaÃ§Ã£o de Gauss
- Usando a fÃ³rmula de substituiÃ§Ã£o regressiva temos:
- x 3 = b 3
a33
= 1.
- x 2 = b 2âˆ’a23x 3
a22
= 3.
- x 1 = (b 1âˆ’(a12x 2+a13x 3)
a11
= 4.
- A extensÃ£o do procedimento para um sistema com n equaÃ§Ãµes Ã© trivial.
1. Transforme o sistema em triangular superior usando operaÃ§Ãµes linhas.
2. Resolva o novo sistema usando substituiÃ§Ã£o regressiva.
- Potenciais problemas do mÃ©todo de eliminaÃ§Ã£o de Gauss:
- O elemento pivÃ´ Ã© zero.
- O elemento pivÃ´ Ã© pequeno em relaÃ§Ã£o aos demais termos.

EliminaÃ§Ã£o de Gauss com pivotaÃ§Ã£o

EliminaÃ§Ã£o de Gauss com pivotaÃ§Ã£o
- Considere o sistema
0x 1 + 2x 2 + 3x 2 = 46
4x 1 âˆ’ 3x 2 + 2x 3 = 16
2x 1 + 4x 2 âˆ’ 3x 3 = 12
- Neste caso o pivÃ´ Ã© zero e o procedimento nÃ£o pode comeÃ§ar.
- PivotaÃ§Ã£o - trocar a ordem das linhas.
1. Evitar pivÃ´s zero.
2. Diminuir o nÃºmero de operaÃ§Ãµes necessÃ¡rias para triangular o sistema.
4x 1 âˆ’ 3x 2 + 2x 3 = 16
2x 1 + 4x 2 âˆ’ 3x 3 = 12
0x 1 + 2x 2 + 3x 2 = 46

EliminaÃ§Ã£o de Gauss com pivotaÃ§Ã£o
- Se durante o procedimento uma equaÃ§Ã£o pivÃ´ tiver um elemento nulo e o sistema tiver
soluÃ§Ã£o, uma equaÃ§Ã£o com um elemento pivÃ´ diferente de zero sempre existirÃ¡.
- CÃ¡lculos numÃ©ricos sÃ£o menos propensos a erros e apresentam menores erros de
arredondamento se o elemento pivÃ´ for grande em valor absoluto.
- Ã‰ usual ordenar as linhas para que o maior valor seja o primeiro pivÃ´.

Passo 1: obtendo uma matriz triangular superior.
gauss <- function(A, b) {
Ae <- cbind(A, b) ## Sistema aumentado
rownames(Ae) <- paste0("x", 1:length(b))
n_row <- nrow(Ae)
n_col <- ncol(Ae)
SOL <- matrix(NA, n_row, n_col) ## Matriz para receber os resultados
SOL[1,] <- Ae[1,]
pivo <- matrix(0, n_col, n_row)
for(j in 1:c(n_row-1)) {
for(i in c(j+1):c(n_row)) {
pivo[i,j] <- Ae[i,j]/SOL[j,j]
SOL[i,] <- Ae[i,] - pivo[i,j]*SOL[j,]
Ae[i,] <- SOL[i,]
}
}
return(SOL)
}

EliminaÃ§Ã£o de Gauss sem pivotaÃ§Ã£o
- Passo 2: substituiÃ§Ã£o regressiva
sub_reg <- function(SOL) {
n_row <- nrow(SOL)
n_col <- ncol(SOL)
A <- SOL[1:n_row,1:n_row]
b <- SOL[,n_col]
n <- length(b)
x <- c()
x[n] <- b[n]/A[n,n]
for(i in (n-1):1) {
x[i] <- (b[i] - sum(A[i,c(i+1):n]*x[c(i+1):n] ))/A[i,i]
}
return(x)
}

EliminaÃ§Ã£o de Gauss sem pivotaÃ§Ã£o
- Resolva o sistema:
â¡
â¢
â£
3 2 6
2 4 3
5 3 4
â¤
â¥
â¦
â¡
â¢
â£
x 1
x 2
x 3
â¤
â¥
â¦
= â¡
â¢
â£
24
23
33
â¤
â¥
â¦
.
A <- matrix(c(3,2,5,2,4,3,6,3,4),3,3)
b <- c(24,23,33)
S <- gauss(A, b) ## Passo 1: TriangularizaÃ§Ã£o
sol = sub_reg(SOL = S) ## Passo 2: SubstituiÃ§Ã£o regressiva
sol
## [1] 4 3 1
A%*%sol ## Verificando a soluÃ§Ã£o
## [,1]
## [1,] 24
## [2,] 23
## [3,] 33

EliminaÃ§Ã£o de Gauss com pivotaÃ§Ã£o
- Resolva o seguinte sistema usando
EliminaÃ§Ã£o de Gauss com pivotaÃ§Ã£o.
0x 1 + 2x 2 + 3x 2 = 46
4x 1 âˆ’ 3x 2 + 2x 3 = 16
2x 1 + 4x 2 âˆ’ 3x 3 = 12
## Entrando com o sistema original
A <- matrix(c(0,4,2,2,-3,4,3,2,-3), 3,3)
b <- c(46,16,12)
## Pivoteamento
A_order <- A[order(A[,1], decreasing = TRUE),]
b_order <- b[order(A[,1], decreasing = TRUE)]
#### TriangulaÃ§Ã£o
S <- gauss(A_order, b_order)
S
## [,1] [,2] [,3] [,4]
## [1,] 4 -3.0 2.000000 16.00000
## [2,] 0 5.5 -4.000000 4.00000
## [3,] 0 0.0 4.454545 44.54545
#### SubstituiÃ§Ã£o regressiva
sol <- sub_reg(SOL = S)
sol
## [1] 5 8 10
#### SoluÃ§Ã£o
A_order%*%sol
## [,1]
## [1,] 16
## [2,] 12
## [3,] 46

EliminaÃ§Ã£o de Gauss-Jordan

MÃ©todos diretos: EliminaÃ§Ã£o de Gauss-Jordan
- O sistema original Ã© manipulado atÃ© obter um sistema equivalente na forma diagonal.
[
a11 a12 a13 a14
a21 a22 a23 a24
a31 a23 a33 a34
a41 a24 a34 a44
] [
x 1
x 2
x 3
x 4
] = [
b 1
b 2
b 3
b 4
] â†’ [
1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 1
] [
x 1
x 2
x 3
x 4
] = [
b â€²
1
b â€²
2
b â€²
3
b â€²
4
]
- Algoritmo Gauss-Jordan
1. Normalize a equaÃ§Ã£o pivÃ´ com a divisÃ£o de todos os seus termos pelo coeficiente pivÃ´.
2. Elimine os elementos fora da diagonal principal em TODAS as demais equaÃ§Ãµes usando
operaÃ§Ãµs de linha.
- O mÃ©todo de Gauss-Jordan pode ser combinado com pivotaÃ§Ã£o igual ao mÃ©todo de
eliminaÃ§Ã£o de Gauss.


MÃ©todos iterativos
- Nos mÃ©todos iterativos, as equaÃ§Ãµes sÃ£o colocadas em uma forma explÃ­cita onde cada
incÃ³gnita Ã© escrita em termos das demais, i.e.
a11x 1 + a12x 2 + a13x 3 = b 1
a21x 1 + a22x 2 + a23x 3 = b 2
a31x 1 + a32x 2 + a33x 3 = b 3
â†’
x 1 = [b 1 âˆ’ (a12x 2 + a13x 3)]/a11
x 2 = [b 2 âˆ’ (a21x 1 + a23x 3)]/a22
x 3 = [b 3 âˆ’ (a31x 1 + a32x 2)]/a33
.
- Dado um valor inicial para as incÃ³gnitas estas serÃ£o atualizadas atÃ© a convergÃªncia.
- AtualizaÃ§Ã£o: MÃ©todo de Jacobi
x i  = 1
ai i 
[b i  âˆ’ (
j =n
\sum{}
j =1;j â‰ i 
ai j x j )] i  = 1, â€¦ , n.

- AtualizaÃ§Ã£o: MÃ©todo de Gauss-Seidel
x ğ‘˜+1
1 = 1
a11
[b 1 âˆ’
j =n
\sum{}
j =2
a1j x (ğ‘˜)
j  ] ,
x (ğ‘˜+1)
i  = 1
ai i 
[b i  âˆ’ (
j =i âˆ’1
\sum{}
j =1
ai j x (ğ‘˜+1)
j  +
j =n
\sum{}
j =i +1
ai j x (ğ‘˜)
j  )] i  = 2, 3, â€¦ , n âˆ’ 1 e
x (ğ‘˜+1)
n = 1
ann
[b n âˆ’
j =nâˆ’1
\sum{}
j =1
anj x (ğ‘˜+1)
j  ] .

MÃ©todo iterativo de Jacobi
- ImplementaÃ§Ã£o computacional
jacobi <- function(A, b, inicial, max_iter = 10, tol = 1e-04) {
n <- length(b)
x_temp <- matrix(NA, ncol = n, nrow = max_iter)
x_temp[1,] <- inicial
x <- x_temp[1,]
for(j in 2:max_iter) { #### EquaÃ§Ã£o de atualizaÃ§Ã£o
for(i in 1:n) {
x_temp[j,i] <- (b[i] - sum(A[i,1:n][-i]*x[-i]))/A[i,i]
}
x <- x_temp[j,]
if(sum(abs(x_temp[j,] - x_temp[c(j-1),])) < tol) break #### CritÃ©rio de parada
}
return(list("Solucao" = x, "Iteracoes" = x_temp))
}

MÃ©todo iterativo de Jacobi
- Resolva o seguinte sistema de equaÃ§Ãµes
lineares usando o mÃ©todo de Jacobi.
9x 1 âˆ’ 2x 2 + 3x 3 + 2x 4 = 54.5
2x 1 + 8x 2 âˆ’ 2x 3 + 3x 4 = âˆ’14
âˆ’3x 1 + 2x 2 + 11x 3 âˆ’ 4x 4 = 12.5
âˆ’2x 1 + 3x 2 + 2x 3 âˆ’ 10x 4 = âˆ’21
- Computacionalmente
A <- matrix(c(9,2,-3,-2,-2,8,2,
3,3,-2,11,2,2,3,-4,10),4,4)
b <- c(54.5, -14, 12.5, -21)
ss <- jacobi(A = A, b = b,
inicial = c(0,0,0,0),
max_iter = 15)
## SoluÃ§Ã£o aproximada
ss$Solucao
## [1] 4.999502 -1.999771 2.500056 -1.000174
## SoluÃ§Ã£o exata
solve(A, b)
## [1] 5.0 -2.0 2.5 -1.0

MÃ©todos iterativo de Jacobi e Gauss-Seidel
- Em R o pacote Rlinsolve fornece
implementaÃ§Ãµes eficientes dos mÃ©todos
de Jacobi e Gauss-Seidel.
- Rlinsolve inclui suporte para matrizes
esparsas via Matrix.
- Rlinsolve Ã© implementado em C++
usando o pacote Rcpp.
A <- matrix(c(9,2,-3,-2,-2,8,2,3,3,-2,11,
2,2,3,-4,10),4,4)
b <- c(54.5, -14, 12.5, -21)
## pacote extra
require(Rlinsolve)
lsolve.jacobi(A, b)$x ## MÃ©todo de jacobi
## [,1]
## [1,] 4.9999708
## [2,] -2.0000631
## [3,] 2.5000163
## [4,] -0.9999483
lsolve.gs(A, b)$x ## MÃ©todo de Gauss-Seidell
## [,1]
## [1,] 4.999955
## [2,] -2.000071
## [3,] 2.500018
## [4,] -0.999968


### DecomposiÃ§Ã£o LU
- Nos mÃ©todos de eliminaÃ§Ã£o de Gauss e Gauss-Jordan resolvemos sistemas do tipo
$$ Ax  = b .$$
- Sendo dois sistemas
$$Ax  = b_1, e \space Ax  = b_2$$
- CÃ¡lculos do primeiro nÃ£o ajudam a resolver o segundo.
- IDEAL! - OperaÃ§Ãµes realizadas em A fossem dissociadas das operaÃ§Ãµes em b .

DecomposiÃ§Ã£o LU
- Suponha que precisamos resolver vÃ¡rios sistemas do tipo
Ax  = b .
para diferentes b â€²ğ‘ .
- OpÃ§Ã£o 1 - calcular a inversa Aâˆ’1, assim a soluÃ§Ã£o
x  = Aâˆ’1b .
- CÃ¡lculo da inversa Ã© computacionalmente ineficiente.

DecomposiÃ§Ã£o LU: algoritmo
- Decomponha (fatore) a matriz A em um produto de duas matrizes
A = LU,
onde L Ã© triangular inferior e U Ã© triangular superior.
- Baseado na decomposiÃ§Ã£o o sistema tem a forma:
LUx  = b . (3)
- Defina Ux  = y .
- Substituindo em 3 tem-se
Ly  = b . (4)
- SoluÃ§Ã£o Ã© obtida em dois passos
- Resolva Eq.(4) para obter y  usando substituiÃ§Ã£o progressiva.
- Resolva Eq.(3) para obter x  usando substituiÃ§Ã£o regressiva.

Obtendo as matrizes L e U
- MÃ©todo de eliminaÃ§Ã£o de Gauss e mÃ©todo de Crout.
- Dentro do processo de eliminaÃ§Ã£o de Gauss as matrizes L e U sÃ£o obtidas como um
subproduto, i.e.
[
a11 a12 a13 a14
a21 a22 a23 a24
a31 a32 a33 a34
a41 a41 a43 a44
] = [
1
ğ‘š21 1
ğ‘š31 ğ‘š32 1
ğ‘š41 ğ‘š42 ğ‘š43 1
] [
a11 a12 a13 a14
0 aâ€²
22 aâ€²
23 aâ€²
24
0 0 aâ€²
33 aâ€²
34
0 0 0 aâ€²
44
] .
- Os elementos ğ‘šâ€²
i j ğ‘  sÃ£o os multiplicadores que multiplicam a equaÃ§Ã£o pivÃ´.

Obtendo as matrizes L e U
- Relembre o exemplo de eliminaÃ§Ã£o de Gauss.
$$
[ [3] 2 6
2 âˆ’ 2
3 3 4 âˆ’ 2
3 2 3 âˆ’ 2
3 6
5 âˆ’ 5
3 3 3 âˆ’ 5
3 2 4 âˆ’ 5
3 6
] [ 24
23 âˆ’ 2
3 24
33 âˆ’ 5
3 24
] â†’ [[3] 2 6
0 8
3 âˆ’1
0 âˆ’ 1
3 âˆ’6
] [24
7
âˆ’7]
[3 2 6
0 [ 8
3 ] âˆ’1
0 âˆ’ 1
3 âˆ’ (âˆ’ 3
24 ) ( 8
3 ) âˆ’6 âˆ’ (âˆ’ 3
24 )(âˆ’1)
] [ 24
7
âˆ’7 âˆ’ (âˆ’ 3
24 )(7)] â†’ [3 2 6
0 [ 8
3 ] âˆ’1
0 0 âˆ’ 147
24
] [ 24
7
âˆ’ 147
24
]
- Neste caso, tem-se
L = â¡
â¢
â£
1
2
3 1
5
3 âˆ’ 3
24 1
â¤
â¥
â¦
e U = â¡
â¢
â£
3 2 6
0 8
3 âˆ’1
0 0 âˆ’ 147
24
â¤
â¥
â¦
$$

DecomposiÃ§Ã£o LU com pivotaÃ§Ã£o
- O mÃ©todo de eliminaÃ§Ã£o de Gauss foi realizado sem pivotaÃ§Ã£o.
- Como discutido a pivotaÃ§Ã£o pode ser necessÃ¡ria.
- Quando realizada a pivotaÃ§Ã£o as mudanÃ§as feitas devem ser armazenadas, tal que
PA = LU.
- P Ã© uma matriz de permutaÃ§Ã£o.
- Se as matrizes LU forem usadas para resolver o sistema
Ax  = b ,
entÃ£o a ordem das linhas de b  deve ser alterada de forma consistente com a pivotaÃ§Ã£o,
i.e. Pb .

ImplementaÃ§Ã£o: DecomposiÃ§Ã£o LU
- Podemos facilmente modificar a funÃ§Ã£o gauss() para obter a decomposiÃ§Ã£o LU.
my_lu <- function(A) {
n_row <- nrow(A)
n_col <- ncol(A)
SOL <- matrix(NA, n_row, n_col) ## Matriz para receber os resultados
SOL[1,] <- A[1,]
pivo <- matrix(0, n_col, n_row)
for(j in 1:c(n_row-1)) {
for(i in c(j+1):c(n_row)) {
pivo[i,j] <- A[i,j]/SOL[j,j]
SOL[i,] <- A[i,] - pivo[i,j]*SOL[j,]
A[i,] <- SOL[i,]
}
}
diag(pivo) <- 1
return(list("L" = pivo, "U" = SOL)) }

AplicaÃ§Ã£o: DecomposiÃ§Ã£o LU
- Fazendo a decomposiÃ§Ã£o.
LU <- my_lu(A) ## DecomposiÃ§Ã£o
LU
## $L
## [,1] [,2] [,3] [,4]
## [1,] 1.0000000 0.0000000 0.000000 0
## [2,] 0.2222222 1.0000000 0.000000 0
## [3,] -0.3333333 0.1578947 1.000000 0
## [4,] -0.2222222 0.3026316 0.279661 1
##
## $U
## [,1] [,2] [,3] [,4]
## [1,] 9 -2.000000e+00 3.000000 2.000000
## [2,] 0 8.444444e+00 -2.666667 2.555556
## [3,] 0 0.000000e+00 12.421053 -3.736842
## [4,] 0 -4.440892e-16 0.000000 10.716102
LU$L %*% LU$U ## Verificando a soluÃ§Ã£o
## [,1] [,2] [,3] [,4]
## [1,] 9 -2 3 2
## [2,] 2 8 -2 3
## [3,] -3 2 11 -4
## [4,] -2 3 2 10

AplicaÃ§Ã£o: DecomposiÃ§Ã£o LU
- Resolvendo o sistema de equaÃ§Ãµes.
## Passo 1: SubstituiÃ§Ã£o progressiva
y = forwardsolve(LU$L, b)
## Passo 2: SubstituiÃ§Ã£o regressiva
x = backsolve(LU$U, y)
x
## [1] 5.0 -2.0 2.5 -1.0
A%*%x ## Verificando a soluÃ§Ã£o
## [,1]
## [1,] 54.5
## [2,] -14.0
## [3,] 12.5
## [4,] -21.0
- FunÃ§Ã£o lu() do Matrix fornece a
decomposiÃ§Ã£o LU.
require(Matrix)
## Calcula mas nÃ£o retorna
LU_M <- lu(A)
## Captura as matrizes L U e P
LU_M <- expand(LU_M)
## SubstituiÃ§Ã£o progressiva.
y <- forwardsolve(LU_M$L, LU_M$P%*%b)
## SubstituiÃ§Ã£o regressiva
x = backsolve(LU_M$U, y)
x
## [1] 5.0 -2.0 2.5 -1.0

## Obtendo a inversa

### Obtendo a inversa via decomposiÃ§Ã£o LU
- O mÃ©todo LU Ã© especialmente adequado para o cÃ¡lculo da inversa.
- Lembre-se que a inversa de A Ã© tal que
AAâˆ’1 = I.
- O procedimento de cÃ¡lculo da inversa Ã© essencialmente o mesmo da soluÃ§Ã£o de um
sistema de equaÃ§Ãµes lineares, porÃ©m com mais incognitas.
â¡
â¢
â£
a11 a12 a13
a21 a22 a23
a31 a32 a33
â¤
â¥
â¦
â¡
â¢
â£
x 11 x 12 x 13
x 21 x 22 x 23
x 31 x 32 x 33
â¤
â¥
â¦
= â¡
â¢
â£
1 0 0
0 1 0
0 0 1
â¤
â¥
â¦
- TrÃªs sistemas de equaÃ§Ãµes diferentes, em cada sistema, uma coluna da matriz X Ã© a
incognita.

ImplementaÃ§Ã£o: inversa via decomposiÃ§Ã£o LU
- FunÃ§Ã£o para resolver o sistema usando
decomposiÃ§Ã£o LU.
```{r}
solve_lu <- function(LU, b) {
  y <- forwardsolve(LU_M$L, LU_M$P%*%b)
  x = backsolve(LU_M$U, y)
  return(x)
}
```


- Resolvendo vÃ¡rios sistemas
```{r}
my_solve <- function(LU, B) {
  n_col <- ncol(B)
  n_row <- nrow(B)
  inv <- matrix(NA, n_col, n_row)
  for(i in 1:n_col) {
    inv[,i] <- solve_lu(LU, B[,i])
  }
  return(inv)
}
```


AplicaÃ§Ã£o: inversa via decomposiÃ§Ã£o LU
- Calcule a inversa de
A = â¡
â¢
â£
3 2 6
2 4 3
5 3 4
â¤
â¥
â¦
A <- matrix(c(3,2,5,2,4,3,6,3,4),3,3)
I <- Diagonal(3, 1)
## DecomposiÃ§Ã£o LU
LU <- my_lu(A)
## Obtendo a inversa
inv_A <- my_solve(LU = LU, B = I)
inv_A
## Verificando o resultado
A%*%inv_A

CÃ¡lculo da inversa via mÃ©todo de Gauss-Jordan
- Procedimento Gauss-Jordan:
â¡
â¢
â£
a11 a21 a31 1 0 0
a21 a22 a32 0 1 0
a31 a32 a33 0 0 1
â¤
â¥
â¦
â†’ â¡
â¢
â£
1 0 0 aâ€²
11 aâ€²
21 aâ€²
31
0 1 0 aâ€²
21 aâ€²
22 aâ€²
32
0 0 1 aâ€²
31 aâ€²
32 aâ€²
33
â¤
â¥
â¦

- FunÃ§Ã£o solve() usa a decomposiÃ§Ã£o LU com pivotaÃ§Ã£o.
- R bÃ¡sico Ã© construÃ­do sobre a biblioteca lapack escrita em C.
- Veja documentaÃ§Ã£o em http://www.netlib.org/lapack/lug/node38.html.

Autovalores e autovetores
- ReduÃ§Ã£o de dimensionalidade Ã© fundamental em ciÃªncia de dados.
- AnÃ¡lise de componentes principais (PCA)
- AnÃ¡lise fatorial (AF).
- Decompor grandes e complicados relacionamentos multivariados em simples
componentes nÃ£o relacionados.
- Vamos discutir apenas os aspectos matemÃ¡ticos.

IntuiÃ§Ã£o
- Podemos decompor um vetor $\upsilon$  em duas informaÃ§Ãµes separadas: direÃ§Ã£o $d$ e tamanho $\lambda $, i.e

$$\lambda  = ||\upsilon || = \sqrt \sum{}j ğœˆ2j  , e d = \upsilon \lambda $$
- Ã‰ mais fÃ¡cil interpretar o tamanho de um vetor enquanto ignorando a sua direÃ§Ã£o e
vice-versa.
- Esta ideia pode ser estendida para matrizes.
- Uma matriz nada mais Ã© do que um conjunto de vetores.
- IDEIA - decompor a informaÃ§Ã£o de uma matriz em outros componentes de mais fÃ¡cil
interpretaÃ§Ã£o/representaÃ§Ã£o matemÃ¡tica.

Autovalores e Autovetores
- Autovalores e autovetores sÃ£o definidos por uma simples igualdade
A\upsilon  = \lambda \upsilon . (5)
- Os vetores \upsilon â€™s que satisfazem Eq. (5) sÃ£o os autovetores.
- Os valores \lambda â€™s que satisfazem Eq. (5) sÃ£o os autovalores.
- Vamos considerar o caso em que A Ã© simÃ©trica.
- A ideia pode ser estendida para matrizes nÃ£o simÃ©tricas.


- Se A Ã© uma matriz simÃ©trica n x  n, entÃ£o existem exatamente n pares (\lambda j , \upsilon j ) que
satisfazem a equaÃ§Ã£o:
A\upsilon  = \lambda \upsilon .
- Se A tem autovalores \lambda 1, â€¦ , \lambda n, entÃ£o:
- tr(A) = \sum{}n
i =1 \lambda i .
- det(A) = âˆn
i =1 \lambda i .
- A Ã© positiva definida, se e somente se todos \lambda j  > 0.
- A Ã© semi-positiva definida, se e somente se todos \lambda j  â‰¥ 0.
- A ideia do PCA Ã© decompor/fatorar a matriz A em componentes mais simples de
interpretar.

DecomposiÃ§Ã£o em autovalores e autovetores
- Teorema: qualquer matriz simÃ©trica A pode ser fatorada em
A = Q\lambda QâŠ¤,
onde \lambda  Ã© diagonal contendo os autovalores de A e as colunas de Q contÃªm os autovetores
ortonormais.
- Vetores ortonormais: sÃ£o mutuamente ortogonais e de comprimento unitÃ¡rio.
- Teorema: se A tem autovetores Q e autovalores \lambda j . EntÃ£o Aâˆ’1 tem autovetores Q e
autovalores \lambda âˆ’1
j  .
- ImplicaÃ§Ã£o: se A = Q\lambda QâŠ¤ entÃ£o Aâˆ’1 = Q\lambda âˆ’1QâŠ¤.

DiagonalizaÃ§Ã£o
- Autovalores sÃ£o utÃ©is porque eles permitem lidar com matrizes da mesma forma que
lidamos com nÃºmeros.
- Todos os cÃ¡lculos sÃ£o feitos na matriz diagonal \lambda .
- Este processo Ã© chamado de diagonalizaÃ§Ã£o.
- Um dos resultados mais poderosos em Ãlgebra Linear Ã© que qualquer matriz pode ser
diagonalizada.
- O processo de diagonalizaÃ§Ã£o Ã© chamado de DecomposiÃ§Ã£o em valores singulares.

DecomposiÃ§Ã£o em valores singulares (SVD)
- Teorema: qualquer matriz A pode ser decomposta em,
A = UDVâŠ¤,
onde D Ã© diagonal com entradas nÃ£o negativas e U e V sÃ£o ortogonais,
i.e. UâŠ¤U = VâŠ¤V = I.
- Matrizes nÃ£o quadradas nÃ£o tem autovalores.
- Os elementos de D sÃ£o chamados de valores singulares.
- Os valores singulares sÃ£o os autovalores de AâŠ¤A.

### DimensÃ£o da SVD
- Se A Ã© n x  n, entÃ£o U, D e V sÃ£o n x  n.
- Se A Ã© n x  p, sendo n > p, entÃ£o U Ã© n x  p, D e V sÃ£o p x  p.
- Se A Ã© n x  p, sendo n < p, entÃ£o VâŠ¤ Ã© n x  p, D e U sÃ£o n x  n.
- D serÃ¡ sempre quadrada com dimensÃ£o igual ao mÃ­nimo entre p e n.

### DecomposiÃ§Ã£o em autovalores e autovetores em R
- FunÃ§Ã£o eigen() fornece a decomposiÃ§Ã£o
```{r}
A <- matrix(c(1,0.8, 0.3, 0.8, 1,
0.2, 0.3, 0.2, 1),3,3)
isSymmetric.matrix(A)
## [1] TRUE
out <- eigen(A)
Q <- out$vectors ## Autovetores
D <- diag(out$values) ## Autovalores
Q
## [,1] [,2] [,3]
## [1,] -0.6712373 -0.1815663 0.71866142
## [2,] -0.6507744 -0.3198152 -0.68862977
## [3,] -0.3548708 0.9299204 -0.09651322
- Verificando a soluÃ§Ã£o
D
## [,1] [,2] [,3]
## [1,] 1.934216 0.0000000 0.0000000
## [2,] 0.000000 0.8726419 0.0000000
## [3,] 0.000000 0.0000000 0.1931419
Q%*%D%*%t(Q) ## Verificando
## [,1] [,2] [,3]
## [1,] 1.0 0.8 0.3
## [2,] 0.8 1.0 0.2
## [3,] 0.3 0.2 1.0
```


DecomposiÃ§Ã£o em valores singulares em R
- FunÃ§Ã£o svd() fornece a decomposiÃ§Ã£o
```{r}
svd(A)
## $d
## [1] 1.9342162 0.8726419 0.1931419
##
## $u
## [,1] [,2] [,3]
## [1,] -0.6712373 0.1815663 0.71866142
## [2,] -0.6507744 0.3198152 -0.68862977
## [3,] -0.3548708 -0.9299204 -0.09651322
##
## $v
## [,1] [,2] [,3]
## [1,] -0.6712373 0.1815663 0.71866142
## [2,] -0.6507744 0.3198152 -0.68862977
## [3,] -0.3548708 -0.9299204 -0.09651322
```


### RegressÃ£o ridge
- Relembrando: regressÃ£o linear mÃºltipla
â¡
â¢
â¢
â£
y 1
y 2
â‹®
y n
â¤
â¥
â¥
â¦
nx 1
= â¡
â¢
â¢
â£
1 x 11 â€¦ x p1
1 x 12 â€¦ x p1
â‹® â‹® â‹± â‹®
1 x 1n â€¦ x pn
â¤
â¥
â¥
â¦
nx p
â¡
â¢
â¢
â£
\beta 0
\beta 1
â‹®
\beta p
â¤
â¥
â¥
â¦
px 1
- Usando uma notaÃ§Ã£o mais compacta,
y
nx 1
= X
nx p \beta 
px 1
.
- Minimiza a perda quadrÃ¡tica:Ì‚
\beta  = (XâŠ¤X)âˆ’1XâŠ¤y.

### RegressÃ£o ridge
- Se p > n o sistema Ã© singular (mÃºltiplas soluÃ§Ãµes)!
- Como podemos ajustar o modelo?
- Introduzir uma penalidade pela complexidade.
- Soma de quadrados penalizada
ğ‘ƒ ğ‘†ğ‘„(\beta ) =
n
\sum{}
i =1
(y i  âˆ’ x âŠ¤
i  \beta )2 + \lambda 
p
\sum{}
j =1
\beta 2
j  .
- Matricialmente, tem-se
ğ‘ƒ ğ‘†ğ‘„(\beta ) = (y  âˆ’ X\beta )âŠ¤(y  âˆ’ X\beta ) + \lambda \beta âŠ¤\beta .
- IMPORTANTE !!
- y  centrado (mÃ©dia zero).
- X padronizada por coluna (mÃ©dia zero e variÃ¢ncia um).

RegressÃ£o ridge
- Objetivo: minizar a soma de quadrados penalizada.
- Derivada
\partial ğ‘ƒ ğ‘„ğ‘†(\beta )
\partial \beta  = \partial 
\partial \beta  [(y  âˆ’ X\beta )âŠ¤(y  âˆ’ X\beta ) + \lambda \beta âŠ¤\beta ]
= [ \partial 
\partial \beta  (y  âˆ’ X\beta )âŠ¤] (y  âˆ’ X\beta ) + (y  âˆ’ X\beta )âŠ¤ [ \partial 
\partial \beta  (y  âˆ’ X\beta )] +
\lambda  {[ \partial \beta âŠ¤
\partial \beta  ] \beta  + \beta âŠ¤ [ \partial \beta 
\partial \beta  ]}
= âˆ’2XâŠ¤(y  âˆ’ X\beta ) + 2\lambda \beta 
= âˆ’XâŠ¤(y  âˆ’ X\beta ) + \lambda \beta .

AplicaÃ§Ã£o: regressÃ£o ridge
- Resolvendo o sistema linear, tem-se
âˆ’XâŠ¤(y  âˆ’ XÌ‚\beta ) + \lambda IÌ‚ \beta  = 0
âˆ’XâŠ¤y  + XâŠ¤XÌ‚\beta  + \lambda IÌ‚ \beta  = 0
XâŠ¤XÌ‚\beta  + \lambda IÌ‚ \beta  = XâŠ¤y 
(XâŠ¤X + \lambda I)Ì‚ \beta  = XâŠ¤y Ì‚
\beta  = (XâŠ¤X + \lambda I)âˆ’1 XâŠ¤y .
- SoluÃ§Ã£o depende de \lambda .
- A inclusÃ£o de \lambda  faz o sistema ser nÃ£o singular.
- Na verdade quando fixamos \lambda  selecionamos uma soluÃ§Ã£o em particular.

AplicaÃ§Ã£o: regressÃ£o ridge
- CalcularÌ‚ \hat{\beta}  envolve a inversÃ£o de uma matriz p x  p potencialmente grande.
$$\hat{\beta}  = (XâŠ¤X + \lambda I)âˆ’1 XâŠ¤y$$
- Usando a decomposiÃ§Ã£o SVD, tem-se
$$ X = UDVâŠ¤$$
- Ã‰ possÃ­vel mostrar que,

$$ \hat{\beta} = Vdiag ( dj d2 j  + \lambda  ) UâŠ¤y .$$

ImplementaÃ§Ã£o: regressÃ£o ridge
- Simulando o conjunto de dados (n = 100, p = 200).

```{r}
set.seed(123)
X <- matrix(NA, ncol = 200, nrow = 100)
X[,1] <- 1 ## Intercepto
for(i in 2:200) {
X[,i] <- rnorm(100, mean = 0, sd = 1)
X[,i] <- (X[,i] - mean(X[,i]))/var(X[,i])
}
## ParÃ¢metros
beta <- rbinom(200, size = 1, p = 0.1)*rnorm(200, mean = 10)
mu <- X%*%beta
## ObservaÃ§Ãµes
y <- rnorm(100, mean = mu, sd = 10)
```


Implementando o modelo.
- Modelo passo-a-passo
```{r}
y_c <- y - mean(y)
X_svd <- svd(X) ## DecomposiÃ§Ã£o svd
lambda = 0.5 ## PenalizaÃ§Ã£o
DD <- Diagonal(100, X_svd$d/(X_svd$d^2 + lambda))
DD[1] <- 0 ## NÃ£o penalizar o intercepto
beta_hat = as.numeric(X_svd$v%*%DD%*%t(X_svd$u)%*%y_c)
```

Resultados: regressÃ£o ridge
- Ajustados versus verdadeiros.
```{r}
plot(beta ~ beta_hat, xlab = expression(hat(beta)), ylab = expression(beta))
```

âˆ’4 âˆ’2 0 2 4 6 8
0 2 4 6 8 10 12
\beta 
^
\beta 

Resultados: regressÃ£o ridge
- RegressÃ£o com penalizaÃ§Ã£o ridge, bem como, outras penalizaÃ§Ãµes sÃ£o eficientemente
implementadas em R via pacote glmnet.
**IMPORTANTE!** A penalizaÃ§Ã£o no glmnet Ã© ligeiramente diferente, por isso os $\hat{\beta}$â€™s nÃ£o
sÃ£o idÃªnticos a nossa implementaÃ§Ã£o naive.
- O glmnet oferece opÃ§Ãµes para selecionar $\lambda$ via validaÃ§Ã£o cruzada.

```{r}
require(glmnet)
beta_glm <- cv.glmnet(X[,-1], y_c, nlambda = 100)
```



Resultados: regressÃ£o ridge
- ValidaÃ§Ã£o cruzada.
```{r}
plot(beta_glm)
```


800 1000 1200 1400 1600 1800
Log(\lambda  )
Meanâˆ’Squared Error
103 96 94 90 88 82 80 76 74 67 60 52 43 38 32 27 22 15 9 3 0

Resultados: regressÃ£o ridge
- Ajustados (glmnet) versus verdadeiros.
plot(beta ~ as.numeric(coef(beta_glm)), xlab = expression(hat(beta)), ylab = expression(beta))âˆ’2 0 2 4 6 8
0 2 4 6 8 10 12
\beta 
^
\beta 

ComentÃ¡rios
- SoluÃ§Ã£o de sistemas lineares:
- MÃ©todos diretos: EliminaÃ§Ã£o de Gauss e Gauss-Jordan.
- MÃ©todos iterativos: Jacobi e Gauss-Seidel.
- Inversa de matrizes.
- DecomposiÃ§Ã£o ou fatorizaÃ§Ã£o
- LU resolve sistema lineares pode ser usada para obter inversas.
- Autovalores e autovetores.
- Valores singulares.
- Existem muitas outras fatorizaÃ§Ãµes: QR, Cholesky, Cholesky modificadas, etc.




```{r}

```

