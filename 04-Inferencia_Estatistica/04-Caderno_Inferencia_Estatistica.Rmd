---
title: "Caderno_Inferencia_Estatistica"
output:
  pdf_document:
  html_document: 
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
      number_sections: true
date: "`r Sys.Date()`"
---

```{r setup, echo=TRUE, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, warning = TRUE, error = TRUE)
library(ggplot2)
```

# Sumario

# Aula

## Subtitulo

### Silde

# InstrumentaÃ§Ã£o MatemÃ¡tica para EstatÃ­stica â€“ Prof Wagner Hugo Bonat â€“ 09/03/2024

- MatemÃ¡tica (5 partes)
-	Probabilidade (1 parte)
-	InferÃªncia (3 partes)

### TÃ³picos em matemÃ¡tica customizados para DS:

-	Fornecer base matemÃ¡tica para entender e criar tÃ©cnicas de anÃ¡lise de dados
-	VisÃ£o geral e intuitiva
-	Focar nos resultados e suas aplicaÃ§Ãµes 
-	NÃ£o ser exaustivo em cada tÃ³pico ou matematicamente (muito) rigoroso
-	Suporte computacional para compreender conceitos matemÃ¡ticos abstratos
-	Formar uma base sÃ³lida para entender tÃ©cnicas avanÃ§adas:
  -	Modelagem estatÃ­stica
  -	Machine learnig

### O curso nÃ£o Ã© de receitas, Ã© de fundamentos

- Os objetivos desta abordagem sÃ£o:
  -	Desmestificar o processo peos quais os algoritmos resolvem problemas
  -	Mostrar que apesar de existir um conjunto enrme de tÃ©cnicas, muitas delas sÃ£o pequenas melhorias em tÃ©cnicas jÃ¡ existentes
-	Promover um uso qualificado das ferramentas jÃ¡ disponÃ­veis

## Referencias:

-	Deep Learning, Ian Goodfellow and Yoshua Bengio and Aaro Courville, MIT Press, 2016.
-	Mathematics for Machine Learning, Marc Peter Deisenroth, A. Aldo Faisal and Cheng Soon Ong, Cambridge, 2019
-	Livro do prof: MatemÃ¡tica para CiÃªncias de Dados

## O que precisamos saber?

-	CÃ¡lculo Diferencial e Integral
  -	FunÃ§Ãµes, limites e continuidade
  -	Derivadas
  -	Integrais
-	Ãlgebra Matricial
  -	Vetores e escalares
  -	Matrizes
  -	Sistemas de equaÃ§Ãµes lineares
  -	DecomposiÃ§Ãµes matriciais
-	MÃ©todos NumÃ©ricos
  -	Sistemas de equaÃ§Ãµes nÃ£o-lineares
  -	DiferenciaÃ§Ã£o e integraÃ§Ã£o numÃ©rica
  -	OtimizaÃ§Ã£o

## Exemplos motivacionais:

### Classificador binÃ¡rio
Ferramenta popular em modelagem estatÃ­stica e aprendizagem de maquina

Objetivo: classificar um individuo ou observaÃ§Ã£o em uma entre duas categorias

Exemplos:

-	Classificar um paciente como sadio ou doente
-	Classificar um cliente como bom ou mal pagador etc
	
## Diversos algoritmos disponÃ­veis:

-	Arvores de classificaÃ§Ã£o
-	MÃ¡quinas de vetores de suporte
-	Redes neurais
-	Gradient boost
-	RegressÃ£o logÃ­stica Ã© muito popular

## DescriÃ§Ã£o matemÃ¡tica:

- Suponha que temos um conjunto de dados $y_{i}$  para $i=1,â€¦,n$ .
-	Cada $y_{i}  \in [0,1]$ (Ã© zero ou 1) -> sim ou nÃ£o, saudÃ¡vel ou doente etc

Potenciais objetivos:

-	Descrever o relacionamento de $y_{i}$ com um conjunto de variÃ¡veis explanatÃ³rios $x_{ij}$  com $j=1,â€¦,p$
-	Classificar uma nova observaÃ§Ã£o como 0 ou 1

Exemplo - Conjunto de dados com 3 colunas:

- Renda anual do usuÃ¡rio
-	Anos de experiencia do usuÃ¡rio
-	Se Ã© premium ou nÃ£o

Objetivos:

-	Identificar como as covariÃ¡veis renda e anos influenciem a compra premium
-	Predizer se um novo usuÃ¡rio serÃ¡ ou nÃ£o premium
-	Orientar campanha de marketing


```{r chunk-1}
dados_reg_log <- read.table("./Data_Files/reg_log.txt", header = TRUE)

head(dados_reg_log,n =10)

```

```{r chunk-2}
plot(dados_reg_log$Anos, dados_reg_log$Renda, main = "Anos de Exp vs Renda")
```

```{r chunk-3}
library(ggplot2)
grafico <- ggplot(dados_reg_log,aes(x = Anos, y = Renda, colour = Premium)) + geom_point()

grafico
```




$i$	$y$	$=f($ 	$x_{i1} = renda$ $x_{i2} = anos)$

$y_{i} = f (x_{ij})$
$y_{i} = f(x_{ij})+erro$
$erro = y_{i}-f (x_{ij} )$


ConstruÃ§Ã£o do classificador:

-	Explicar o modelo que descreve a relaÃ§Ã£o entre $y_{i}$ e $x_{ij}$ (i linha-usuÃ¡rio, j coluna-covariÃ¡vel)

$y=f(renda,xp)$, ou seja,  y Ã© funÃ§Ã£o dependente de renda e xp

-	 Especificar funÃ§Ã£o perda (medida de erro)

$erro=g( y_{i},f(x_{ij}))$ funÃ§Ã£o g

```{r chunk-4}
f_logit <- function(par, y, renda, anos){
  mu <- 1/(1+exp(-(par[1] + par[2]*renda + par[3]*anos)))
  SQ_logit <- sum((y - mu)^2)
  return(SQ_logit)
}


#f_logit()
```


-	CaracterÃ­sticas satisfaÃ§a duas equaÃ§Ãµes de distÃ¢ncia: 
$d(y,\mu)>0 | y= \mu$     e     $d(y,\mu)=0 | \mu=f(x_{ij})$

Otimizar a funÃ§Ã£o perda:

-	Qual algoritmo escolher?
-	Como implementÃ¡-la?
-	Analisar o modelo ajustando

## Kmeans

ClusterizaÃ§Ã£o usando kmeans

- Agrupar indivÃ­duos semelhantes
-	Individuos no mesmo grupo sejam mais parecidos do que indivÃ­duos em grupos diferentes
-	DistÃ¢ncia da media 



# Math part

## Linha reta

$$y_{i} = \beta_{0} + \beta_{1} * renda$$

## Sigmoide

$$y_{i} = \frac{1}{1 + exp^{ -(\beta_{0} + \beta_{1} * renda +  \beta_{2} * anos)}} $$

Combinando o modelo logistico com a funÃ§Ã£o perda


$$SQ_{logit}(\beta) = \sum_{i=1}^{n}(y_{1} - \frac{1}{1 + exp^{ -(\beta_{0} + \beta_{1} * renda +  \beta_{2} * anos)}})^2 $$

```{r chunk-5}
# f_logit <- funcao(par, y, renda, anos) {
#   mu <- 1 / (1 + exp(-(par[1] + par[2] * renda + par[3] * anos)))
#   SQ_logit <- sum((y - mu) ^ 2)
#   return(SQ_logit)
# }
```


# CÃ¡lculo Diferencial e Integral

## Problemas convencionais em ciÃªncia de dados

- Problemas convencionais em ciÃªncia de dados
- PrevisÃ£o ou prediÃ§Ã£o â†’ O que vai acontecer?
- ClassificaÃ§Ã£o â†’ Qual o tipo de um determinado objeto?
- Agrupamento â†’ Qual a melhor forma de agregar objetos?
- PrescriÃ§Ã£o â†’ O que devo fazer?


- Como resolvÃª-los?
- Em geral usamos algum tipo de modelo.
- O que Ã© um modelo?
- RepresentaÃ§Ã£o simplificada da realidade.
- Qual o objetivo de um modelo?
- Representar como o cientista imagina ou supÃµe que a realidade estÃ¡ sendo gerada e refletida por meio dos dados.
- CaracterÃ­sticas de um bom modelo
- Deve representar os principais aspectos do fenÃ´meno sendo avaliado.
- Pode conter uma ou mais quantidades desconhecidas (parÃ¢metros).
- Deve permitir generalizaÃ§Ãµes.
- Deve fornecer um resumo rÃ¡pido e interpretÃ¡vel do fenÃ´meno em estudo.
- Deve ser matematicamente preciso e coerente.


## FunÃ§Ãµes

- DefiniÃ§Ã£o: uma funÃ§Ã£o escrita como $y = f(x)$ associa um nÃºmero $y$ a cada valor de $x$.
- $x$ Ã© chamada de variÃ¡vel independente.
- DomÃ­nio de $f(x)$ Ã© a faixa de valores que $x$ pode assumir.
- $y$ Ã© chamada de variÃ¡vel dependente.
- Imagem de $f(x)$ Ã© a faixa de valores que $y$ pode assumir.
- Resumindo temos,

$$ \frac{x \in Dominio}{Independente} \longrightarrow f(x) \longrightarrow \frac{x \in Imagem}{Dependente}$$


- O domÃ­nio e imagem de uma funÃ§Ã£o
sÃ£o intervalos.
- Tipos de intervalos:
  - Intervalo aberto nÃ£o contÃ©m as extremidades: NotaÃ§Ã£o (a,b).
  - Intervalo fechado contÃ©m as extremidades: NotaÃ§Ã£o [a,b].
- O que entra e o que sai de uma funÃ§Ã£o?
  - Naturais: $ {N} = \{0, 1, 2, 3...\} $ {N} = \{0,1,2,3, â€¦ \}$.
  - Inteiros: ${Z} = {â€¦ , -3, -2, -1, 0, 1,2,3, â€¦}$
  - Racionais ${Q} = {ab |a, b \in {Z}, b {!=} 0}$
  - Irracionais: Conjunto de nÃºmeros que nÃ£o sÃ£o racionais.
  - Reais: UniÃ£o de todos os nÃºmeros mencionados acima, notaÃ§Ã£o {R}.
- DistinÃ§Ã£o importante ${R}$ (double) e ${Z}$
(integer).


- Considere a funÃ§Ã£o $y = x^2$.
- Em R temos
```{r chunk-6}
minha_funcao <- function(x) {
  y <- x^2
  return(y)
}
```

- Avaliando a funÃ§Ã£o em alguns pontos.
```{r}
x_vec <- c(-5, -4, -3, -2, -1,
0, 1, 2, 3, 4, 5) #concatenaÃ§Ã£o
minha_funcao(x = x_vec) #automaticamente vetorizado
```


## FunÃ§Ãµes unidimensionais

- Uma funÃ§Ã£o $y = f(x)$ Ã© dita ser de apenas uma variÃ¡vel (unidimensional). Ou seja, sÃ³ uma entrada
- Pode ser desenhada em um espaÃ§o bidimensional, o chamado $R_{2}$. Gafico de $x$ e $y$
- O espaÃ§o $R_{2}$ Ã© formado por todas as duplas ordenadas de valores reais.
- A variÃ¡vel dependente $y$ Ã© representada no eixo vertical.
- A variÃ¡vel dependente $x$ Ã© representada no eixo horizontal.

```{r}

## Avaliando a funÃ§Ã£o
y <- minha_funcao(x = x_vec)
## GrÃ¡fico da funÃ§Ã£o
plot(y ~ x_vec, xlab = "x", type = "l",
ylab = expression(y = f(x)))
points(x_vec,y)

## ou com GGPLOT

ggplot(mapping = aes(x_vec,y))

```

## FunÃ§Ãµes parametrizadas

- DefiniÃ§Ã£o - parÃ¢metro Ã© uma quantidade conhecida que indexa ou parametriza uma determinada funÃ§Ã£o.
- Os parÃ¢metros mudam o comportamento da funÃ§Ã£o e descrevem quantidades/caracterÃ­sticas de interesse.
- NotaÃ§Ã£o: $y = f(x - \theta)$, onde $\theta$ denota o parÃ¢metro.
- O conjunto de valores que $\theta$ (theta minusculo) pode assumir Ã© chamado de espaÃ§o paramÃ©trico (theta maiusculo).
- NotaÃ§Ã£o 
$$\theta \in \Theta $$
- Exemplo: $y = (x - \theta)^2$. Theta joga o grafico mais para direita ou esquerda
- Computacionalmente:

```{r}
fx <- function(x, theta) {
out <- (x - theta)^2
return(out)
}
```


```{r}
## Criar grafico
```

## FunÃ§Ãµes com vÃ¡rios parÃ¢metros

- Em geral uma funÃ§Ã£o pode ter vÃ¡rios parÃ¢metros.
- O ideal Ã© que cada parÃ¢metro controle um aspecto da funÃ§Ã£o.
- Exemplo: $y = f(x; \theta)$, onde $\theta$ Ã© um vetor de parÃ¢metros.
- FunÃ§Ã£o com dois parÃ¢metros:
$$ y = \frac{(x - \theta_{1})^2}{\theta_{2}}$$

```{r}
## Criar grafico
```



## Declividade

- A declividade mede a variaÃ§Ã£o "delta maiusculo" no valor de y dividido pela variaÃ§Ã£o no valor de x, ou seja, declividade Ã© 
$$ \frac{\Delta y}{\Delta x}$$ 
(quanto varia y quando mudamos x).
- A declividade do desenho de uma funÃ§Ã£o pode ser constante (A), positiva (B) ou negativa (C).


```{r}
## Criar grafico
```
Figura 4. Exemplos de declividade.

- O intercepto vertical Ã© o ponto no qual o grÃ¡fico cruza o eixo vertical e Ã© obtido quando $x = 0$.

## FunÃ§Ãµes com duas ou mais variÃ¡veis independentes

FunÃ§Ãµes com duas ou mais variÃ¡veis independentes
- DefiniÃ§Ã£o - uma funÃ§Ã£o escrita como $y = f(x)$ associa um nÃºmero $y$ a cada vetor de entrada $x$. (**AtenÃ§Ã£o**: $x$ Ã© um vetor nesse caso!)
- $x = (x_1, â€¦ , x_p)^T$ denota um vetor linha transposto (vetor coluna).
- Exemplo: considere a funÃ§Ã£o de duas variÃ¡veis $x_1$ e $x_2$ definida por

$$ f(x1, x2) = \sqr(25 - x21- x22)$$
,avalie a funÃ§Ã£o nos pontos $x = (0, 0)^T$, $x = (3, 0)^T$ e desenhe seu grÃ¡fico.
- Avaliando nos pontos
[[ARRUMAR]]
y =
\sqr
25 - 02 - 02 = 5 e y =
\sqr
25 - 32 - 02 = 4.

Computacionalmente
- ImplementaÃ§Ã£o computacional
```{r}
fx1x2 <- function(x) {
y = sqrt(25 - x[1]^2 - x[2]^2)
return(y)
}
entrada1 <- c(0, 0)
entrada2 <- c(3, 0)
fx1x2(x = entrada1)
## [1] 5
fx1x2(x = entrada2)
## [1] 4
```

- Avaliando uma funÃ§Ã£o bidimensional.
```{r}

entrada <- matrix(c(entrada1, entrada2),
                  ncol = 2, nrow = 2,
                  byrow = TRUE)
entrada
## [,1] [,2]
## [1,] 0 0
## [2,] 3 0

saida <- c()
for(i in 1:2) {
  saida[i] <- fx1x2(entrada[i,])
}
saida
## [1] 5 4
```

- O grÃ¡fico da funÃ§Ã£o Ã© o conjunto das triplas ordenadas (y, x1, x2) que satisfazem a funÃ§Ã£o.


## Passo-a-passo para desenhar funÃ§Ãµes bidimensionais

- Neste caso estamos no espaÃ§o $R_3$.
- (A) Montar uma grade de valores combinando valores para x1 com valores para x2.
- (B) Avaliar a funÃ§Ã£o em cada um dos pontos criados.
- (C) Representar o valor da funÃ§Ã£o no grÃ¡fico. Neste caso usando uma paleta de cores. (poderia ser uma topografia, seria uma hemi-esfera)

```{r}
## Fazer grafico
```

Figura 5. Passo-a-passo para desenhar uma funÃ§Ã£o de duas variÃ¡veis independentes.

GrÃ¡ficos bidimensionais
- Em geral usamos uma grade mais precisa.

curva de nivel ou iso-linha

```{r}
# Fazer grafico
```


Figura 6. IlustraÃ§Ã£o do grÃ¡fico de uma funÃ§Ã£o de duas variÃ¡veis de entrada.

## FunÃ§Ãµes multidimensionais

- DefiniÃ§Ã£o - uma funÃ§Ã£o escrita como $y = f(x; \theta)$ associa um nÃºmero $y$ a cada vetor de entrada $x$ e $\theta$ denota um vetor de parÃ¢metros conhecidos.
- Para funÃ§Ãµes com mais de duas variÃ¡veis de entrada nÃ£o temos uma forma simples de representaÃ§Ã£o grÃ¡fica.
- Em termos prÃ¡ticos as funÃ§Ãµes vÃ£o representar ou modelar situaÃ§Ãµes reais.
- Precisamos de funÃ§Ãµes flexÃ­veis para representar fenÃ´menos complexos.

### FunÃ§Ãµes polinÃ´miais
- FunÃ§Ãµes polinÃ´miais sÃ£o funÃ§Ãµes do tipo $$ y = \beta_{0} + \beta_{1}x + \beta_{2}$$
$$x^2 + â€¦ \beta_{p} x^p$$
- Exemplo: funÃ§Ãµes polinÃ´miais de grau atÃ© trÃªs.  
  - FunÃ§Ã£o linear: 
  $$y = \beta_0 + \beta_1x$$
  - FunÃ§Ã£o quadrÃ¡tica: 
  $$y = \beta_0 + \beta_1x + \beta_2x^2$$
  - FunÃ§Ã£o cÃºbica: 
  $$y = \beta_0 + \beta_1x + \beta_2x^2 + \beta_3x^3$$
- O grÃ¡fico de uma funÃ§Ã£o quadrÃ¡tica Ã© uma parÃ¡bola aberta para cima se $\beta_2 > 0$ ou para baixo se $\beta_2 < 0$ 

- Graficamente:
```{r}
# Fazer grafico
```


Figura 8. Exemplos de grÃ¡ficos de funÃ§Ãµes polinÃ´miais.

FunÃ§Ãµes do tipo potÃªncia
- FunÃ§Ãµes do tipo potÃªncia sÃ£o funÃ§Ãµes da forma
$$y = x^a$$
em que a Ã© um expoente constante.
- Por definiÃ§Ã£o, $x^0 = 1 $ e note que um
nÃºmero sem expoente estÃ¡ elevado a 1.
1. $$xa(xğ‘) = xa+ğ‘;$$
2. 
$$(xa)ğ‘ = xağ‘;$$
3. 
$$3. (xğ‘§)a = xa(ğ‘§a);$$
4. 
$$4. (xğ‘§ )ğ‘ = xğ‘ ğ‘§ğ‘ ;$$
5. 
$$5. 1 xa = x-a;$$
6. 
$$6. xa xğ‘ = xa-ğ‘;$$
7. 
$$7. \sqr x = x1/2;$$
8. 
$$8. a \sqrx = x1/a;$$
9. 
$$9. ğ‘ \sqrxa = xa/ğ‘.$$

## FunÃ§Ãµes exponenciais

- FunÃ§Ãµes exponenciais sÃ£o funÃ§Ãµes do tipo $y = a^x$ onde a Ã© maior que zero e diferente de 1 e $x$ Ã© o expoente.
- FunÃ§Ãµes exponenciais naturais sÃ£o funÃ§Ãµes exponenciais que tem como base
$$\lim_{n \to \infty} (1 + \frac{1}{n})n = 2.718281828$$

- Propriedades importantes:
$$
$$
1. 
$$e0 = 1.$$
2. 
$$e1 = ğ‘’ = 2.71828$$
3. 
$$e(ğ‘’b) = ğ‘’a+b.$$
4. 
$$(ğ‘’a)b = ğ‘’ab$$
5. 
$$ğ‘’ağ‘’b = ğ‘’a-b.$$

## FunÃ§Ãµes logarÃ­tmicas

- FunÃ§Ãµes logarÃ­tmicas ou logaritmo Ã© a potÃªncia Ã  qual uma dada base deve ser elevada
para se obter um particular nÃºmero.
- Logaritmos comuns utilizam a base 10 e sÃ£o escritos log10.
- Por exemplo, uma vez que $10^2 = 100$, 2 Ã© o log de 100.
- Para qualquer funÃ§Ã£o exponencial $y = a^x$, onde a Ã© a base e $x$ o expoente, 
$a$ potÃªncia Ã  qual a deve ser elevado, para obter-se $y$.

[[ARRUMAR]] log a $y = x x Ã©

## RelaÃ§Ãµes entre funÃ§Ãµes logarÃ­tmicas e exponenciais.
- Se $\log_{10}(y) = 2x$, entÃ£o $y = 10^{2x}$.
- Se $\log_{a}(y) = xz$, entÃ£o $y = a^{xz}$.
- Se $\ln(y) = 5t$, entÃ£o $y = e^{5t}$.
- Se $y = a^{3x}$, entÃ£o $\log_a(y) = 3x$.
- Se $y = 10^{6x}$, entÃ£o $\log_{10}(y) = 6x$.
- Se $y = e^{t+1}$, entÃ£o $\ln(y) = t+1$
 **ObservaÃ§Ã£o**: $e = 2.718281828459045$ = nÃºmero de Euler.

## Outras funÃ§Ãµes de interesse
- SigmÃ³ide ou logÃ­stica: $y = 1 1+ğ‘’-x$
- Tangente hiperbÃ³lica: $y = ğ‘’x-ğ‘’-x
ğ‘’x+ğ‘’-x$ .
- Linear retificada (ReLU): $y = max{0,x}$. 
(maximo entre 0 e x?. Vale 0 atÃ© o 0 e depois "sobe")
- Leaky ReLU: $y = max{ğ›¼x, x}$, onde $\alpha$ Ã© uma parÃ¢metro conhecido.

##Desenho do grÃ¡fico das funÃ§Ãµes

```{r}
#desenho aqui
```

## Normal

$$ y= (x-0)^2/\theta$$

$$ exp{-(n - \theta_{1})^2/ \theta_{2}}$$
```{r}
## fazer grafico
```

[[ARRUMAR]]
 .....normal

## Limites e continuidade

Limite de uma funÃ§Ã£o
- DefiniÃ§Ã£o - se uma funÃ§Ã£o $f(x)$ se aproxima de um nÃºmero $L$ conforme $x$ tende a um nÃºmero a vindo da direita ou da esquerda, dizemos que o limite de $f(x)$ tende a $L$ quando $x$ tende a $a$.
- NotaÃ§Ã£o
$$\lim_{x \to a} f(x) = f(a) = L$$

- O limite pode nÃ£o existir.
- Se o limite de uma funÃ§Ã£o existe ele Ã© Ãºnico.
- Considere o limite
$$\lim_{x \to 1} x+1 = 2$$

- Exemplo: 
  - Considere o limite
  
$$\lim_{x \to 1} x^2$$

[[ARRUMAR]]
lim
xâ†’1
x2 - 1
x - 1
= ?

Ã© 2

- Computacionalmente
```{r}
fx <- function(x) {
out <- (x^2 - 1)/(x - 1)
return(out)
}
fx(x = 1)
## [1] NaN
```


```{r}
## Figura 11. Desenho do grÃ¡fico da funÃ§Ã£o
```

Exemplo
- Note que
$$
$$
[[ARRUMAR]]

- DefiniÃ§Ã£o intuitiva: o limite de uma funÃ§Ã£o Ã© o valor que achamos natural para ela em um determinado ponto.
- Essa funÃ§Ã£o nÃ£o Ã© continua (no ponto)



## Continuidade de uma funÃ§Ã£o
- DefiniÃ§Ã£o - dizemos que uma funÃ§Ã£o Ã© contÃ­nua em $x = a$ se trÃªs condiÃ§Ãµes forem satisfeitas:
  - $f(a)$ existe,
  - $\lim_{x \to a} f(x)$ existe e
  - $\lim_{x \to a} f(x) = f(a)$.
- Continuidade significa que pequenas variaÃ§Ãµes na variÃ¡vel independente levam a pequenas variaÃ§Ãµes na variÃ¡vel dependente.(mudanÃ§as suaves, ou nÃ£o abruptas)
- Teorema do valor intermediÃ¡rio: se a funÃ§Ã£o $f(x)$ Ã© contÃ­nua no intervalo fechado $[a,b]$,
entÃ£o existe pelo menos um nÃºmero $c$ em $[a,b]$ tal que $f(c) = M$
- ImplicaÃ§Ã£o: se $f(x)$ Ã© contÃ­nua seu grÃ¡fico nÃ£o contÃ©m salto vertical.
- Em geral podemos pensar em funÃ§Ãµes contÃ­nuas como sendo funÃ§Ãµes suaves.

### FunÃ§Ã£o nÃ£o contÃ­nua
- Considere a funÃ§Ã£o nÃ£o continua em 0.
$$
\lim_{x \to 0} \frac{|x|}{x} = \{-1   x < 0 e 1 x > 0
$$



```{r}
## Figura 12. FunÃ§Ã£o descontinua.
```

Propriedades de limites
- Se

$$\lim_{x \to p} f(x) = L_1$$
e

$$\lim_{x \to p} g(x) = L_2$$
entÃ£o

$$\lim_{x \to p} [f(x) + g(x)] = L_1 + L_2$$
$$\lim_{x \to p} k f(x) = k$$
$$\lim_{x \to p} f(x) = k L_1$$

$$\lim_{x \to p} f(x) g(x) = \lim_{x \to p} f(x) * \lim_{x \to p} g(x) = L_1 * L_2$$
$$\lim_{x \to p} f(x) g(x) = L_1 * L_2$$
, desde que $L_2 \neq 0$.


## Derivadas

- DefiniÃ§Ã£o - derivada ordinÃ¡ria, derivada primeira, ou simplesmente, derivada de uma funÃ§Ã£o $y = f(x)$ em um ponto $x = a$ no domÃ­nio de $f$ Ã© representada por 
$$\frac{dy}{dx}$$ ou
$$yâ€²$$ ou
$$\frac{df}{dx}$$ ou
$$fâ€²(a)$$ 
Ã© o valor

$$ \frac{dy}{dx} | x=a = fâ€²(a)$$
[[ARRUMAR]]



$$
\lim_{h \to 0}  = \frac{f(a+h)-f(a)}h
$$

- InterpretaÃ§Ã£o da derivada
- Taxa de mudanÃ§a instÃ¢ntanea.
- No limite quando $x \longrightarrow a$ a derivada Ã© a reta tangente ao ponto $(a, f(a))$.
- EquaÃ§Ã£o da **reta tangente** ao ponto a: $y - f(a) = fâ€²(a)(x - a)$.(coenficiente angular Ã© $\beta 1 -> y = \beta_0 + \beta_1*x)$ [[ARRUMAR]]

## Exemplo
Obtenha a derivada de $f(x) = -x^2$
$$
f'(x) = \lim_{h \to 0} \frac{f(x + h) - f(x)}{h}
$$
$$
= \lim_{h \to 0} \frac{-(x + h)^2 - (-x2)}{h}
$$
$$
= \lim_{h \to 0} \frac{-(x^2 + 2xh + h^2) + x^2}{h}
$$
$$
= \lim_{h \to 0} \frac{-x^2 - 2xh - h^2 + x^2}{h}
$$
$$
= \lim_{h \to 0} \frac{- 2xh - h^2}{h}
$$

$$
= \lim_{h \to 0} - 2x - h = -2x
$$
$$
= fâ€²(x) = -2x
$$

[[CONFERIR]]



Obtenha a reta tangente a $f(x)$ nos
pontos $x = 2$ e $x = -2$.
- Temos:
$$f(x = 2) = -4$$
$$fâ€²(x = 2) = -4$$ 
assim
$$y - f(x = 2) = fâ€²(x = 2)(x - 2)$$
$$y - (-4) = -4(x - 2)$$
$$y + 4 = = -4x + 8$$
$$y = 4 - 4x$$
- Computacionalmente
- f(x) e fâ€²(x).
```{r}
fx <- function(x) {
out <- - x^2
return(out)
}
f_prime <- function(x) {
out <- -2*x
return(out)
}
```


- EquaÃ§Ã£o da reta y = a + b âˆ— x.
```{r}
intercept = (fx(x = 2) - f_prime(x = 2)*2)
slope <- f_prime(x = 2)
c(intercept, slope)
## [1] 4 -4
```

```{r}
## Figura 14. Desenho de uma funÃ§Ã£o e retas tangentes.
```


## Regras de derivaÃ§Ã£o
- Seja $n \neq 0$ um natural. SÃ£o vÃ¡lidas as fÃ³rmulas de derivaÃ§Ã£o:
1. Se $f(x) = c$ entÃ£o $fâ€²(x) = 0$.
2. Se $f(x) = xn$ entÃ£o $fâ€²(x) = n * n^-1$
3. Se $f(x) = x-n$ entÃ£o $fâ€²(x) = -n * -n^{-1}$
4. Se $f(x) = \frac{x_1}{n}$ entÃ£o $fâ€²(x) = \frac{1}{n} * \frac{x_1}{n} -1$

- Derivada de funÃ§Ãµes especiais
5. Se $f(x) = exp(x)$ entÃ£o $fâ€²(x) = exp(x)$.
6. Se $f(x) = ln(x)$ entÃ£o $fâ€²(x) = 1x, x > 0$
- Sendo, $f(x)$ e $g(x)$ derivÃ¡veis em $x$ e $c$ uma constante.
7. (f + ğ‘”)â€² = fâ€²(x) + ğ‘”â€²(x).
8. (ğ‘f)â€²(x) = ğ‘fâ€²(x).
9. (f â‹… ğ‘”)â€²(x) = fâ€²(x)ğ‘”(x) + f(x)ğ‘”â€²(x).
10. ( f
ğ‘” )â€²(x) = fâ€²(x)ğ‘”(x)-f(x)ğ‘”â€²(x)
[ğ‘”(x)]2 .
- Exemplo: obtenha a derivada de
f(x) = 2 + 3x.
- SoluÃ§Ã£o: fâ€²(x) = 3.
- Computacionalmente
```{r}
D(expression(2 + 3*x), name = "x")
## [1] 3
```



## Regra da cadeia
- "Uma funÃ§Ã£o dentro da outra"
- Sejam y = f(x) e x = ğ‘”(ğ‘¡) duas funÃ§Ãµes derivÃ¡veis, com ğ¼ \in ğ·f . A funÃ§Ã£o composta
â„(ğ‘¡) = f(ğ‘”(ğ‘¡)) Ã© derivÃ¡vel, sendo
â„â€²(ğ‘¡) = fâ€²(ğ‘”(ğ‘¡))ğ‘”â€²(ğ‘¡), ğ‘¡ \in ğ·ğ‘”.
- Existe uma infinidade de fÃ³rmulas de derivaÃ§Ã£o.
- Na prÃ¡tica Ã© comum usar um software de matemÃ¡tica simbÃ³lica como o wxMaxima.
- Em R as funÃ§Ãµes deriv() e deriv3().


## Exemplo regra da cadeia
- Obtenha a derivada de sin(2x3 - 4x).
1. Note que temos uma funÃ§Ã£o composta (derivada de sen e cos)
sin(ğ‘”(x)), onde ğ‘”(x) = 2x3 - 4x.
2. Usando a regra da cadeia temos:
fâ€²(ğ‘”(x)) = cos(2x3 - 4x) and ğ‘”â€²(x) = 6x2 - 4.
3. Assim, a derivada fica dada por
cos(2x3 - 4x) â‹… (6x2 - 4).
4. Computacionalmente
```{r}
D(expression( sin(2*x^3 - 4*x)), name = "x")
## cos(2 * x^3 - 4 * x) * (2 * (3 * x^2) - 4)
## ou cos(2x^3 - 4x) * (6x^2) - 4)

D(D(expression( sin(2*x^3 - 4*x)), name = "x"), name = "x")
## cos(2 * x^3 - 4 * x) * (2 * (3 * (2 * x))) - sin(2 * x^3 - 4 * x) * (2 * (3 * x^2) - 4) * (2 * (3 * x^2) - 4)
```

## Derivadas de ordem superior
- A derivada fâ€²(x) Ã© tambÃ©m chamada de derivada de primeira ordem e mede a variaÃ§Ã£o da
funÃ§Ã£o original ou primitiva.
- A derivada de segunda ordem denotada por fâ€²â€²(x) mede a taxa de variaÃ§Ã£o da primeira
derivada.
- A derivada de terceira ordem fâ€²â€²â€²(x) mede a taxa de variaÃ§Ã£o da segunda derivada e assim
por diante atÃ© a ğ‘›-Ã©sima derivada.
- NotaÃ§Ã£o comum: ğ‘‘ğ‘›y
ğ‘‘xğ‘› que Ã© interpretada como a ğ‘›-Ã©sima derivada de y em relaÃ§Ã£o a x
- Exemplo: obtenha as derivadas atÃ© a ordem 5 da funÃ§Ã£o y = 2x4 + 5x3 + 2x2.
ğ‘‘y
ğ‘‘x = 8x3 + 15x2 + 4x, ğ‘‘2y
ğ‘‘x2 = 24x2 + 30x + 4, ğ‘‘3y
ğ‘‘x3 = 48x + 30, ğ‘‘4y
ğ‘‘x4 = 48 e ğ‘‘5y
ğ‘‘x5 = 0.
daqui em diante Ã© zero


## MÃ¡ximos e mÃ­nimos
- Dizemos que um ponto $c$ Ã© um valor mÃ¡ximo relativo de $f(x)$ se existir um intervalo aberto contendo $c$, no qual $f(x)$4 esteja definida, tal que $f(c) >= f(x)$ para todo $x$ neste intervalo.
- Dizemos que um ponto $c$ Ã© um valor mÃ­nimo relativo de $f(x)$ se existir um intervalo aberto contendo $c$, no qual f(x) esteja definida, tal que f(ğ‘) â‰¤ f(x) para todo x neste intervalo. (maximo e minimo dentro de um trecho de grafico)

```{r}
## Figura 15. IlustraÃ§Ã£o de mÃ¡ximo/mÃ­nimo relativos.
```

- Multiplicando a funÃ§Ã£o por -1 invertemos a sua concavidade.


## Pontos extremos (pico do morro ou fundo do vale)
- Se f(x) existe para todos os valores de x no intervalo aberto (ğ‘,ğ‘), e se f(x) tem um extremo relativo em ğ‘, em que ğ‘ < ğ‘ < ğ‘, entÃ£o fâ€²(ğ‘) existe e fâ€²(ğ‘) = 0.
- ImplicaÃ§Ã£o - Sendo f(x) diferenciÃ¡vel os pontos extremos de f(x) vÃ£o ocorrer quando fâ€²(x) = 0.
- $f'(x)$ pode ser igual a zero mesmo nÃ£o sendo um extremo relativo.
```{r}
## Figura 16. IlustraÃ§Ã£o de uma funÃ§Ã£o onde derivada zero nÃ£o Ã© ponto extremo.
```


## MÃ¡ximos e mÃ­nimos
Seja $c$ um ponto extremo de uma funÃ§Ã£o $f(x)$ no qual $f'(c) = 0$, e suponha que $f'(x)$ exista
para todos os valores de x em um intervalo aberto contendo ğ‘. Se fâ€²â€²(ğ‘) existe, entÃ£o
- Se fâ€²â€²(ğ‘) < 0, entÃ£o f(x) tem um mÃ¡ximo relativo em ğ‘.
- Se fâ€²â€²(ğ‘) > 0, entÃ£o f(x) tem um mÃ­nimo relativo em ğ‘.
## Concavidade
- Se fâ€²â€²(ğ‘) > 0 o grÃ¡fico de f(x) Ã© cÃ´ncavo para cima em (ğ‘, f(ğ‘));
- Se fâ€²â€²(ğ‘) < 0 o grÃ¡fico de f(x) Ã© cÃ´ncavo para baixo em (ğ‘, f(ğ‘)).



Por que derivadas sÃ£o importantes?
- ObtenÃ§Ã£o de mÃ¡ximo ou mÃ­nino (relativo).

ponto extremo tem inclinaÃ§Ã£o zero (a3 na figura)


```{r}
## Figura 17. IlustraÃ§Ã£o de uma funÃ§Ã£o com a reta tangente.
```

## ReduÃ§Ã£o de dados

VocÃª jÃ¡ trabalha com dados? Se sim,
- Por qual razÃ£o vocÃª usa a mÃ©dia ou a mediana como uma medida resumo?
- VocÃª acha que existe algum procedimento mais geral que leva a obtenÃ§Ã£o destas medidas resumo?
- Se sim, como este procedimento estÃ¡ relacionado com o que vimos em relaÃ§Ã£o a funÃ§Ãµes e seu comportamento?


- Suponha que temos um conjunto de observaÃ§Ãµes $y_{i}$ para $i = 1, â€¦ , n$.
- Objetivo: resumir a informaÃ§Ã£o contida em $y_{i}$ em um Ãºnico nÃºmero, digamos $\mu$.
- Problema: como encontrar $\mu$?
- SoluÃ§Ã£o: encontrar o valor $\mu$, tal que $f(\mu) = \sum_{i=1}^{n} (y_{i} - \mu)^2 $ (soma de quadrados da diferenÃ§a de cada valor para media, ou seja o quanto eu perdi ao trocar os $y$ por $\mu$), seja a menor possÃ­vel.
- Uma vez que temos os nÃºmeros observados $y_{i}$ a Ãºnica quantidade desconhecida Ã©$\mu$.
- Note que $\mu$ Ã© o parÃ¢metro da nossa funÃ§Ã£o.
- A funÃ§Ã£o $f(\mu)$ mede o quanto perdemos em representar $y_{i}$ apenas usando $\mu$.
- FunÃ§Ãµes perda muito populares sÃ£o a perda quadrÃ¡tica, perda absoluta, minmax e a cross entropia.
- dervar e igualar a 0.

FunÃ§Ãµes em R.
```{r}
y <- c(8,9,14,10,10,15,11,5,4,13)
fmu <- function(mu, y) {
out <- sum((y - mu)^2)
return(out)
}
fmu <- Vectorize(fmu, "mu")
fmu(mu = c(10, 12, 0, 8), y = y)
## [1] 117 161
f_prime <- function(mu, y) {
out <- -2*sum(y-mu)
return(out)
}
```

Graficamente
```{r}

```

- Note que o melhor resumo dos dados de um nÃºmero, corresponde ao ponto de mÃ­nimo da funÃ§Ã£o
$$
f(y) = \sum_{i=1}^{n} (y_{i} -\mu )^2
$$

- Como o mÃ­nimo estÃ¡ relacionado com a derivada de $f(\mu)$?


## Exemplo: reduÃ§Ã£o de dados
- No ponto de mÃ­nimo/mÃ¡ximo a inclinaÃ§Ã£o da reta tangente a $f(\mu)$ Ã© zero.
- Denote por ğœ‡Ì‚ o ponto de mÃ­nimo/mÃ¡ximo de ğ‘“(ğœ‡), entÃ£o ğ‘“â€²(ğœ‡)Ì‚ = 0.
- Assim, temos (regra da cadeia!!)

$$ f(y) = \sum_{i=1}^{n} (y_{i} -\mu )^2$$
$$\varepsilon_{i} = y_{i} -\mu$$
$$  f'(\mu) = \sum_{i=1}^{n} (\varepsilon_{i})^2 $$
$$  f'(\mu) = 2 \sum_{i=1}^{n} (y_{i} -\mu) \frac{d}{d \mu} (y_{i} -\mu) $$

$$ f'(\mu) = 2 \sum_{i=1}^{n} (y_{i} -\mu) (-1)$$

$$ f'(\mu) = -2 \sum_{i=1}^{n} (y_{i} -\mu)$$

Exemplo: reduÃ§Ã£o de dados
- Agora precisamos achar o ponto ğœ‡Ì‚ tal que ğ‘“â€²(ğœ‡)Ì‚ = 0.

$$ f'(\mu_{chapeu}) = 0$$

$$  -2 \sum_{i=1}^{n} (y_{i} -\mu_{chapeu}) = 0$$
$$  -\sum_{i=1}^{n} (y_{i} -n \mu_{chapeu}) = 0$$
$$ n\mu_{chapeu} = \sum_{i=1}^{n} y_{i}$$

$$ \mu_{chapeu} = \frac{\sum_{i=1}^{n} y_{i}}{n}$$
OU SEJA MÃ‰DIA!!!

ComentÃ¡rios
- Por qual razÃ£o vocÃª usa a mÃ©dia ou a mediana como uma medida resumo?
  - Minimiza a perda quadrÃ¡tica.
  - Medida Ã³tima no sentido de perda quadrÃ¡tica.
- VocÃª acha que existe algum procedimento mais geral que leva a obtenÃ§Ã£o destas medidas resumo?
  - EspecificaÃ§Ã£o do modelo.
  - Escolha da funÃ§Ã£o perda.
  - Treinamento (otimizaÃ§Ã£o).
- Se sim, como este procedimento estÃ¡ relacionado com o que vimos em relaÃ§Ã£o a funÃ§Ãµes e
seu comportamento?
  - Estudar o comportamento de funÃ§Ãµes.

## Derivadas parciais


- Uma funÃ§Ã£o pode ter mais do que uma variÃ¡vel independente.
- A derivada parcial mede a taxa de variaÃ§Ã£o instantÃ¢nea da variÃ¡vel dependente (ğ‘¦) com relaÃ§Ã£o a variÃ¡vel independente ğ‘¥1, quando a outra variÃ¡vel independente ğ‘¥2 Ã© mantida constante.
- Como obter a derivada parcial?
- A derivada parcial em relaÃ§Ã£o a ğ‘¥1 Ã© obtida derivando ğ‘“(ğ‘¥1, ğ‘¥2) â€œfingindoâ€ que ğ‘¥2 Ã© uma constante.
- A derivada parcial de ğ‘“(ğ‘¥1, ğ‘¥2) em relaÃ§Ã£o a ğ‘¥2 Ã© obtida derivando ğ‘“(ğ‘¥1, ğ‘¥2) mantendo ğ‘¥1
constante.
- A diferenciaÃ§Ã£o parcial segue as mesmas regras da diferenciaÃ§Ã£o ordinÃ¡ria.


Exemplo
Obtenha as derivadas parciais em relaÃ§Ã£o a $x_{1}$ e  $x_{2}$ de $y = 5 x_{1}^{3} + 3 x_{1} x_{2} + 4 x_{2}^{2}$

$$\displaystyle \frac{\partial y}{\partial x_{1}} = 15 x_{1}^{2} + 3 x_{2}$$
ou
```{r}
D(expression( 5 * x^3 + 3 * x * c + 4* c^2 ), name = "x")
```


$$\displaystyle \frac{\partial y}{\partial x_{2}} = 3 x_{1} + 8 x_{2}$$
```{r}
D(expression( 5 * x^3 + 3 * x * c + 4* c^2 ), name = "c")
```



Derivadas parciais de ordem superior
- Derivadas parciais de segunda ordem
ğœ•2ğ‘“(ğ‘¥1,ğ‘¥2)
ğœ•ğ‘¥21
e ğœ•2ğ‘“(ğ‘¥1,ğ‘¥2)
ğœ•ğ‘¥22
indica que a funÃ§Ã£o foi diferenciada parcialmente em relaÃ§Ã£o a ğ‘¥1 ou ğ‘¥2 duas vezes.
- Derivada parcial cruzada (ou mista)
ğœ•2ğ‘“(ğ‘¥1,ğ‘¥2)
ğœ•ğ‘¥1ğœ•ğ‘¥2
indica que primeiro derivamos em ğ‘¥1 e depois em ğ‘¥2.
- A ordem da derivada cruzada nÃ£o importa (se ambas contÃ­nuas), ou seja
ğœ•2ğ‘“(ğ‘¥1,ğ‘¥2)
ğœ•ğ‘¥1ğœ•ğ‘¥2
=
ğœ•2ğ‘“(ğ‘¥1,ğ‘¥2)
ğœ•ğ‘¥2ğœ•ğ‘¥1
.


Exemplo: derivadas parciais de segunda ordem
- Obtenha as derivadas parciais de atÃ© segunda ordem em relaÃ§Ã£o a ğ‘¥1 e ğ‘¥2 de
ğ‘¦ = 7ğ‘¥31
+ 9ğ‘¥1ğ‘¥2 + 2ğ‘¥52
.
- Derivadas parciais de primeira ordem
ğœ•ğ‘¦
ğœ•ğ‘¥1
= 21ğ‘¥21
+ 9ğ‘¥2,
ğœ•ğ‘¦
ğœ•ğ‘¥2
= 9ğ‘¥1 + 10ğ‘¥42
.
- Derivadas parciais de segunda ordem (segunda derivadas direta)
ğœ•2ğ‘¦
ğœ•ğ‘¥21
= 42ğ‘¥1,
ğœ•2ğ‘¦
ğœ•ğ‘¥22
= 40ğ‘¥32
.
- Derivadas parciais de segunda ordem (termos cruzados)
ğœ•2ğ‘¦
ğœ•ğ‘¥1ğ‘¥2
=
ğœ•21ğ‘¥21
+ 9ğ‘¥2
ğœ•ğ‘¥2
= 9,
ğœ•2ğ‘¦
ğœ•ğ‘¥2ğ‘¥1
=
ğœ•9ğ‘¥1 + 10ğ‘¥42
ğœ•ğ‘¥1
= 9.

- As cruzadas dÃ£o sempre iguais.


## MÃ¡ximos e mÃ­nimos funÃ§Ãµes muldimensionais
- Pontos crÃ­ticos: as derivadas parciais de primeira ordem devem ser iguais a zero **simultaneamente**.
- Derivadas parciais **principais** de segunda ordem no ponto crÃ­tico forem ambas **positivas -> ponto de mÃ­nimo**.
- Derivadas parciais de segunda ordem no ponto crÃ­tico forem ambas **negativas -> ponto de mÃ¡ximo**.
- Outras situaÃ§Ãµes ver material suplementar.


## Exemplo
Considere a funÃ§Ã£o ğ‘¦ = 6ğ‘¥21
âˆ’ 9ğ‘¥1 âˆ’ 3ğ‘¥1ğ‘¥2 âˆ’ 7ğ‘¥2 + 5ğ‘¥22
. Encontre os pontos crÃ­ticos e
determine se sÃ£o de mÃ¡ximo ou minÃ­mo.
- Graficamente
```{r}

```

- Calcular as derivadas parciais de primeira ordem da funÃ§Ã£o
ğ‘¦ = 6ğ‘¥21
âˆ’ 9ğ‘¥1 âˆ’ 3ğ‘¥1ğ‘¥2 âˆ’ 7ğ‘¥2 + 5ğ‘¥22
.
- Derivando em ğ‘¥1, temos
ğœ•ğ‘¦
ğœ•ğ‘¥1
= 12ğ‘¥1 âˆ’ 9 âˆ’ 3ğ‘¥2.
- Derivando em ğ‘¥2, temos
ğœ•ğ‘¦
ğœ•ğ‘¥2
= âˆ’3ğ‘¥1 âˆ’ 7 + 10ğ‘¥2.

- Resolver o sistema de equaÃ§Ãµes
12ğ‘¥1 âˆ’ 9 âˆ’ 3ğ‘¥2 = 0
âˆ’3ğ‘¥1 âˆ’ 7 + 10ğ‘¥2 = 0.
- SoluÃ§Ã£o: ğ‘¥1 = 1 e ğ‘¥2 = 1. ## USA NA PROXIMA

- Verificar se o ponto encontrado Ã© de mÃ­nimo calculando a segunda derivada parcial principal e avaliando o seu sinal.



ğœ•2ğ‘¦
ğœ•ğ‘¥21
=
ğœ•
ğœ•ğ‘¥1
(12ğ‘¥1 âˆ’ 9 âˆ’ 3ğ‘¥2) = 12,
ğœ•2ğ‘¦
ğœ•ğ‘¥22
=
ğœ•
ğœ•ğ‘¥2
(3ğ‘¥1 âˆ’ 7 + 10ğ‘¥2) = 10.


- Calcular as derivadas cruzadas e verificar se o produto das derivadas principais Ã© maior que o produto das cruzadas

ğœ•2ğ‘¦
ğœ•ğ‘¥1ğ‘¥2
=
ğœ•12ğ‘¥1 âˆ’ 9 âˆ’ 3ğ‘¥2
ğœ•ğ‘¥2
= âˆ’3,
ğœ•2ğ‘¦
ğœ•ğ‘¥2ğ‘¥1
=
ğœ• âˆ’ 3ğ‘¥1 âˆ’ 7 + 10ğ‘¥2
ğœ•ğ‘¥2
= âˆ’3.

Assim, temos que
ğœ•2ğ‘¦
ğœ•ğ‘¥21
ğœ•2ğ‘¦
ğœ•ğ‘¥22
 (
ğœ•2ğ‘¦
ğœ•ğ‘¥1ğ‘¥2
)
2
12 â‹… 10 > (âˆ’3)2
120 > 9.

- A funÃ§Ã£o estÃ¡ em um ponto de mÃ­nimo quando examinada de todas as direÃ§Ãµes.

## Gradiente e Hessiano


## Gradiente

- Derivadas de primeira e segunda ordem aparecem com tanta frequÃªncia que receberam nomes especiais.
- O vetor gradiente de uma funÃ§Ã£o ğ‘“(ğ‘¥1,ğ‘¥2) Ã© o **vetor composto pelas derivadas primeira** de ğ‘“(ğ‘¥1,ğ‘¥2) em relaÃ§Ã£o a ğ‘¥1 e ğ‘¥2,
âˆ‡ğ‘“(ğ‘¥1,ğ‘¥2) = (
ğœ•ğ‘“(ğ‘¥1,ğ‘¥2)
ğœ•ğ‘¥1
,
ğœ•ğ‘“(ğ‘¥1,ğ‘¥2)
ğœ•ğ‘¥2
)
âŠ¤
.
- A definiÃ§Ã£o estende-se naturalmente para funÃ§Ãµes multidimensionais.
- Sendo, ğ‘“(ğ‘¥) onde ğ‘¥ Ã© um vetor ğ‘ Ã— 1 de variÃ¡veis independentes o vetor gradiente de ğ‘“(ğ‘¥) Ã© dado por
âˆ‡ğ‘“(ğ‘¥) = (
ğœ•ğ‘“(ğ‘¥)
ğœ•ğ‘¥1
, â€¦ ,
ğœ•ğ‘“(ğ‘¥)
ğœ•ğ‘¥ğ‘
)
âŠ¤
.


## Hessiano
- A matriz hessiana de uma funÃ§Ã£o ğ‘“(ğ‘¥1,ğ‘¥2) Ã© a matriz composta pelas **derivadas de segunda ordem** de ğ‘“(ğ‘¥1,ğ‘¥2), na seguinte estrutura
H = (
ğœ•2ğ‘“(ğ‘¥1,ğ‘¥2)
ğœ•ğ‘¥21
ğœ•ğ‘“(ğ‘¥1,ğ‘¥2)
ğœ•ğ‘¥1ğœ•ğ‘¥2
ğœ•ğ‘“(ğ‘¥1,ğ‘¥2)
ğœ•ğ‘¥2ğœ•ğ‘¥1
ğœ•2ğ‘“(ğ‘¥1,ğ‘¥2)
ğœ•ğ‘¥22
) .
- E para o caso multidimensional
H =
â›âœâœâœ
â
ğœ•2ğ‘“(ğ‘¥)
ğœ•ğ‘¥21
â‹¯ ğœ•ğ‘“(ğ‘¥)
ğœ•ğ‘¥1ğœ•ğ‘¥ğ‘
â‹® â‹± â‹®
ğœ•ğ‘“(ğ‘¥)
ğœ•ğ‘¥ğ‘ğœ•ğ‘¥1
â‹¯ ğœ•2ğ‘“(ğ‘¥)
ğœ•ğ‘¥2
ğ‘
ââŸâŸâŸ
â 


## SÃ©ries de Taylor

- Suponha que uma funÃ§Ã£o ğ‘“(ğ‘¥) Ã© derivÃ¡vel (ğ‘› + 1) vezes em um intervalo contendo
ğ‘¥ = ğ‘¥0.
- ExpansÃ£o em SÃ©rie de Taylor de ğ‘“(ğ‘¥) em torno de ğ‘¥ = ğ‘¥0 consiste em reescrever ğ‘“(ğ‘¥) da
seguinte forma:
ğ‘“(ğ‘¥) = ğ‘“(ğ‘¥0) + (ğ‘¥ âˆ’ ğ‘¥0)
ğ‘‘ğ‘“(ğ‘¥)
ğ‘‘ğ‘¥
|ğ‘¥=ğ‘¥0 +
(ğ‘¥ âˆ’ ğ‘¥0)2
2!
ğ‘‘2ğ‘“(ğ‘¥)
ğ‘‘ğ‘¥2 |ğ‘¥=ğ‘¥0+ (2)
(ğ‘¥ âˆ’ ğ‘¥0)3
3!
ğ‘‘3ğ‘“(ğ‘¥)
ğ‘‘ğ‘¥3 |ğ‘¥=ğ‘¥0 + â€¦ +
(ğ‘¥ âˆ’ ğ‘¥0)ğ‘›
ğ‘›!
ğ‘‘ğ‘›ğ‘“(ğ‘¥)
ğ‘‘ğ‘¥ğ‘› |ğ‘¥=ğ‘¥0 + ğ‘…ğ‘›(ğ‘¥) (3)
onde o termo ğ‘…ğ‘›(ğ‘¥) Ã© chamado de resÃ­duo ou erro, e dado por
ğ‘…ğ‘›(ğ‘¥) =
(ğ‘¥ âˆ’ ğ‘¥0)ğ‘›+1
(ğ‘› + 1)!
ğ‘‘ğ‘›+1ğ‘“(ğ‘¥)
ğ‘‘ğ‘¥ğ‘›+1 |ğ‘¥=ğœ–
sendo ğœ– um valor entre ğ‘¥ e ğ‘¥0.


## Exemplo
- Seja ğ‘“(ğ‘¥) = exp(ğ‘¥). Determine a expansÃ£o de Taylor de ordens 1 e 2, de ğ‘“(ğ‘¥) ao redor de
ğ‘¥0 = 0.
- AproximaÃ§Ã£o de primeira ordem
ğ‘ƒ1(ğ‘¥) = ğ‘“(ğ‘¥ = 0) + ğ‘“â€²(ğ‘¥ = 0)(ğ‘¥ âˆ’ 0)
= exp(0) + exp(0)(ğ‘¥ âˆ’ 0)
= 1 + ğ‘¥.
- AproximaÃ§Ã£o de segunda ordem
ğ‘ƒ2(ğ‘¥) = ğ‘“(ğ‘¥ = 0) + ğ‘“â€²(ğ‘¥ = 0)(ğ‘¥ âˆ’ 0) +
ğ‘“â€²â€²(ğ‘¥ = 0)
2
(ğ‘¥ âˆ’ 0)2
= exp(0) + exp(0)(ğ‘¥ âˆ’ 0) + exp(0)
2
(ğ‘¥ âˆ’ 0)2
= 1 + ğ‘¥ +
1
2
ğ‘¥2.

Graficos

- Quanto mais se afasta de zero pior fica a aproximaÃ§Ã£o.
- A Tailor em n+1 graus Ã© igual a original


## RegressÃ£o linear simples
- RegressÃ£o linear Ã© uma das tÃ©cnicas mais populares em ciÃªncia de dados.
- Objetivo: descrever o comportamento de uma variÃ¡vel dependente ğ‘¦ por meio do conhecimento de outra variÃ¡vel
independente ğ‘¥.
- Predizer ğ‘¦ dado um valor de ğ‘¥.
- Descrever a relaÃ§Ã£o entre ğ‘¦ e ğ‘¥.
- Exemplo
  - Como o tamanho (em metros quadrados) de um apartamento estÃ¡ associado ao seu preÃ§o (em reais)?
  - Suponha que um conjunto de 20 apartamentos foi medido e avaliado.

Grafico

RegressÃ£o linear simples
- Ideia simples! â†’ O preÃ§o deve ser uma funÃ§Ã£o do tamanho do apartamento.
- FormalizaÃ§Ã£o matemÃ¡tica:
- Denote por ğ‘¦ğ‘– para ğ‘– = 1, â€¦ ,ğ‘› o preÃ§o do apartamento ğ‘– e neste caso ğ‘› = 20.
- Denote por ğ‘¥ğ‘– o tamanho do apartamento ğ‘– em metros quadrados.
- FunÃ§Ã£o relacionando preÃ§o âˆ¼ tamanho
$$ PreÃ§o = f(metroquadrado) $$
$$ y_{i} = f^{*}(x_{i})$$

- Qual Ã© a funÃ§Ã£o ğ‘“âˆ—(ğ‘¥ğ‘–)?
- NÃ£o conhecemos e em geral nunca vamor conhecer ğ‘“âˆ—(ğ‘¥ğ‘–).
- Aproximar ğ‘“âˆ—(ğ‘¥ğ‘–) por outra funÃ§Ã£o ğ‘“(ğ‘¥ğ‘–) conhecida.
- Problema: qual ğ‘“(ğ‘¥ğ‘–) e como fazer a aproximaÃ§Ã£o!

- Uma opÃ§Ã£o Ã© usar a expansÃ£o em sÃ©rie de Taylor para obter uma aproximaÃ§Ã£o.

- AproximaÃ§Ã£o em sÃ©rie de Taylor de primeira ordem
$$ f^{*}(x) = f^{*}(x_0) + (x - x_0)f^{*'}(x) + R_n(x)$$

- Ignorando o termo residual ğ‘…ğ‘›(ğ‘¥)
ğ‘“âˆ—(ğ‘¥) â‰ˆ ğ‘“âˆ—(ğ‘¥0) + (ğ‘¥ âˆ’ ğ‘¥0)ğ‘“âˆ—â€²(ğ‘¥0).

- Rearranjando os termos obtemos
ğ‘“âˆ—(ğ‘¥) â‰ˆ âŸ{ğ‘“âŸâˆ—(âŸğ‘¥0âŸ)âŸâˆ’ ğ‘“âˆ—âŸâ€²(ğ‘¥âŸ0âŸ)ğ‘¥âŸ0}
ğ›½0
+ ğ‘“âŸâˆ—â€²(ğ‘¥0)
ğ›½1
ğ‘¥
ğ‘“âˆ—(ğ‘¥) â‰ˆ ğ›½0 + ğ›½1ğ‘¥.
- De forma equivalente, temos
ğ‘¦ğ‘– = ğ›½0 + ğ›½1ğ‘¥ğ‘– + ğ‘…ğ‘›(ğ‘¥ğ‘–),
em que o termo ğ‘…ğ‘›(ğ‘¥ğ‘–) Ã© o erro cometido em aproximar ğ‘¦ğ‘– por ğ›½0 + ğ›½1ğ‘¥ğ‘–.

- NotaÃ§Ã£o usual ğœ–ğ‘– = ğ‘¦ğ‘– âˆ’ (ğ›½0 + ğ›½1ğ‘¥ğ‘–).
- Note que o erro Ã© uma funÃ§Ã£o dos parÃ¢metros desconhecidos ğ›½0 e ğ›½1.
- Objetivo: minimizar a soma de quadrados dos erros ou resÃ­duos
$$ 
$$

ğ‘†ğ‘„(ğ›½0, ğ›½1) =
ğ‘›Î£
ğ‘–=1
ğœ–2(ğ›½0, ğ›½1)ğ‘–
ğ‘†ğ‘„(ğ›½0, ğ›½1) =
ğ‘›Î£
ğ‘–=1
(ğ‘¦ğ‘– âˆ’ (ğ›½0 + ğ›½1ğ‘¥ğ‘–))2.

- Obter o vetor gradiente
âˆ‡ğ‘†ğ‘„(ğ›½0,ğ›½1) = (
ğœ•ğ‘†ğ‘„(ğ›½0,ğ›½1)
ğœ•ğ›½0
,
ğœ•ğ‘†ğ‘„(ğ›½0, ğ›½1)
ğœ•ğ›½1
) .
- Encontrar ğ›½0Ì‚ e ğ›½1Ì‚ tal que
âˆ‡ğ‘†ğ‘„(ğ›½0Ì‚ ,ğ›½1Ì‚ ) = 0.



1. Chame ğ‘¦ğ‘– âˆ’ (ğ›½0 + ğ›½1ğ‘¥ğ‘–) = ğœ–ğ‘–.
2. Chame ğ›½0 + ğ›½1ğ‘¥ğ‘– = ğœ‡ğ‘–.
3. Assim,
âˆ‡ğ‘†ğ‘„(ğ›½0,ğ›½1) = (
ğœ•ğ‘†ğ‘„(ğ›½0,ğ›½1)
ğœ•ğœ–ğ‘–
ğœ•ğœ–ğ‘–
ğœ‡ğ‘–
ğœ•ğœ‡ğ‘–
ğœ•ğ›½0
,
ğœ•ğ‘†ğ‘„(ğ›½0,ğ›½1)
ğœ•ğœ–ğ‘–
ğœ•ğœ–ğ‘–
ğœ‡ğ‘–
ğœ•ğœ‡ğ‘–
ğœ•ğ›½1
) ,
em que
ğœ•ğ‘†ğ‘„(ğ›½0,ğ›½1)
ğœ•ğœ–ğ‘–
=
ğœ•
ğœ•ğœ–ğ‘–
ğ‘›Î£
ğ‘–=1
ğœ–2
ğ‘–
= 2
ğ‘›Î£
ğ‘–=1
ğœ–ğ‘–.
ğœ•ğœ–ğ‘–
ğœ•ğœ‡ğ‘–
=
ğœ•
ğœ•ğœ‡ğ‘–
(ğ‘¦ğ‘– âˆ’ ğœ‡ğ‘–) = âˆ’1,
ğœ•ğœ‡ğ‘–
ğœ•ğ›½0
=
ğœ•
ğœ•ğ›½0
ğ›½0 + ğ›½1ğ‘¥ğ‘– = 1,
ğœ•ğœ‡ğ‘–
ğœ•ğ›½1
=
ğœ•
ğœ•ğ›½1
ğ›½0 + ğ›½1ğ‘¥ğ‘– = ğ‘¥ğ‘–.


Vetor gradiente
- Portanto,
âˆ‡ğ‘†ğ‘„(ğ›½0,ğ›½1) = (âˆ’2
ğ‘›Î£
ğ‘–=1
ğœ–ğ‘–(1); âˆ’2
ğ‘›Î£
ğ‘–=1
ğœ–ğ‘–ğ‘¥ğ‘–)
= (âˆ’2
ğ‘›Î£
ğ‘–=1
(ğ‘¦ğ‘– âˆ’ ğ›½0 âˆ’ ğ›½1ğ‘¥ğ‘–); âˆ’2
ğ‘›Î£
ğ‘–=1
(ğ‘¦ğ‘– âˆ’ ğ›½0 âˆ’ ğ›½1ğ‘¥ğ‘–)ğ‘¥ğ‘–) .
- Resolver o sistema de equaÃ§Ãµes simultÃ¢neas (derivadas de beta 0 e beta 1)
âˆ’2
ğ‘›Î£
ğ‘–=1
(ğ‘¦ğ‘– âˆ’ ğ›½0Ì‚ âˆ’ ğ›½1Ì‚ ğ‘¥ğ‘–) = 0
âˆ’2
ğ‘›Î£
ğ‘–=1
(ğ‘¦ğ‘– âˆ’ ğ›½0Ì‚ âˆ’ ğ›½1Ì‚ ğ‘¥ğ‘–)ğ‘¥ğ‘– = 0.

- SoluÃ§Ã£o
ğ›½0Ì‚ = ğ‘¦Ì„âˆ’ ğ›½1Ì‚ ğ‘¥,Ì„
ğ›½1Ì‚ =
Î£ğ‘›
ğ‘–=1 ğ‘¦ğ‘–ğ‘¥ğ‘– âˆ’ ğ‘¦Ì„Î£ğ‘›
ğ‘–=1 ğ‘¥ğ‘–
Î£ğ‘›
ğ‘–=1 ğ‘¥2
ğ‘–
âˆ’ ğ‘¥Ì„Î£ğ‘›
ğ‘–=1 ğ‘¥ğ‘–




```{r}
## Carregando a base de dados
dados <- read.table("Data_files/reglinear.csv",
                    header = TRUE)
dados
```

```{r}
## Obtendo beta1
beta1 <- (sum(dados$y*dados$x) -
            mean(dados$y)*sum(dados$x))/
  (sum(dados$x^2) - mean(dados$x)*sum(dados$x))
# Obtendo beta0
beta0 <- mean(dados$y) - beta1*mean(dados$x)
c(beta0, beta1)
## [1] 2622.752 3608.499
## Verificando
```



```{r}
coef(lm(y ~ x, data = dados))
## (Intercept) x
## 2622.752 3608.499
```
Modelo:
ychapeu = 2622.752 + 3608.499* metrosquadrados


## DiscussÃ£o
- Derivadas sÃ£o essenciais em estatÃ­stica.
- Maximizar/minimizar funÃ§Ãµes perda/objetivo.
- O cÃ¡lculo Ã© por vezes difÃ­cil e tedioso.
- SoluÃ§Ã£o de sistemas lineares Ã© tedioso quando possÃ­vel.
- Ãlgebra linear ajuda a generalizar as soluÃ§Ãµes.
- Em situaÃ§Ãµes mais gerais expressÃµes analÃ­ticas nÃ£o serÃ£o possÃ­veis de obter.
- MÃ©todos numÃ©ricos para resoluÃ§Ã£o de sistemas lineares.
- MÃ©todos numÃ©ricos para resoluÃ§Ã£o de sistemas nÃ£o-lineares.
- MÃ©todos de otimizaÃ§Ã£o numÃ©rica.


# Aula 2024-04-06

## Integrais

Anti-derivada

### Integral indefinida

- Chamamos de integral indefinida o oposto ou o inverso da derivada, tambÃ©m chamada
de antiderivada.
- A integral indefinida da funÃ§Ã£o $f(x)$ Ã© expressa por
$$
âˆ« ğ‘“(ğ‘¥)ğ‘‘ğ‘¥ = ğ¹ (ğ‘¥) + ğ‘.
$$
- Exemplo,
$$ 
âˆ« ğ‘¥ğ‘‘ğ‘¥ = ğ‘¥2
$$
2 + ğ‘,
uma vez que se derivarmos ğ‘¥2
2 encontramos ğ‘¥

[...]

### Soma de Riemann

```{r}
soma_riemann <- function(n, a, b, fx, ...) {
  intervalos <- seq(a, b, length = n)
  ci <- c()
  soma <- c()
  for(i in 1:c(n-1)) {
    Deltai <- (intervalos[i+1] - intervalos[i]) # Tamanho do intervalo
    ci[i] <- (intervalos[i+1] + intervalos[i])/2 # Ponto central do intervalo
    soma[i] <- fx(ci[i])*Deltai # Cada elemento da soma
  }
  return(sum(soma))
}
soma_riemann <- Vectorize(soma_riemann, "n")

soma_riemann(n = 2, a = 1, b = 2, fx = function(x) x^2)

soma_riemann(n = 10, a = 1, b = 2, fx = function(x) x^2)

soma_riemann(n = 50, a = 1, b = 2, fx = function(x) x^2)

soma_riemann(n = 100, a = 1, b = 2, fx = function(x) x^2)

soma_riemann(n = 1000, a = 1, b = 2, fx = function(x) x^2)

```





### IntegraÃ§Ã£o numÃ©rica em R
- O R tem uma funÃ§Ã£o nativa para o cÃ¡lculo de integrais ?integrate.
- Exemplo:

```{r}
fx <- function(x) x^2
integrate(fx, lower = 1, upper = 2)

## 2.333333 with absolute error < 2.6e-14
```
- Outros tipos de integrais
  - Integrais multidimensionais.
  - Integrais imprÃ³prias.

### DiscussÃ£o
- Integrais sÃ£o extremamente Ãºteis para obter alguns resultados teÃ³ricos em probabilidade.
- Permitem o cÃ¡lculo de probabilidades para variÃ¡veis aleatÃ³rias contÃ­nuas.
- TÃ©cnicas (bÃ¡sicas) de modelagem estatÃ­stica e machine learning nÃ£o usam integrais
diretamente.
- Em geral integrais sÃ£o mais difÃ­ceis de calcular do que derivadas.
- Ã‰ possÃ­vel estender a ideia de integrais para funÃ§Ãµes com duas ou mais variÃ¡veis de forma
anÃ¡loga feita para derivadas.
- Integrais em alta dimensÃ£o sÃ£o extremamente difÃ­ceis de calcular e/ou aproximar
numÃ©ricamente.

# Ãlgebra Matricial

## Vetores e escalares
- Um vetor Ã© uma lista de ğ‘› nÃºmeros (escalares) escritos em linha ou coluna.
- NotaÃ§Ã£o (primeiro a em negrito)

$$
a = (a_{i1} ... a_{in})
$$
ou

$$
a = \begin{bmatrix}
a_{i1}\\
.\\
.\\
.\\
a_{in}\\
\end{bmatrix}
$$

- Vetor linha e vetor coluna.
- Um elemento do vetor Ã© chamado de ğ‘ğ‘–, sendo ğ‘– a sua posiÃ§Ã£o.
- O tamanho de um vetor Ã© o seu nÃºmero de elementos.
- O mÃ³dulo de um vetor Ã© o seu comprimento
$$
|a| = \sqrğ‘2
1 + â€¦ + ğ‘2
ğ‘›.
$$
- Vetor unitÃ¡rio Ã© aquele que tem tamanho
$$
a = a
|a| .
$$
- Dois vetores sÃ£o iguais se tem o mesmo
tamanho e os seus elementos em posiÃ§Ãµes
equivalentes sÃ£o iguais.

### OperaÃ§Ãµes com vetores

1. Soma $a + b = (ğ‘ğ‘– + ğ‘ğ‘–) = (ğ‘1 + ğ‘1, â€¦ , ğ‘ğ‘› + ğ‘ğ‘›)$.

$a = (1, 2, 3)$
$b = (3, 2, 1)$
$a+b = (4, 4, 4)$
$a-b = (-2, 0, 2)$

2. SubtraÃ§Ã£o $a âˆ’ b = (ğ‘ğ‘– âˆ’ ğ‘ğ‘–) = (ğ‘1 âˆ’ ğ‘1, â€¦ , ğ‘ğ‘› âˆ’ ğ‘ğ‘›)$.

$$
a-b = (-2, 0, 2)
$$

3. MultiplicaÃ§Ã£o por escalar $ğ›¼a = (ğ›¼ğ‘1, â€¦ , ğ›¼ğ‘ğ‘›)$.

$$
5 * a = (5*1, 5*2, 5*3)
$$

4. Transposta de um vetor:
[...]

5. Produto interno ou escalar entre dois vetores resulta em um escalar (mutiplica dois vetores e dÃ¡ um nÃºmero sÃ³ como resultado)
a â‹… b = (ğ‘1ğ‘1 + ğ‘2ğ‘2 + â€¦ + ğ‘ğ‘›ğ‘ğ‘›).

- **CondiÃ§Ãµes: os vetores devem ser do mesmo tipo e tamanho.**

### Vetores ortogonais
- Dois vetores sÃ£o ortogonais entre si se o Ã¢ngulo ğœƒ entre eles Ã© de 90âˆ˜.(= correlaÃ§Ã£o de Pearson)
- ImplicaÃ§Ãµes: 
$$cos(ğœƒ) = 0 e aâŠ¤b = 0.$$
$$ cov (a,b) / raiz(variacia[a]) * raiz(variacia[b])$$

- O co-seno do Ã¢ngulo ğœƒ entre os vetores Ã© dado por:
$$cos(ğœƒ) = aâŠ¤b / \sqraâŠ¤a\sqrbâŠ¤b .$$

### OperaÃ§Ãµes com vetores em R
- Declarando vetores
```{r}
a <- c(4,5,6)
b <- c(1,2,3)
```

- Sendo a e b compatÃ­veis
```{r}
#### Soma
a + b
## [1] 5 7 9
#### SubstraÃ§Ã£o
a - b
## [1] 3 3 3
```


- MultiplicaÃ§Ã£o por escalar
```{r}
alpha = 10
alpha*a
## [1] 40 50 60
```
- Produto de Hadamard (nÃ£o Ã© produto interno)

```{r}
a*b
## [1] 4 10 18
```
- Produto vetorial (ou produto interno)

```{r}
a%*%b
##    [,1]
## [1,] 32
```
- Co-seno do Ã¢ngulo entre dois vetores
```{r}
cos <- t(a)%*%b/(sqrt(t(a)%*%a)*sqrt(t(b)%*%b))
```
- Lei da reciclagem (nÃ£o avalia se pode somar antes de somar)
```{r}
a <- c(4,5,6,5,6,7)
b <- c(1,2,3)
a + b
## [1] 5 7 9 6 8 10
```

## Matrizes

- Uma matriz Ã© um arranjo retangular ou quadrado de nÃºmeros ou variÃ¡veis.
- A matriz costuma ser representada por uma letra maiuscula em negrito

- Uma matriz (ğ‘› Ã— ğ‘š) tem ğ‘› linhas e ğ‘š colunas:

$$A = \begin{pmatrix}\
a_{11} & a_{12} & ... & a_{1m}\\
a_{21} & a_{22} & ... & a_{2m}\\
... & ... & ... & ... \\
a_{n1} & a_{11} & ... & a_{nm}\\
\end{pmatrix}$$

- O primeiro subscrito representa linha e o segundo representa coluna.
- A dimensÃ£o de uma matriz Ã© o seu nÃºmero de linhas e colunas.
- Duas matrizes sÃ£o iguais se tem a mesma dimensÃ£o e se os elementos das correspondentes
posiÃ§Ãµes sÃ£o iguais.

### Matriz transposta
- A operaÃ§Ã£o de transposiÃ§Ã£o rearranja uma matriz de forma que suas linhas sÃ£o transformadas em colunas e vice-versa.
â›âœ
â
1 2
3 4
5 6
ââŸ
â 
âŠ¤
= (1 3 5
2 4 6) .
- Note que (AâŠ¤)âŠ¤ = A.
- Computacionalmente
- Declarando matrizes
```{r}
a <- c(1,2,3,4,5,6)
A <- matrix(a, nrow = 3, ncol = 2)
A
## [,1] [,2]
## [1,] 1 4
## [2,] 2 5
## [3,] 3 6
```
- O default preenche por colunas.
- Transposta de uma matriz
```{r}
t(A)
## [,1] [,2] [,3]
## [1,] 1 2 3
## [2,] 4 5 6
```

### OperaÃ§Ãµes com matrizes
- MultiplicaÃ§Ã£o matriz por escalar.
$$\alpha * A = \begin{pmatrix}\
\alpha * a_{11} & \alpha * a_{12} & \alpha * ... & \alpha * a_{1m}\\
\alpha * a_{21} & \alpha * a_{22} & \alpha * ... & \alpha * a_{2m}\\
\alpha * ... & \alpha * ... & \alpha * ... & \alpha * ... \\
\alpha * a_{n1} & \alpha * a_{n2} & \alpha * ... & \alpha * a_{nm}\\
\end{pmatrix}$$

- Computacionalmente
```{r}
A <- matrix(c(1,2,3,4,5,6),
nrow = 3, ncol = 2)
alpha <- 10
alpha*A
## [,1] [,2]
## [1,] 10 40
## [2,] 20 50

```

- Duas matrizes podem ser somadas ou
subtraÃ­das somente se tiverem o mesmo
tamanho.
1. Soma ğ‘ğ‘–ğ‘— = ğ‘ğ‘–ğ‘— + ğ‘ğ‘–ğ‘—.
2. SubtraÃ§Ã£o ğ‘ğ‘–ğ‘— = ğ‘ğ‘–ğ‘— âˆ’ ğ‘ğ‘–ğ‘—.
- Exemplo
$$A = \begin{pmatrix}\
1 & 2\\
3 & 4\\
5 & 6\\
\end{pmatrix}$$

$$B = \begin{pmatrix}\
10 & 20\\
30 & 40\\
50 & 60\\
\end{pmatrix}$$

$$A + B = \begin{pmatrix}\
11 & 22\\
33 & 44\\
55 & 66\\
\end{pmatrix}$$

- Soma de duas matrizes
```{r}
A <- matrix(c(1,2,3,4,5,6),
nrow = 3, ncol = 2)
B <- matrix(c(10,20,30,40,50,60),
nrow = 3, ncol = 2)
C = A + B
C
## [,1] [,2]
## [1,] 11 44
## [2,] 22 55
## [3,] 33 66
```

- CondiÃ§Ã£o para multiplicar matrizes
$$
C_{m, n} = A_{m,q} B_{q,n}
$$
(q tem que ser igual)

C
ğ‘šÃ—ğ‘› = A
ğ‘šÃ—ğ‘ B
ğ‘Ã—ğ‘›.
- Cada elemento ğ‘ğ‘–ğ‘— = âˆ‘ğ‘
ğ‘˜=1 ğ‘ğ‘–ğ‘˜ğ‘ğ‘˜ğ‘—.
â›âœ
â
2 âˆ’1
8 3
6 7
ââŸ
â 
( 4 9 1 âˆ’3
âˆ’5 2 4 6 ) =
â›âœ
â
((2 â‹… 4) + (âˆ’1 â‹… âˆ’5)) ((2 â‹… 9) + (âˆ’1 â‹… 2)) ((2 â‹… 1) + (âˆ’1 â‹… 4)) ((2 â‹… âˆ’3) + (âˆ’1 â‹… 6))
((8 â‹… 4) + (3 â‹… âˆ’5)) ((8 â‹… 9) + (3 â‹… 2)) ((8 â‹… 1) + (3 â‹… 4)) ((8 â‹… âˆ’3) + (3 â‹… 6))
((6 â‹… 4) + (7 â‹… âˆ’5)) ((6 â‹… 9) + (7 â‹… 2)) ((6 â‹… 1) + (7 â‹… 4)) ((6 â‹… âˆ’3) + (7 â‹… 6))
ââŸ
â 
=
â›âœ
â
13 16 âˆ’2 âˆ’12
17 78 20 âˆ’6
âˆ’11 68 34 24
ââŸ
â 
.

- Computacionalmente.
- Matrizes compatÃ­veis
```{r}
A <- matrix(c(2,8,6,-1,3,7),
nrow = 3, ncol = 2)
B <- matrix(c(4,-5,9,2,1,4,-3,6),
nrow = 2, ncol = 4)
C = A%*%B
C
## [,1] [,2] [,3] [,4]
## [1,] 13 16 -2 -12
## [2,] 17 78 20 -6
## [3,] -11 68 34 24
```

- Matrizes nÃ£o compatÃ­veis
```{r message=TRUE, warning=TRUE}
B %*% A
## Error in B %*% A: argumentos nÃ£o compatÃ­veis
```


Produto de Hadamard
- Produto simples ou de Hadamard

$$A \odot B = \begin{pmatrix}\
a_{11}*b_{11} & a_{12}*b_{12} & ... & a_{1m}*b_{1m}\\
a_{21}*b_{21} & a_{22}*b_{22} & ... & a_{2m}*b_{2m}\\
... & ... & ... & ... \\
a_{n1}*b_{n1} & a_{n2}*b_{n2} & ... & a_{nm}*b_{nm}\\
\end{pmatrix}$$


- Computacionalmente
```{r}
A <- matrix(c(1,2,3,4),
nrow = 2, ncol = 2)
B <- matrix(c(10,20,30,40),
nrow = 2, ncol = 2)
A*B
## [,1] [,2]
## [1,] 10 90
## [2,] 40 160
```


Propriedades envolvendo operaÃ§Ãµes com matrizes
- Sendo A, B, C e D compatÃ­veis temos,
1. $A + B = B + A$
2. $(A + B) + C = A + (B + C)$.
3. $ğ›¼(A + B) = ğ›¼A + ğ›¼B$.
4. $(ğ›¼ + ğ›½)A = ğ›¼A + ğ›½A$.
5. $ğ›¼(AB) = (ğ›¼A)B = A(ğ›¼B)$.
6. $A(B Â± C) = AB Â± AC$.
7. $(A Â± B)C = AC Â± BC$.
8. $(Aâˆ’B)(Câˆ’D) = ACâˆ’BCâˆ’AD+BD$.

- Propriedades envolvendo transposta e
multiplicaÃ§Ã£o
1. Se A Ã© ğ‘› Ã— ğ‘š e B Ã© ğ‘š Ã— ğ‘›, entÃ£o (AB)âŠ¤ = BâŠ¤AâŠ¤.
2. Se A, B e C sÃ£o compatÃ­veis 
$$
(ABC)^{âŠ¤}= C^{âŠ¤}B^{âŠ¤}A^{âŠ¤}.
$$

### Matrizes de formas especiais
- Matriz quadrada (m = n)
Exemplo 4x4
```{r}
A <- matrix(c("a11","a21","a31","a41","a12","a22","a32","a42","a13","a23","a33","a43","a14","a24","a34","a44"), nrow = 4, ncol = 4)
A
```

- ğ‘ğ‘–ğ‘– sÃ£o os elementos da diagonal.
- ğ‘ğ‘–ğ‘— para ğ‘– â‰  ğ‘— â†’ fora da diagonal.
- ğ‘ğ‘–ğ‘— para ğ‘— > ğ‘– â†’ acima da diagonal.
- ğ‘ğ‘–ğ‘— para ğ‘– > ğ‘— â†’ abaixo da diagonal.
- Matriz diagonal
$$D = \begin{pmatrix}\
a_{11} & 0 & 0 & 0\\
0 & a_{22} & 0 & 0\\
0 & 0 & a_{33} & 0 \\
0 & 0 & 0 & a_{44}\\
\end{pmatrix}$$



- Matriz identidade
I = â›âœâœâœâœ
â
1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 1
ââŸâŸâŸâŸ
â 

### Matrizes de formas especiais
- Triangular superior
U = â›âœâœâœâœ
â
ğ‘11 ğ‘12 ğ‘13 ğ‘14
0 ğ‘22 ğ‘23 ğ‘24
0 0 ğ‘33 ğ‘34
0 0 0 ğ‘44
ââŸâŸâŸâŸ
â 
.
- Triangular inferior
L = â›âœâœâœâœ
â
ğ‘11 0 0 0
ğ‘21 ğ‘22 0 0
ğ‘31 ğ‘32 ğ‘33 0
ğ‘41 ğ‘42 ğ‘43 ğ‘44
ââŸâŸâŸâŸ
â 
.
- Matriz nula
0 = â›âœâœâœâœ
â
0 0 0 0
0 0 0 0
0 0 0 0
0 0 0 0
ââŸâŸâŸâŸ
â 
.
- Matriz quadrada simÃ©trica
A = â›âœâœâœâœ
â
1 0.8 0.6 0.4
0.8 1 0.2 0.4
0.6 0.2 1 0.1
0.4 0.4 0.1 1
ââŸâŸâŸâŸ
â 


### CombinaÃ§Ãµes lineares
- Um conjunto de vetores a1, a2, â€¦ , ağ‘› Ã© dito ser linearmente dependente se puderem ser
encontrados escalares ğ‘1, ğ‘2, â€¦ , ğ‘ğ‘› e estes escalares nÃ£o sejam todos iguais a 0 de tal forma
que
$$
ğ‘1a1 + ğ‘2a2 + â€¦ + ğ‘ğ‘›ağ‘› = 0.
$$

Exemplo:
```{r}
a1 <- matrix(c(1,0), nrow = 2, ncol = 1)
a1
a2 <- matrix(c(0,1), nrow = 2, ncol = 1)
a2

#O unico caso que esses c1*a1 + c2*a2 = (0, 0) Ã© se c1 = 0 E c2 =0
#Ou seja Linearmente independente

a1 <- matrix(c(1,2), nrow = 2, ncol = 1)
a1
a2 <- matrix(c(-1,-2), nrow = 2, ncol = 1)
a2

#Existem casos fora os cs = 0 que fazem c1*a1 + c2*a2 = (0, 0)
#Ou seja Linearmente dependente

```


- Caso contrÃ¡rio Ã© dito ser linearmente independente.
- NotaÃ§Ã£o matricial
$$
Ac = 0.
$$
- As colunas de A sÃ£o linearmente independentes se Ac = 0 implicar que c = 0.

### Rank ou posto de uma matriz
- O rank ou posto de qualquer matriz quadrada ou retangular A Ã© definido como
rank(A) = nÃºmero de colunas ou linhas linearmente independentes em A.
- Sendo A uma matriz retangular ğ‘› Ã— ğ‘š o maior rank possÃ­vel para A Ã© o min(ğ‘›,ğ‘š).
- O rank da matrix nula Ã© 0.
- Se o rank da matriz Ã© o min(ğ‘›,ğ‘š) dizemos que a matriz tem rank completo.

### Matriz nÃ£o singular e matriz inversa
- Uma matriz quadrada de posto completo Ã© chamada de nÃ£o singular.
- Sendo A quadrada de posto completo a matriz inversa de A Ã© Ãºnica tal que (sÃ³ se a matriz for quadrada e de ranking completo)
$$
AA^{âˆ’1} = I.
$$
- NÃ£o quadrada (posto incompleto) â†’ nÃ£o terÃ¡ inversa e Ã© dita ser singular.
- Note que 
$$
A^{(-1^{-1})} =A
$$
A^{âˆ’1}^{âˆ’1} = A

## Matriz inversa
- Computacionalmente
```{r}
A <- matrix(c(4, 2, 7, 6), 2, 2)
A

A_inv <- solve(A)
A_inv

I = A %*% A_inv
I

```

- Verificando
```{r}
A%*%A_inv
## [,1] [,2]
## [1,] 1 0
## [2,] 0 1
```


- Propriedades envolvendo inversas
1. Se A Ã© nÃ£o singular, entÃ£o AâŠ¤ Ã© nÃ£o singular e sua inversa Ã© dada por
(AâŠ¤)âˆ’1 = (Aâˆ’1)âŠ¤.
2. Se A e B sÃ£o matrizes nÃ£o singulares de mesmo tamanho, entÃ£o o produto AB Ã©
nÃ£o singular e
(AB)âˆ’1 = Bâˆ’1Aâˆ’1.

### Inversa generalizada
- A inversa generalizada de uma matriz A ğ‘› Ã— ğ‘ Ã© qualquer matriz Aâˆ’ que satisfaÃ§a 
$$
AA^{âˆ’}A = A.
$$

- NÃ£o Ã© Ãºnica exceto quando A Ã© nÃ£o-singular (inversa usual).
- Exemplo

$$

$$

a = â›âœâœâœâœ
â
1
2
3
4
ââŸâŸâŸâŸ
â 
.


- aâˆ’ = (1, 0, 0, 0)

- Verificando

```{r}
a <- matrix(c(1, 2, 3, 4), 4, 1)
a_invg <- matrix(c(1,0,0,0), 1, 4)
a%*%a_invg%*%a
## [,1]
## [1,] 1
## [2,] 2
## [3,] 3
## [4,] 4
```
- Moore-Penrose generalized inverse
```{r}
#### Matriz singular (col 3 = col 2 + col 1)
A <- matrix(c(2, 1, 3, 2, 0,
2, 3, 1, 4), 3, 3)
library(MASS)
A_ginv <- ginv(A)
A%*%A_ginv%*%A ## Verificando
```

## Matrizes positivas definidas

### Formas quadrÃ¡ticas
- Soma de quadrados sÃ£o importantes em ciÃªncia de dados.
- Considere uma matriz A simÃ©trica e y um vetor, o produto
$$
y^{T}Ay = 
\sum(a_{ij}y^{2}_{i}) + 
\sum_{i \differ j}(a_{ij}y_{i}y_{j})
$$
Ã© chamado de forma quadrÃ¡tica.

$$
y^{T}Iy = \sum^{n}_{i=0}(y^{2}_{i})
$$


- Sendo y de dimensÃ£o ğ‘› Ã— 1, 
$$
yâŠ¤Iy = ğ‘¦2
1 + ğ‘¦2
2 + â€¦ , ğ‘¦2
n
$$

- Consequentemente, yâŠ¤y Ã© a soma de quadrados dos elementos do vetor y.
- A raiz quadrada da soma de quadrados Ã© o comprimento de y.
----  28
Matriz positiva definida
- Sendo A uma matriz simÃ©trica com a propriedade yâŠ¤Ay > 0 para todos os possÃ­veis y
exceto para quando y = 0, entÃ£o a forma quadrÃ¡tica yâŠ¤Ay Ã© chamada positiva definida,
e A Ã© dita ser uma matriz positiva definida.
- Exemplo
A = ( 2 âˆ’1
âˆ’1 3 ) .
A forma quadrÃ¡tica associada Ã© dada por (ver abaixo) que Ã© claramente positiva, desde que ğ‘¦1 e ğ‘¦2 sejam diferentes de zero.
$$
yâŠ¤Ay = (ğ‘¦1 ğ‘¦2) ( 2 âˆ’1
âˆ’1 3 ) (ğ‘¦1
ğ‘¦2
) = 2ğ‘¦2
1 âˆ’ 2ğ‘¦1ğ‘¦2 + 3ğ‘¦2
2 ,
$$

### Propriedades de matrizes positivas definidas
1. Se A Ã© positiva definida, entÃ£o todos os valores da diagonal de A sÃ£o positivos.
2. Se A Ã© positiva semi-definida, entÃ£o os elementos da diagonal de A sÃ£o maiores ou iguais a zero.
3. Sendo P uma matriz nÃ£o-singular e A uma matriz positiva definida, o produto PâŠ¤AP Ã© positiva definida.
4. Sendo P uma matriz nÃ£o-singular e A uma matriz positiva semi-definida, o produto PâŠ¤AP Ã© positiva semi-definida.
5. Uma matriz positiva definida Ã© nÃ£o-singular.

### Determinante de uma matriz
- O determinante de uma matriz A Ã© o escalar (= numero)
$$
|A| = \sum((-1)^k a_{1j_{1}} a_{2j_{2}} ... a_{nj_{n}})
$$
onde a soma Ã© realizada para todas as ğ‘›! permutaÃ§Ãµes de grau ğ‘›, e ğ‘˜ Ã© o nÃºmero de
mudanÃ§as necessÃ¡rias para que os segundos subscritos sejam colocados na ordem
1,2, â€¦ , ğ‘›.
- Considere a matriz
A = ( 3 âˆ’2
âˆ’2 4 ) .
|A| = (âˆ’1)0ğ‘11ğ‘22 + (âˆ’1)1ğ‘12ğ‘21 = 1 â‹… (3 â‹… 4) âˆ’ (âˆ’2 â‹… âˆ’2) = 12 âˆ’ 4 = 8.
----  32
Determinante de uma matriz
- Computacionalmente.
A <- matrix(c(3,-2,-2,4),2,2)
determinant(A, logarithm = FALSE)$modulus
## [1] 8
## attr(,"logarithm")
## [1] FALSE
- Determinante em escala log.
determinant(A, logarithm = TRUE)$modulus
## [1] 2.079442
## attr(,"logarithm")
## [1] TRUE
- Alguns aspectos interessantes sobre
determinantes sÃ£o:
1. Se A Ã© singular, |A| = 0.
2. Se A Ã© nÃ£o singular, |A| â‰  0.
3. Se A Ã© positiva definida, |A| > 0.
4. |AâŠ¤| = |A|.
5. Se A Ã© nÃ£o singular, |Aâˆ’1| = 1
|A| .
----  33
TraÃ§o de uma matriz
- O traÃ§o de uma matriz A ğ‘› Ã— ğ‘› Ã© um
escalar definido como a soma dos
elementos da diagonal, tr(A) = âˆ‘ğ‘›
ğ‘–=1 ğ‘ğ‘–ğ‘–.
- Propriedades
1. Se A e B sÃ£o ğ‘› Ã— ğ‘›, entÃ£o
tr(A + B) = tr(A) + tr(B).
2. Se A Ã© ğ‘› Ã— ğ‘ e B e ğ‘ Ã— ğ‘›, entÃ£o
tr(AB) = tr(BA).
- Computacionalmente
```{r}
A <- matrix(c(3,-2,-2,4),2,2)
sum(diag(A))
## [1] 7
```

## CÃ¡lculo vetorial e matricial

### CÃ¡lculo vetorial
- Seja $y = f(x)$ uma funÃ§Ã£o das variÃ¡veis $x_{1}, x_{2}, x_{3}, ... , x_{p}$ e ğœ•ğ‘¦ as respectivas derivadas parciais.

$$
\matrix_vertical de (a1x1, a2x2, .... apxp) 
$$

$$
ğœ•ğ‘¥1
, ğœ•ğ‘¦
ğœ•ğ‘¥2
, â€¦ , ğœ•ğ‘¦
ğœ•ğ‘¥ğ‘
$$

Assim,
$$
\deriv
$$


ğœ•ğ‘¦
ğœ•x =
â›âœâœâœâœâœ
â
ğœ•ğ‘¦
ğœ•ğ‘¥1
ğœ•ğ‘¦
ğœ•ğ‘¥2
â‹®
ğœ•ğ‘¦
ğœ•ğ‘¥ğ‘
ââŸâŸâŸâŸâŸ
â 
.

### CÃ¡lculo vetorial
- Sendo aâŠ¤ = (ğ‘1, ğ‘2, â€¦ , ğ‘ğ‘) um vetor de constantes e A uma matriz simÃ©trica de constantes.
1. Seja ğ‘¦ = aâŠ¤x = xâŠ¤a. EntÃ£o,
$$
ğœ•ğ‘¦
ğœ•x = ğœ•(xâŠ¤a)
ğœ•x = a.
$$
2. Seja ğ‘¦ = xâŠ¤Ax. EntÃ£o,
$$
ğœ•ğ‘¦
ğœ•x = ğœ•(xâŠ¤Ax)
ğœ•x = 2Ax.
$$

### CÃ¡lculo Matricial
- Se ğ‘¦ = ğ‘“(X) onde X Ã© uma matriz ğ‘ Ã— ğ‘. As derivadas parciais de ğ‘¦ em relaÃ§Ã£o a cada ğ‘¥ğ‘–ğ‘—
sÃ£o organizadas em uma matriz.
$$
ğœ•ğ‘¦
ğœ•X = â›âœâœ
â
ğœ•ğ‘¦
ğœ•ğ‘¥11
â€¦ ğœ•ğ‘¦
ğœ•ğ‘¥1ğ‘
â‹® â‹± â‹®
ğœ•ğ‘¦
ğœ•ğ‘¥ğ‘1
â€¦ ğœ•ğ‘¦
ğœ•ğ‘¥ğ‘ğ‘
ââŸâŸ
$$


- Algumas derivadas importantes envolvendo matrizes sÃ£o apresentadas abaixo.
1. Seja ğ‘¦ = tr(XA) sendo X ğ‘ Ã— ğ‘ e definida positiva e A ğ‘ Ã— ğ‘ constantes. EntÃ£o,
$$
ğœ•ğ‘¦
ğœ•X = ğœ•tr(XA)
ğœ•X = A + AâŠ¤ âˆ’ diag(A).
$$
2. Sendo A nÃ£o singular com derivadas ğœ•A
$$
ğœ•ğ‘¥ . EntÃ£o,
ğœ•Aâˆ’1
ğœ•ğ‘¥ = âˆ’Aâˆ’1 ğœ•A
ğœ•ğ‘¥ Aâˆ’1.
$$
3. Sendo A ğ‘› Ã— ğ‘› positiva definida. EntÃ£o,
$$
ğœ• log |A|
ğœ•ğ‘¥ = tr (Aâˆ’1 ğœ•A
ğœ•ğ‘¥ )
$$

## RegressÃ£o linear mÃºltipla

### RegressÃ£o linear mÃºltipla: especificaÃ§Ã£o usual

- RegressÃ£o linear simples
$$
y_{i} = \beta_{0} +\beta_{1}x_{1} + erro_{i}
$$
- RegressÃ£o linear mÃºltipla
$$
y_{i} = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2} + ... + \beta_{p}x_{ip} + erro_{i}
$$
- Modelo para cada observaÃ§Ã£o
$$y_{1} = \beta_{0} + \beta_{1}x_{11} + \beta_{2}x_{12} + ... + \beta_{p}x_{1p} + erro_{1}$$

$$y_{2} = \beta_{0} + \beta_{1}x_{21} + \beta_{2}x_{22} + ... + \beta_{p}x_{2p} + erro_{1}$$
$$...$$
$$y_{n} = \beta_{0} + \beta_{1}x_{n1} + \beta_{2}x_{n2} + ... + \beta_{p}x_{np} + erro_{n}$$

RegressÃ£o linear mÃºltipla: especificaÃ§Ã£o matricial
- NotaÃ§Ã£o matricial
$$
\begin{bmatrix}
y_{1}\\
y_{2}\\
...\\
y_{n}\\
\end{bmatrix}
= 
\begin{bmatrix}
1 & x_{1}\\
1 & x_{2}\\
1 & ...\\
1 & x_{n}\\
\end{bmatrix}
\times
\begin{bmatrix}
\beta_{1}\\
\beta_{2}\\
...\\
\beta_{n}\\
\end{bmatrix}
+
\begin{bmatrix}
erro_{1}\\
erro_{2}\\
...\\
erro_{n}\\
\end{bmatrix}
$$

- NotaÃ§Ã£o mais compacta
$$
y_{(n \times 1)} = X_{(n \times p)} \beta_{(p \times 1)} + erro_{(n \times 1)}
$$
### RegressÃ£o linear mÃºltipla: estimaÃ§Ã£o (treinamento)
- Objetivo: encontrar o vetorÌ‚ ğ›½, tal que $ğ‘†ğ‘„(ğ›½) = (y âˆ’ Xğ›½)âŠ¤(y âˆ’ Xğ›½) $ seja a menor possÃ­vel.

### RegressÃ£o linear mÃºltipla: estimaÃ§Ã£o
1. Passo 1: encontrar o vetor gradiente. Derivando em ğ›½, temos
$$
ğœ•ğ‘†ğ‘„(ğ›½)
ğœ•ğ›½ = ğœ•
ğœ•ğ›½ (y âˆ’ Xğ›½)âŠ¤(y âˆ’ Xğ›½)
= ğœ•
ğœ•ğ›½ ((y âˆ’ Xğ›½)âŠ¤) (y âˆ’ Xğ›½) + (y âˆ’ Xğ›½)âŠ¤ ğœ•
ğœ•ğ›½ (y âˆ’ Xğ›½)
= âˆ’XâŠ¤(y âˆ’ Xğ›½) + (y âˆ’ Xğ›½)âŠ¤(âˆ’X)
= âˆ’2XâŠ¤(y âˆ’ Xğ›½).
$$

### RegressÃ£o linear mÃºltipla: estimaÃ§Ã£o

2. Passo 2: resolver o sistema de equaÃ§Ãµes lineares (esquece o "-2" primeiro)
$$ X^{T} (y - X\hat{\beta}) = 0$$
$$XâŠ¤y âˆ’ XâŠ¤XÌ‚ğ›½ = 0$$
$$XâŠ¤XÌ‚ğ›½ = XâŠ¤yÌ‚$$
$$(XTX)^{-1}  XâŠ¤XÌ‚ğ›½ = XâŠ¤y (XTX)^{-1}$$
$$ Iğ›½ = (XâŠ¤X)âˆ’1XâŠ¤y $$

### RegressÃ£o linear mÃºltipla: exemplo
- Conjunto de dados Boston disponÃ­vel no pacote MASS.
- Cinco primeiras covariÃ¡veis disponÃ­veis:
  - crim: taxa de crimes per capita.
  - zn: proporÃ§Ã£o de terrenos residenciais zoneados para lotes com mais de 25.000 pÃ©s quadrados.
  - indus: proporÃ§Ã£o de acres de negÃ³cios nÃ£o varejistas por cidade.
  - chas: variÃ¡vel dummy de Charles River (1 se a Ã¡rea limita o rio; 0 caso contrÃ¡rio).
  - nox: concentraÃ§Ã£o de Ã³xido de nitrogÃªnio (parte por 10 milhÃµes).
- VariÃ¡vel resposta: medv valor mediano das casas ocupadas em $1000.

### RegressÃ£o linear mÃºltipla: implementaÃ§Ã£o computacional
- Carregando a base de dados 
```{r}
require(MASS)
## Carregando pacotes exigidos: MASS

data(Boston)
head(Boston[, c(1:5,14)])
## crim zn indus chas nox medv
## 1 0.00632 18 2.31 0 0.538 24.0
## 2 0.02731 0 7.07 0 0.469 21.6
## 3 0.02729 0 7.07 0 0.469 34.7
## 4 0.03237 0 2.18 0 0.458 33.4
## 5 0.06905 0 2.18 0 0.458 36.2
## 6 0.02985 0 2.18 0 0.458 28.7
```


- Matriz de delineamento (X).
```{r}
X <- model.matrix(~ crim + zn + indus +
chas + nox, data = Boston)
head(X)
## (Intercept) crim zn indus chas nox
## 1 1 0.00632 18 2.31 0 0.538
## 2 1 0.02731 0 7.07 0 0.469
## 3 1 0.02729 0 7.07 0 0.469
## 4 1 0.03237 0 2.18 0 0.458
## 5 1 0.06905 0 2.18 0 0.458
## 6 1 0.02985 0 2.18 0 0.458
```


- VariÃ¡vel resposta
```{r}
y <- Boston$medv
```

- Estimadores de mÃ­nimos quadrados:
$$
\hat{\beta} = (X^{T}X)^{-1} X^{T}y
$$
- Computacionalmente: versÃ£o ingÃªnua (calcula inversa)
```{r}
round(solve(t(X)%*%X)%*%t(X)%*%y, 2)
## [,1]
## (Intercept) 29.49
## crim -0.22
## zn 0.06
## indus -0.38
## chas 7.03
## nox -5.42
```

- Computacionalmente: versÃ£o eficiente (escalona?)
```{r}
round(solve(t(X)%*%X, t(X)%*%y), 2)
## [,1]
## (Intercept) 29.49
## crim -0.22
## zn 0.06
## indus -0.38
## chas 7.03
## nox -5.42
```

- FunÃ§Ã£o nativa do R
```{r}
t(round(coef(lm(medv ~ crim + zn + indus + chas + nox, data = Boston)), 2))
## (Intercept) crim zn indus chas nox
## [1,] 29.49 -0.22 0.06 -0.38 7.03 -5.42
```

### Matrizes esparsas (tÃ³pico adicional)

- Matrizes aparecem em todos os tipos de aplicaÃ§Ã£o em ciÃªncia de dados.
- Modelos estatÃ­sticos, machine learning, anÃ¡lise de texto, anÃ¡lise de cluster, etc.
- Muitas vezes as matrizes usadas tÃªm uma grande quantidade de zeros.
- Quando uma matriz tem uma quantidade considerÃ¡vel de zeros, dizemos que ela Ã©
esparsa, caso contrÃ¡rio dizemos que a matriz Ã© densa.
- Todas as propriedades que vimos para matrizes em geral valem para matrizes esparsas.
- O R tem um conjunto de mÃ©todos altamente eficiente por meio do pacote Matrix.
- Saber que uma matriz Ã© esparsa Ã© Ãºtil pois permite:
- Planejar formas de armazenar a matriz em memÃ³ria.
- Economizar cÃ¡lculos em algoritmos numÃ©ricos (multiplicaÃ§Ã£o, inversa, determinante,
decomposiÃ§Ãµes, etc).

- Comparando a quantidade de memÃ³ria utilizada.
```{r}
library('Matrix')

m1 <- matrix(0, nrow = 1000, ncol = 1000)
object.size(m1)
## 8000216 bytes

m2 <- Matrix(0, nrow = 1000, ncol = 1000, sparse = TRUE)
object.size(m2)
## 9240 bytes
```


Comparando o tempo computacional


- Matriz densa
```{r}
y <- rnorm(1000)
X <- matrix(NA, ncol = 100, nrow = 1000)
for(i in 1:1000) {X[i,] <- rbinom(100, size = 1, p = 0.1)}
system.time(replicate(100, solve(t(X)%*%X, t(X)%*%y)))
## usuÃ¡rio sistema decorrido
## 0.819 0.004 0.823
```


- Matriz esparsa
```{r}
y <- rnorm(1000)
X <- Matrix(NA, ncol = 100, nrow = 1000)
for(i in 1:1000) {X[i,] <- rbinom(100, size = 1, p = 0.1)}
X <- Matrix(X, sparse = TRUE)
system.time(replicate(100, solve(t(X)%*%X, t(X)%*%y)))
## usuÃ¡rio sistema decorrido
## 0.223 0.000 0.224
```

### Diferentes formas de implementar as operaÃ§Ãµes matriciais

- Criando a base de dados para a comparaÃ§Ã£o
```{r}
library(Matrix)
n <- 10000; p <- 500

#DENSA
x <- matrix(rbinom(n*p, 1, 0.01), nrow=n, ncol=p)
object.size(x)
## 20000216 bytes

#ESPARCA
X <- Matrix(x)
object.size(X)
## 600432 bytes
```

- Diferentes implementaÃ§Ãµes
```{r}
y <- rnorm(n)

print("Matriz densa com %*%:")
system.time(solve(t(x)%*%x, t(x)%*%y))
## usuÃ¡rio sistema decorrido
## 2.053 0.040 2.094

print("Matriz densa com crossprod")
system.time(solve(crossprod(x), crossprod(x, y)))
## usuÃ¡rio sistema decorrido
## 1.731 0.016 1.748

print("Matriz esparÃ§a com %*%")
system.time(solve(t(X)%*%X, t(X)%*%y))
## usuÃ¡rio sistema decorrido
## 0.071 0.000 0.072

print("Matriz esparÃ§a com crossprod")
system.time(solve(crossprod(X), crossprod(X,y)))
## usuÃ¡rio sistema decorrido
## 0.029 0.000 0.050
```

- ImplementaÃ§Ã£o eficiente do modelo de regressÃ£o linear mÃºltipla.
```{r}
library(glmnet)
## Loaded glmnet 4.1-6
system.time(b <- coef(lm(y~x)))
## usuÃ¡rio sistema decorrido
## 2.389 0.044 2.434
system.time(g1 <-glmnet(x, y, nlambda=1, lambda=0, standardize=FALSE))
## usuÃ¡rio sistema decorrido
## 0.065 0.020 0.086
system.time(g2 <- glmnet(X, y, nlambda=1, lambda=0, standardize=FALSE))
## usuÃ¡rio sistema decorrido
## 0.006 0.000 0.006
```


# Proxima aula

### Sistemas lineares
- Sistema com duas equaÃ§Ãµes:
ğ‘“1(ğ‘¥1,ğ‘¥2) = 0
ğ‘“2(ğ‘¥1,ğ‘¥2) = 0.
- SoluÃ§Ã£o numÃ©rica consiste em encontrarÌ‚ ğ‘¥1 eÌ‚ ğ‘¥2 que satisfaÃ§a o sistema de equaÃ§Ãµes.
- Sistema com ğ‘› equaÃ§Ãµes
ğ‘“1(ğ‘¥1, â€¦ , ğ‘¥ğ‘›) = 0
â‹®
ğ‘“ğ‘›(ğ‘¥1, â€¦ , ğ‘¥ğ‘›) = 0.
- Genericamente, tem-se
f(ğ‘¥) = 0.
- EquaÃ§Ãµes podem ser lineares ou nÃ£o-lineares.
----  58
Sistemas de equaÃ§Ãµes lineares
- Cada equaÃ§Ã£o Ã© linear na incÃ³gnita.
- SoluÃ§Ã£o analÃ­tica em geral Ã© possÃ­vel.
- Exemplo:
7ğ‘¥1 + 3ğ‘¥2 = 45
4ğ‘¥1 + 5ğ‘¥2 = 29.
- SoluÃ§Ã£o analÃ­tica:Ì‚ ğ‘¥1 = 6 eÌ‚ ğ‘¥2 = 1.
- Resolver (tedioso!!).
- TrÃªs possÃ­veis casos:
1. Uma Ãºnica soluÃ§Ã£o (sistema nÃ£o singular).
2. Infinitas soluÃ§Ãµes (sistema singular).
3. Nenhuma soluÃ§Ã£o (sistema impossÃ­vel).
----  59
Sistemas de equaÃ§Ãµes lineares
- RepresentaÃ§Ã£o matricial do sistema de equaÃ§Ãµes lineares:
A = [7 3
4 5] , x = [ğ‘¥1
ğ‘¥2
] e b = [45
29] .
- De forma geral, tem-se
Ax = b.
----  60
OperaÃ§Ãµes com linhas
- Sem qualquer alteraÃ§Ã£o na relaÃ§Ã£o linear, Ã© possÃ­vel
1. Trocar a posiÃ§Ã£o de linhas:
4ğ‘¥1 + 5ğ‘¥2 = 29
7ğ‘¥1 + 3ğ‘¥2 = 45.
2. Multiplicar qualquer linha por uma constante, aqui 4ğ‘¥1 + 5ğ‘¥2 por 1
4 , obtendo
ğ‘¥1 + 5
4 ğ‘¥2 = 29
4 (1)
7ğ‘¥1 + 3ğ‘¥2 = 45. (2)
----  61
OperaÃ§Ãµes com linhas
3. Subtrair um mÃºltiplo de uma linha de uma outra, aqui 7 âˆ— ğ¸ğ‘.(1) menos Eq. (2), obtendo
ğ‘¥1 + 5
4 ğ‘¥2 = 29
4
0ğ‘¥1 + ( 35
4 âˆ’ 3)ğ‘¥2 = 203
4 âˆ’ 45.
- Fazendo as contas, tem-se
0ğ‘¥1 + 23
4 ğ‘¥2 = 23
4 .
----  62
SoluÃ§Ã£o de sistemas lineares
- Forma geral de um sistema com ğ‘› equaÃ§Ãµes lineares:
ğ‘11ğ‘¥1 + ğ‘12ğ‘¥2 + â€¦ + ğ‘1ğ‘›ğ‘¥ğ‘› = ğ‘1
ğ‘21ğ‘¥1 + ğ‘22ğ‘¥2 + â€¦ + ğ‘2ğ‘›ğ‘¥ğ‘› = ğ‘2
â‹®
ğ‘ğ‘›1ğ‘¥1 + ğ‘ğ‘›2ğ‘¥2 + â€¦ + ğ‘ğ‘›ğ‘›ğ‘¥ğ‘› = ğ‘ğ‘›
- Matricialmente, tem-se
â¡
â¢
â¢
â£
ğ‘11 ğ‘12 â€¦ ğ‘1ğ‘›
ğ‘21 ğ‘22 â€¦ ğ‘2ğ‘›
â‹® â‹® â‹® â€¦
ğ‘ğ‘›1 ğ‘ğ‘›2 â€¦ ğ‘ğ‘›ğ‘›
â¤
â¥
â¥
â¦
â¡
â¢
â¢
â£
ğ‘¥1
ğ‘¥2
â‹®
ğ‘¥ğ‘›
â¤
â¥
â¥
â¦
= â¡
â¢
â¢
â£
ğ‘1
ğ‘2
â‹®
ğ‘ğ‘›
â¤
â¥
â¥
â¦
- MÃ©todos diretos e mÃ©todos iterativos.
----  63
MÃ©todos diretos
----  64
MÃ©todos diretos
- O sistema de equaÃ§Ãµes Ã© manipulado atÃ© se transformar em um sistema equivalente de
fÃ¡cil resoluÃ§Ã£o.
- Triangular superior:
â¡
â¢
â¢
â£
ğ‘11 ğ‘12 ğ‘13 ğ‘14
0 ğ‘22 ğ‘23 ğ‘24
0 0 ğ‘33 ğ‘34
0 0 0 ğ‘44
â¤
â¥
â¥
â¦
â¡
â¢
â¢
â£
ğ‘¥1
ğ‘¥2
ğ‘¥3
ğ‘¥4
â¤
â¥
â¥
â¦
= â¡
â¢
â¢
â£
ğ‘1
ğ‘2
ğ‘3
ğ‘4
â¤
â¥
â¥
â¦
.
- SubstituiÃ§Ã£o regressiva
ğ‘¥ğ‘› = ğ‘ğ‘›
ğ‘ğ‘›ğ‘›
ğ‘¥ğ‘– = ğ‘ğ‘– âˆ’ âˆ‘ğ‘—=ğ‘›
ğ‘—=ğ‘–+1 ğ‘ğ‘–ğ‘—ğ‘¥ğ‘—
ğ‘ğ‘–ğ‘–
, ğ‘– = ğ‘› âˆ’ 1, ğ‘› âˆ’ 2, â€¦ , 1.
----  65
MÃ©todos diretos
- Triangular inferior:
â¡
â¢
â¢
â£
ğ‘11 0 0 0
ğ‘21 ğ‘22 0 0
ğ‘31 ğ‘32 ğ‘33 0
ğ‘41 ğ‘42 ğ‘43 ğ‘44
â¤
â¥
â¥
â¦
â¡
â¢
â¢
â£
ğ‘¥1
ğ‘¥2
ğ‘¥3
ğ‘¥4
â¤
â¥
â¥
â¦
= â¡
â¢
â¢
â£
ğ‘1
ğ‘2
ğ‘3
ğ‘4
â¤
â¥
â¥
â¦
.
- SubstituiÃ§Ã£o progressiva
ğ‘¥1 = ğ‘1
ğ‘11
ğ‘¥ğ‘– = ğ‘ğ‘– âˆ’ âˆ‘ğ‘—=ğ‘–âˆ’1
ğ‘—=ğ‘– ğ‘ğ‘–ğ‘—ğ‘¥ğ‘—
ğ‘ğ‘–ğ‘–
, ğ‘– = 2, 3, â€¦ , ğ‘›.
----  66
MÃ©todos diretos
- Diagonal:
â¡
â¢
â¢
â£
ğ‘11 0 0 0
0 ğ‘22 0 0
0 0 ğ‘33 0
0 0 0 ğ‘44
â¤
â¥
â¥
â¦
â¡
â¢
â¢
â£
ğ‘¥1
ğ‘¥2
ğ‘¥3
ğ‘¥4
â¤
â¥
â¥
â¦
= â¡
â¢
â¢
â£
ğ‘1
ğ‘2
ğ‘3
ğ‘4
â¤
â¥
â¥
â¦
.
----  67
EliminaÃ§Ã£o de Gauss
----  68
MÃ©todos diretos: EliminaÃ§Ã£o de Gauss
- MÃ©todo de EliminaÃ§Ã£o de Gauss consiste em manipular o sistema original usando
operaÃ§Ãµes de linha atÃ© obter um sistema triangular superior.
[
ğ‘11 ğ‘12 ğ‘13 ğ‘14
ğ‘21 ğ‘22 ğ‘23 ğ‘24
ğ‘31 ğ‘23 ğ‘33 ğ‘34
ğ‘41 ğ‘24 ğ‘34 ğ‘44
] [
ğ‘¥1
ğ‘¥2
ğ‘¥3
ğ‘¥4
] = [
ğ‘1
ğ‘2
ğ‘3
ğ‘4
] â†’ [
ğ‘11 ğ‘12 ğ‘13 ğ‘14
0 ğ‘â€²
22 ğ‘â€²
23 ğ‘â€²
24
0 0 ğ‘â€²
33 ğ‘â€²
34
0 0 0 ğ‘â€²
44
] [
ğ‘¥1
ğ‘¥2
ğ‘¥3
ğ‘¥4
] = [
ğ‘1
ğ‘â€²
2
ğ‘â€²
3
ğ‘â€²
4
]
- Usar eliminaÃ§Ã£o regressiva no novo sistema para obter a soluÃ§Ã£o.
- Resolva o seguinte sistema usando EliminaÃ§Ã£o de Gauss.
â¡
â¢
â£
3 2 6
2 4 3
5 3 4
â¤
â¥
â¦
â¡
â¢
â£
ğ‘¥1
ğ‘¥2
ğ‘¥3
â¤
â¥
â¦
= â¡
â¢
â£
24
23
33
â¤
â¥
â¦
----  69
MÃ©todos diretos: EliminaÃ§Ã£o de Gauss
- Passo 1: encontrar o pivÃ´ e eliminar os elementos abaixo dele usando operaÃ§Ãµes de linha.
[ [3] 2 6
2 âˆ’ 2
3 3 4 âˆ’ 2
3 2 3 âˆ’ 2
3 6
5 âˆ’ 5
3 3 3 âˆ’ 5
3 2 4 âˆ’ 5
3 6
] [ 24
23 âˆ’ 2
3 24
33 âˆ’ 5
3 24
] â†’ [[3] 2 6
0 8
3 âˆ’1
0 âˆ’ 1
3 âˆ’6
] [24
7
âˆ’7]
- Passo 2: encontrar o segundo pivÃ´ e eliminar os elementos abaixo dele usando operaÃ§Ãµes
de linha.
[3 2 6
0 [ 8
3 ] âˆ’1
0 âˆ’ 1
3 âˆ’ (âˆ’ 3
24 ) ( 8
3 ) âˆ’6 âˆ’ (âˆ’ 3
24 )(âˆ’1)
] [ 24
7
âˆ’7 âˆ’ (âˆ’ 3
24 )(7)] â†’ [3 2 6
0 [ 8
3 ] âˆ’1
0 0 âˆ’ 147
24
] [ 24
7
âˆ’ 147
24
]
- Passo 3: substituiÃ§Ã£o regressiva.
----  70
MÃ©todos diretos: EliminaÃ§Ã£o de Gauss
- Usando a fÃ³rmula de substituiÃ§Ã£o regressiva temos:
- ğ‘¥3 = ğ‘3
ğ‘33
= 1.
- ğ‘¥2 = ğ‘2âˆ’ğ‘23ğ‘¥3
ğ‘22
= 3.
- ğ‘¥1 = (ğ‘1âˆ’(ğ‘12ğ‘¥2+ğ‘13ğ‘¥3)
ğ‘11
= 4.
- A extensÃ£o do procedimento para um sistema com ğ‘› equaÃ§Ãµes Ã© trivial.
1. Transforme o sistema em triangular superior usando operaÃ§Ãµes linhas.
2. Resolva o novo sistema usando substituiÃ§Ã£o regressiva.
- Potenciais problemas do mÃ©todo de eliminaÃ§Ã£o de Gauss:
- O elemento pivÃ´ Ã© zero.
- O elemento pivÃ´ Ã© pequeno em relaÃ§Ã£o aos demais termos.
----  71
EliminaÃ§Ã£o de Gauss com pivotaÃ§Ã£o
----  72
EliminaÃ§Ã£o de Gauss com pivotaÃ§Ã£o
- Considere o sistema
0ğ‘¥1 + 2ğ‘¥2 + 3ğ‘¥2 = 46
4ğ‘¥1 âˆ’ 3ğ‘¥2 + 2ğ‘¥3 = 16
2ğ‘¥1 + 4ğ‘¥2 âˆ’ 3ğ‘¥3 = 12
- Neste caso o pivÃ´ Ã© zero e o procedimento nÃ£o pode comeÃ§ar.
- PivotaÃ§Ã£o - trocar a ordem das linhas.
1. Evitar pivÃ´s zero.
2. Diminuir o nÃºmero de operaÃ§Ãµes necessÃ¡rias para triangular o sistema.
4ğ‘¥1 âˆ’ 3ğ‘¥2 + 2ğ‘¥3 = 16
2ğ‘¥1 + 4ğ‘¥2 âˆ’ 3ğ‘¥3 = 12
0ğ‘¥1 + 2ğ‘¥2 + 3ğ‘¥2 = 46
----  73
EliminaÃ§Ã£o de Gauss com pivotaÃ§Ã£o
- Se durante o procedimento uma equaÃ§Ã£o pivÃ´ tiver um elemento nulo e o sistema tiver
soluÃ§Ã£o, uma equaÃ§Ã£o com um elemento pivÃ´ diferente de zero sempre existirÃ¡.
- CÃ¡lculos numÃ©ricos sÃ£o menos propensos a erros e apresentam menores erros de
arredondamento se o elemento pivÃ´ for grande em valor absoluto.
- Ã‰ usual ordenar as linhas para que o maior valor seja o primeiro pivÃ´.
----  74
Passo 1: obtendo uma matriz triangular superior.
gauss <- function(A, b) {
Ae <- cbind(A, b) ## Sistema aumentado
rownames(Ae) <- paste0("x", 1:length(b))
n_row <- nrow(Ae)
n_col <- ncol(Ae)
SOL <- matrix(NA, n_row, n_col) ## Matriz para receber os resultados
SOL[1,] <- Ae[1,]
pivo <- matrix(0, n_col, n_row)
for(j in 1:c(n_row-1)) {
for(i in c(j+1):c(n_row)) {
pivo[i,j] <- Ae[i,j]/SOL[j,j]
SOL[i,] <- Ae[i,] - pivo[i,j]*SOL[j,]
Ae[i,] <- SOL[i,]
}
}
return(SOL)
}
----  75
EliminaÃ§Ã£o de Gauss sem pivotaÃ§Ã£o
- Passo 2: substituiÃ§Ã£o regressiva
sub_reg <- function(SOL) {
n_row <- nrow(SOL)
n_col <- ncol(SOL)
A <- SOL[1:n_row,1:n_row]
b <- SOL[,n_col]
n <- length(b)
x <- c()
x[n] <- b[n]/A[n,n]
for(i in (n-1):1) {
x[i] <- (b[i] - sum(A[i,c(i+1):n]*x[c(i+1):n] ))/A[i,i]
}
return(x)
}
----  76
EliminaÃ§Ã£o de Gauss sem pivotaÃ§Ã£o
- Resolva o sistema:
â¡
â¢
â£
3 2 6
2 4 3
5 3 4
â¤
â¥
â¦
â¡
â¢
â£
ğ‘¥1
ğ‘¥2
ğ‘¥3
â¤
â¥
â¦
= â¡
â¢
â£
24
23
33
â¤
â¥
â¦
.
A <- matrix(c(3,2,5,2,4,3,6,3,4),3,3)
b <- c(24,23,33)
S <- gauss(A, b) ## Passo 1: TriangularizaÃ§Ã£o
sol = sub_reg(SOL = S) ## Passo 2: SubstituiÃ§Ã£o regressiva
sol
## [1] 4 3 1
A%*%sol ## Verificando a soluÃ§Ã£o
## [,1]
## [1,] 24
## [2,] 23
## [3,] 33
----  77
EliminaÃ§Ã£o de Gauss com pivotaÃ§Ã£o
- Resolva o seguinte sistema usando
EliminaÃ§Ã£o de Gauss com pivotaÃ§Ã£o.
0ğ‘¥1 + 2ğ‘¥2 + 3ğ‘¥2 = 46
4ğ‘¥1 âˆ’ 3ğ‘¥2 + 2ğ‘¥3 = 16
2ğ‘¥1 + 4ğ‘¥2 âˆ’ 3ğ‘¥3 = 12
## Entrando com o sistema original
A <- matrix(c(0,4,2,2,-3,4,3,2,-3), 3,3)
b <- c(46,16,12)
## Pivoteamento
A_order <- A[order(A[,1], decreasing = TRUE),]
b_order <- b[order(A[,1], decreasing = TRUE)]
#### TriangulaÃ§Ã£o
S <- gauss(A_order, b_order)
S
## [,1] [,2] [,3] [,4]
## [1,] 4 -3.0 2.000000 16.00000
## [2,] 0 5.5 -4.000000 4.00000
## [3,] 0 0.0 4.454545 44.54545
#### SubstituiÃ§Ã£o regressiva
sol <- sub_reg(SOL = S)
sol
## [1] 5 8 10
#### SoluÃ§Ã£o
A_order%*%sol
## [,1]
## [1,] 16
## [2,] 12
## [3,] 46
----  78
EliminaÃ§Ã£o de Gauss-Jordan
----  79
MÃ©todos diretos: EliminaÃ§Ã£o de Gauss-Jordan
- O sistema original Ã© manipulado atÃ© obter um sistema equivalente na forma diagonal.
[
ğ‘11 ğ‘12 ğ‘13 ğ‘14
ğ‘21 ğ‘22 ğ‘23 ğ‘24
ğ‘31 ğ‘23 ğ‘33 ğ‘34
ğ‘41 ğ‘24 ğ‘34 ğ‘44
] [
ğ‘¥1
ğ‘¥2
ğ‘¥3
ğ‘¥4
] = [
ğ‘1
ğ‘2
ğ‘3
ğ‘4
] â†’ [
1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 1
] [
ğ‘¥1
ğ‘¥2
ğ‘¥3
ğ‘¥4
] = [
ğ‘â€²
1
ğ‘â€²
2
ğ‘â€²
3
ğ‘â€²
4
]
- Algoritmo Gauss-Jordan
1. Normalize a equaÃ§Ã£o pivÃ´ com a divisÃ£o de todos os seus termos pelo coeficiente pivÃ´.
2. Elimine os elementos fora da diagonal principal em TODAS as demais equaÃ§Ãµes usando
operaÃ§Ãµs de linha.
- O mÃ©todo de Gauss-Jordan pode ser combinado com pivotaÃ§Ã£o igual ao mÃ©todo de
eliminaÃ§Ã£o de Gauss.
----  80
MÃ©todos iterativos
----  81
MÃ©todos iterativos
- Nos mÃ©todos iterativos, as equaÃ§Ãµes sÃ£o colocadas em uma forma explÃ­cita onde cada
incÃ³gnita Ã© escrita em termos das demais, i.e.
ğ‘11ğ‘¥1 + ğ‘12ğ‘¥2 + ğ‘13ğ‘¥3 = ğ‘1
ğ‘21ğ‘¥1 + ğ‘22ğ‘¥2 + ğ‘23ğ‘¥3 = ğ‘2
ğ‘31ğ‘¥1 + ğ‘32ğ‘¥2 + ğ‘33ğ‘¥3 = ğ‘3
â†’
ğ‘¥1 = [ğ‘1 âˆ’ (ğ‘12ğ‘¥2 + ğ‘13ğ‘¥3)]/ğ‘11
ğ‘¥2 = [ğ‘2 âˆ’ (ğ‘21ğ‘¥1 + ğ‘23ğ‘¥3)]/ğ‘22
ğ‘¥3 = [ğ‘3 âˆ’ (ğ‘31ğ‘¥1 + ğ‘32ğ‘¥2)]/ğ‘33
.
- Dado um valor inicial para as incÃ³gnitas estas serÃ£o atualizadas atÃ© a convergÃªncia.
- AtualizaÃ§Ã£o: MÃ©todo de Jacobi
ğ‘¥ğ‘– = 1
ğ‘ğ‘–ğ‘–
[ğ‘ğ‘– âˆ’ (
ğ‘—=ğ‘›
âˆ‘
ğ‘—=1;ğ‘—â‰ ğ‘–
ğ‘ğ‘–ğ‘—ğ‘¥ğ‘—)] ğ‘– = 1, â€¦ , ğ‘›.
----  82
MÃ©todos iterativos
- AtualizaÃ§Ã£o: MÃ©todo de Gauss-Seidel
ğ‘¥ğ‘˜+1
1 = 1
ğ‘11
[ğ‘1 âˆ’
ğ‘—=ğ‘›
âˆ‘
ğ‘—=2
ğ‘1ğ‘—ğ‘¥(ğ‘˜)
ğ‘— ] ,
ğ‘¥(ğ‘˜+1)
ğ‘– = 1
ğ‘ğ‘–ğ‘–
[ğ‘ğ‘– âˆ’ (
ğ‘—=ğ‘–âˆ’1
âˆ‘
ğ‘—=1
ğ‘ğ‘–ğ‘—ğ‘¥(ğ‘˜+1)
ğ‘— +
ğ‘—=ğ‘›
âˆ‘
ğ‘—=ğ‘–+1
ğ‘ğ‘–ğ‘—ğ‘¥(ğ‘˜)
ğ‘— )] ğ‘– = 2, 3, â€¦ , ğ‘› âˆ’ 1 e
ğ‘¥(ğ‘˜+1)
ğ‘› = 1
ğ‘ğ‘›ğ‘›
[ğ‘ğ‘› âˆ’
ğ‘—=ğ‘›âˆ’1
âˆ‘
ğ‘—=1
ğ‘ğ‘›ğ‘—ğ‘¥(ğ‘˜+1)
ğ‘— ] .
----  83
MÃ©todo iterativo de Jacobi
- ImplementaÃ§Ã£o computacional
jacobi <- function(A, b, inicial, max_iter = 10, tol = 1e-04) {
n <- length(b)
x_temp <- matrix(NA, ncol = n, nrow = max_iter)
x_temp[1,] <- inicial
x <- x_temp[1,]
for(j in 2:max_iter) { #### EquaÃ§Ã£o de atualizaÃ§Ã£o
for(i in 1:n) {
x_temp[j,i] <- (b[i] - sum(A[i,1:n][-i]*x[-i]))/A[i,i]
}
x <- x_temp[j,]
if(sum(abs(x_temp[j,] - x_temp[c(j-1),])) < tol) break #### CritÃ©rio de parada
}
return(list("Solucao" = x, "Iteracoes" = x_temp))
}
----  84
MÃ©todo iterativo de Jacobi
- Resolva o seguinte sistema de equaÃ§Ãµes
lineares usando o mÃ©todo de Jacobi.
9ğ‘¥1 âˆ’ 2ğ‘¥2 + 3ğ‘¥3 + 2ğ‘¥4 = 54.5
2ğ‘¥1 + 8ğ‘¥2 âˆ’ 2ğ‘¥3 + 3ğ‘¥4 = âˆ’14
âˆ’3ğ‘¥1 + 2ğ‘¥2 + 11ğ‘¥3 âˆ’ 4ğ‘¥4 = 12.5
âˆ’2ğ‘¥1 + 3ğ‘¥2 + 2ğ‘¥3 âˆ’ 10ğ‘¥4 = âˆ’21
- Computacionalmente
A <- matrix(c(9,2,-3,-2,-2,8,2,
3,3,-2,11,2,2,3,-4,10),4,4)
b <- c(54.5, -14, 12.5, -21)
ss <- jacobi(A = A, b = b,
inicial = c(0,0,0,0),
max_iter = 15)
## SoluÃ§Ã£o aproximada
ss$Solucao
## [1] 4.999502 -1.999771 2.500056 -1.000174
## SoluÃ§Ã£o exata
solve(A, b)
## [1] 5.0 -2.0 2.5 -1.0
----  85
MÃ©todos iterativo de Jacobi e Gauss-Seidel
- Em R o pacote Rlinsolve fornece
implementaÃ§Ãµes eficientes dos mÃ©todos
de Jacobi e Gauss-Seidel.
- Rlinsolve inclui suporte para matrizes
esparsas via Matrix.
- Rlinsolve Ã© implementado em C++
usando o pacote Rcpp.
A <- matrix(c(9,2,-3,-2,-2,8,2,3,3,-2,11,
2,2,3,-4,10),4,4)
b <- c(54.5, -14, 12.5, -21)
## pacote extra
require(Rlinsolve)
lsolve.jacobi(A, b)$x ## MÃ©todo de jacobi
## [,1]
## [1,] 4.9999708
## [2,] -2.0000631
## [3,] 2.5000163
## [4,] -0.9999483
lsolve.gs(A, b)$x ## MÃ©todo de Gauss-Seidell
## [,1]
## [1,] 4.999955
## [2,] -2.000071
## [3,] 2.500018
## [4,] -0.999968
----  86
DecomposiÃ§Ã£o LU
----  87
DecomposiÃ§Ã£o LU
- Nos mÃ©todos de eliminaÃ§Ã£o de Gauss e Gauss-Jordan resolvemos sistemas do tipo
Ağ‘¥ = ğ‘.
- Sendo dois sistemas
Ağ‘¥ = ğ‘1, e Ağ‘¥ = ğ‘2.
- CÃ¡lculos do primeiro nÃ£o ajudam a resolver o segundo.
- IDEAL! - OperaÃ§Ãµes realizadas em A fossem dissociadas das operaÃ§Ãµes em ğ‘.
----  88
DecomposiÃ§Ã£o LU
- Suponha que precisamos resolver vÃ¡rios sistemas do tipo
Ağ‘¥ = ğ‘.
para diferentes ğ‘â€²ğ‘ .
- OpÃ§Ã£o 1 - calcular a inversa Aâˆ’1, assim a soluÃ§Ã£o
ğ‘¥ = Aâˆ’1ğ‘.
- CÃ¡lculo da inversa Ã© computacionalmente ineficiente.
----  89
DecomposiÃ§Ã£o LU: algoritmo
- Decomponha (fatore) a matriz A em um produto de duas matrizes
A = LU,
onde L Ã© triangular inferior e U Ã© triangular superior.
- Baseado na decomposiÃ§Ã£o o sistema tem a forma:
LUğ‘¥ = ğ‘. (3)
- Defina Uğ‘¥ = ğ‘¦.
- Substituindo em 3 tem-se
Lğ‘¦ = ğ‘. (4)
- SoluÃ§Ã£o Ã© obtida em dois passos
- Resolva Eq.(4) para obter ğ‘¦ usando substituiÃ§Ã£o progressiva.
- Resolva Eq.(3) para obter ğ‘¥ usando substituiÃ§Ã£o regressiva.
----  90
Obtendo as matrizes L e U
- MÃ©todo de eliminaÃ§Ã£o de Gauss e mÃ©todo de Crout.
- Dentro do processo de eliminaÃ§Ã£o de Gauss as matrizes L e U sÃ£o obtidas como um
subproduto, i.e.
[
ğ‘11 ğ‘12 ğ‘13 ğ‘14
ğ‘21 ğ‘22 ğ‘23 ğ‘24
ğ‘31 ğ‘32 ğ‘33 ğ‘34
ğ‘41 ğ‘41 ğ‘43 ğ‘44
] = [
1
ğ‘š21 1
ğ‘š31 ğ‘š32 1
ğ‘š41 ğ‘š42 ğ‘š43 1
] [
ğ‘11 ğ‘12 ğ‘13 ğ‘14
0 ğ‘â€²
22 ğ‘â€²
23 ğ‘â€²
24
0 0 ğ‘â€²
33 ğ‘â€²
34
0 0 0 ğ‘â€²
44
] .
- Os elementos ğ‘šâ€²
ğ‘–ğ‘—ğ‘  sÃ£o os multiplicadores que multiplicam a equaÃ§Ã£o pivÃ´.
----  91
Obtendo as matrizes L e U
- Relembre o exemplo de eliminaÃ§Ã£o de Gauss.
[ [3] 2 6
2 âˆ’ 2
3 3 4 âˆ’ 2
3 2 3 âˆ’ 2
3 6
5 âˆ’ 5
3 3 3 âˆ’ 5
3 2 4 âˆ’ 5
3 6
] [ 24
23 âˆ’ 2
3 24
33 âˆ’ 5
3 24
] â†’ [[3] 2 6
0 8
3 âˆ’1
0 âˆ’ 1
3 âˆ’6
] [24
7
âˆ’7]
[3 2 6
0 [ 8
3 ] âˆ’1
0 âˆ’ 1
3 âˆ’ (âˆ’ 3
24 ) ( 8
3 ) âˆ’6 âˆ’ (âˆ’ 3
24 )(âˆ’1)
] [ 24
7
âˆ’7 âˆ’ (âˆ’ 3
24 )(7)] â†’ [3 2 6
0 [ 8
3 ] âˆ’1
0 0 âˆ’ 147
24
] [ 24
7
âˆ’ 147
24
]
- Neste caso, tem-se
L = â¡
â¢
â£
1
2
3 1
5
3 âˆ’ 3
24 1
â¤
â¥
â¦
e U = â¡
â¢
â£
3 2 6
0 8
3 âˆ’1
0 0 âˆ’ 147
24
â¤
â¥
â¦
.
----  92
DecomposiÃ§Ã£o LU com pivotaÃ§Ã£o
----  93
DecomposiÃ§Ã£o LU com pivotaÃ§Ã£o
- O mÃ©todo de eliminaÃ§Ã£o de Gauss foi realizado sem pivotaÃ§Ã£o.
- Como discutido a pivotaÃ§Ã£o pode ser necessÃ¡ria.
- Quando realizada a pivotaÃ§Ã£o as mudanÃ§as feitas devem ser armazenadas, tal que
PA = LU.
- P Ã© uma matriz de permutaÃ§Ã£o.
- Se as matrizes LU forem usadas para resolver o sistema
Ağ‘¥ = ğ‘,
entÃ£o a ordem das linhas de ğ‘ deve ser alterada de forma consistente com a pivotaÃ§Ã£o,
i.e. Pğ‘.
----  94
ImplementaÃ§Ã£o: DecomposiÃ§Ã£o LU
- Podemos facilmente modificar a funÃ§Ã£o gauss() para obter a decomposiÃ§Ã£o LU.
my_lu <- function(A) {
n_row <- nrow(A)
n_col <- ncol(A)
SOL <- matrix(NA, n_row, n_col) ## Matriz para receber os resultados
SOL[1,] <- A[1,]
pivo <- matrix(0, n_col, n_row)
for(j in 1:c(n_row-1)) {
for(i in c(j+1):c(n_row)) {
pivo[i,j] <- A[i,j]/SOL[j,j]
SOL[i,] <- A[i,] - pivo[i,j]*SOL[j,]
A[i,] <- SOL[i,]
}
}
diag(pivo) <- 1
return(list("L" = pivo, "U" = SOL)) }
----  95
AplicaÃ§Ã£o: DecomposiÃ§Ã£o LU
- Fazendo a decomposiÃ§Ã£o.
LU <- my_lu(A) ## DecomposiÃ§Ã£o
LU
## $L
## [,1] [,2] [,3] [,4]
## [1,] 1.0000000 0.0000000 0.000000 0
## [2,] 0.2222222 1.0000000 0.000000 0
## [3,] -0.3333333 0.1578947 1.000000 0
## [4,] -0.2222222 0.3026316 0.279661 1
##
## $U
## [,1] [,2] [,3] [,4]
## [1,] 9 -2.000000e+00 3.000000 2.000000
## [2,] 0 8.444444e+00 -2.666667 2.555556
## [3,] 0 0.000000e+00 12.421053 -3.736842
## [4,] 0 -4.440892e-16 0.000000 10.716102
LU$L %*% LU$U ## Verificando a soluÃ§Ã£o
## [,1] [,2] [,3] [,4]
## [1,] 9 -2 3 2
## [2,] 2 8 -2 3
## [3,] -3 2 11 -4
## [4,] -2 3 2 10
----  96
AplicaÃ§Ã£o: DecomposiÃ§Ã£o LU
- Resolvendo o sistema de equaÃ§Ãµes.
## Passo 1: SubstituiÃ§Ã£o progressiva
y = forwardsolve(LU$L, b)
## Passo 2: SubstituiÃ§Ã£o regressiva
x = backsolve(LU$U, y)
x
## [1] 5.0 -2.0 2.5 -1.0
A%*%x ## Verificando a soluÃ§Ã£o
## [,1]
## [1,] 54.5
## [2,] -14.0
## [3,] 12.5
## [4,] -21.0
- FunÃ§Ã£o lu() do Matrix fornece a
decomposiÃ§Ã£o LU.
require(Matrix)
## Calcula mas nÃ£o retorna
LU_M <- lu(A)
## Captura as matrizes L U e P
LU_M <- expand(LU_M)
## SubstituiÃ§Ã£o progressiva.
y <- forwardsolve(LU_M$L, LU_M$P%*%b)
## SubstituiÃ§Ã£o regressiva
x = backsolve(LU_M$U, y)
x
## [1] 5.0 -2.0 2.5 -1.0
----  97
Obtendo a inversa
----  98
Obtendo a inversa via decomposiÃ§Ã£o LU
- O mÃ©todo LU Ã© especialmente adequado para o cÃ¡lculo da inversa.
- Lembre-se que a inversa de A Ã© tal que
AAâˆ’1 = I.
- O procedimento de cÃ¡lculo da inversa Ã© essencialmente o mesmo da soluÃ§Ã£o de um
sistema de equaÃ§Ãµes lineares, porÃ©m com mais incognitas.
â¡
â¢
â£
ğ‘11 ğ‘12 ğ‘13
ğ‘21 ğ‘22 ğ‘23
ğ‘31 ğ‘32 ğ‘33
â¤
â¥
â¦
â¡
â¢
â£
ğ‘¥11 ğ‘¥12 ğ‘¥13
ğ‘¥21 ğ‘¥22 ğ‘¥23
ğ‘¥31 ğ‘¥32 ğ‘¥33
â¤
â¥
â¦
= â¡
â¢
â£
1 0 0
0 1 0
0 0 1
â¤
â¥
â¦
- TrÃªs sistemas de equaÃ§Ãµes diferentes, em cada sistema, uma coluna da matriz X Ã© a
incognita.
----  99
ImplementaÃ§Ã£o: inversa via decomposiÃ§Ã£o LU
- FunÃ§Ã£o para resolver o sistema usando
decomposiÃ§Ã£o LU.
solve_lu <- function(LU, b) {
y <- forwardsolve(LU_M$L, LU_M$P%*%b)
x = backsolve(LU_M$U, y)
return(x)
}
- Resolvendo vÃ¡rios sistemas
my_solve <- function(LU, B) {
n_col <- ncol(B)
n_row <- nrow(B)
inv <- matrix(NA, n_col, n_row)
for(i in 1:n_col) {
inv[,i] <- solve_lu(LU, B[,i])
}
return(inv)
}
----  100
AplicaÃ§Ã£o: inversa via decomposiÃ§Ã£o LU
- Calcule a inversa de
A = â¡
â¢
â£
3 2 6
2 4 3
5 3 4
â¤
â¥
â¦
A <- matrix(c(3,2,5,2,4,3,6,3,4),3,3)
I <- Diagonal(3, 1)
## DecomposiÃ§Ã£o LU
LU <- my_lu(A)
## Obtendo a inversa
inv_A <- my_solve(LU = LU, B = I)
inv_A
## Verificando o resultado
A%*%inv_A
----  101
CÃ¡lculo da inversa via mÃ©todo de Gauss-Jordan
- Procedimento Gauss-Jordan:
â¡
â¢
â£
ğ‘11 ğ‘21 ğ‘31 1 0 0
ğ‘21 ğ‘22 ğ‘32 0 1 0
ğ‘31 ğ‘32 ğ‘33 0 0 1
â¤
â¥
â¦
â†’ â¡
â¢
â£
1 0 0 ğ‘â€²
11 ğ‘â€²
21 ğ‘â€²
31
0 1 0 ğ‘â€²
21 ğ‘â€²
22 ğ‘â€²
32
0 0 1 ğ‘â€²
31 ğ‘â€²
32 ğ‘â€²
33
â¤
â¥
â¦
.
- FunÃ§Ã£o solve() usa a decomposiÃ§Ã£o LU com pivotaÃ§Ã£o.
- R bÃ¡sico Ã© construÃ­do sobre a biblioteca lapack escrita em C.
- Veja documentaÃ§Ã£o em http://www.netlib.org/lapack/lug/node38.html.
----  102
Autovalores e autovetores
----  103
Autovalores e autovetores
- ReduÃ§Ã£o de dimensionalidade Ã© fundamental em ciÃªncia de dados.
- AnÃ¡lise de componentes principais (PCA)
- AnÃ¡lise fatorial (AF).
- Decompor grandes e complicados relacionamentos multivariados em simples
componentes nÃ£o relacionados.
- Vamos discutir apenas os aspectos matemÃ¡ticos.
----  104
IntuiÃ§Ã£o
- Podemos decompor um vetor ğ‘£ em duas informaÃ§Ãµes separadas: direÃ§Ã£o ğ‘‘ e tamanho ğœ†, i.e
ğœ† = ||ğ‘£|| = \sqrâˆ‘
ğ‘—
ğœˆ2
ğ‘— , e ğ‘‘ = ğ‘£
ğœ† .
- Ã‰ mais fÃ¡cil interpretar o tamanho de um vetor enquanto ignorando a sua direÃ§Ã£o e
vice-versa.
- Esta ideia pode ser estendida para matrizes.
- Uma matriz nada mais Ã© do que um conjunto de vetores.
- IDEIA - decompor a informaÃ§Ã£o de uma matriz em outros componentes de mais fÃ¡cil
interpretaÃ§Ã£o/representaÃ§Ã£o matemÃ¡tica.
----  105
Autovalores e Autovetores
- Autovalores e autovetores sÃ£o definidos por uma simples igualdade
Ağ‘£ = ğœ†ğ‘£. (5)
- Os vetores ğ‘£â€™s que satisfazem Eq. (5) sÃ£o os autovetores.
- Os valores ğœ†â€™s que satisfazem Eq. (5) sÃ£o os autovalores.
- Vamos considerar o caso em que A Ã© simÃ©trica.
- A ideia pode ser estendida para matrizes nÃ£o simÃ©tricas.
----  106
Autovalores e Autovetores
- Se A Ã© uma matriz simÃ©trica ğ‘› Ã— ğ‘›, entÃ£o existem exatamente ğ‘› pares (ğœ†ğ‘—, ğ‘£ğ‘—) que
satisfazem a equaÃ§Ã£o:
Ağ‘£ = ğœ†ğ‘£.
- Se A tem autovalores ğœ†1, â€¦ , ğœ†ğ‘›, entÃ£o:
- tr(A) = âˆ‘ğ‘›
ğ‘–=1 ğœ†ğ‘–.
- det(A) = âˆğ‘›
ğ‘–=1 ğœ†ğ‘–.
- A Ã© positiva definida, se e somente se todos ğœ†ğ‘— > 0.
- A Ã© semi-positiva definida, se e somente se todos ğœ†ğ‘— â‰¥ 0.
- A ideia do PCA Ã© decompor/fatorar a matriz A em componentes mais simples de
interpretar.
----  107
DecomposiÃ§Ã£o em autovalores e autovetores
- Teorema: qualquer matriz simÃ©trica A pode ser fatorada em
A = QÎ›QâŠ¤,
onde Î› Ã© diagonal contendo os autovalores de A e as colunas de Q contÃªm os autovetores
ortonormais.
- Vetores ortonormais: sÃ£o mutuamente ortogonais e de comprimento unitÃ¡rio.
- Teorema: se A tem autovetores Q e autovalores ğœ†ğ‘—. EntÃ£o Aâˆ’1 tem autovetores Q e
autovalores ğœ†âˆ’1
ğ‘— .
- ImplicaÃ§Ã£o: se A = QÎ›QâŠ¤ entÃ£o Aâˆ’1 = QÎ›âˆ’1QâŠ¤.
----  108
DiagonalizaÃ§Ã£o
- Autovalores sÃ£o utÃ©is porque eles permitem lidar com matrizes da mesma forma que
lidamos com nÃºmeros.
- Todos os cÃ¡lculos sÃ£o feitos na matriz diagonal Î›.
- Este processo Ã© chamado de diagonalizaÃ§Ã£o.
- Um dos resultados mais poderosos em Ãlgebra Linear Ã© que qualquer matriz pode ser
diagonalizada.
- O processo de diagonalizaÃ§Ã£o Ã© chamado de DecomposiÃ§Ã£o em valores singulares.
----  109
DecomposiÃ§Ã£o em valores singulares
(SVD)
----  110
DecomposiÃ§Ã£o em valores singulares (SVD)
- Teorema: qualquer matriz A pode ser decomposta em,
A = UDVâŠ¤,
onde D Ã© diagonal com entradas nÃ£o negativas e U e V sÃ£o ortogonais,
i.e. UâŠ¤U = VâŠ¤V = I.
- Matrizes nÃ£o quadradas nÃ£o tem autovalores.
- Os elementos de D sÃ£o chamados de valores singulares.
- Os valores singulares sÃ£o os autovalores de AâŠ¤A.
----  111
DimensÃ£o da SVD
- Se A Ã© ğ‘› Ã— ğ‘›, entÃ£o U, D e V sÃ£o ğ‘› Ã— ğ‘›.
- Se A Ã© ğ‘› Ã— ğ‘, sendo ğ‘› > ğ‘, entÃ£o U Ã© ğ‘› Ã— ğ‘, D e V sÃ£o ğ‘ Ã— ğ‘.
- Se A Ã© ğ‘› Ã— ğ‘, sendo ğ‘› < ğ‘, entÃ£o VâŠ¤ Ã© ğ‘› Ã— ğ‘, D e U sÃ£o ğ‘› Ã— ğ‘›.
- D serÃ¡ sempre quadrada com dimensÃ£o igual ao mÃ­nimo entre ğ‘ e ğ‘›.
----  112
DecomposiÃ§Ã£o em autovalores e autovetores em R
- FunÃ§Ã£o eigen() fornece a decomposiÃ§Ã£o
A <- matrix(c(1,0.8, 0.3, 0.8, 1,
0.2, 0.3, 0.2, 1),3,3)
isSymmetric.matrix(A)
## [1] TRUE
out <- eigen(A)
Q <- out$vectors ## Autovetores
D <- diag(out$values) ## Autovalores
Q
## [,1] [,2] [,3]
## [1,] -0.6712373 -0.1815663 0.71866142
## [2,] -0.6507744 -0.3198152 -0.68862977
## [3,] -0.3548708 0.9299204 -0.09651322
- Verificando a soluÃ§Ã£o
D
## [,1] [,2] [,3]
## [1,] 1.934216 0.0000000 0.0000000
## [2,] 0.000000 0.8726419 0.0000000
## [3,] 0.000000 0.0000000 0.1931419
Q%*%D%*%t(Q) ## Verificando
## [,1] [,2] [,3]
## [1,] 1.0 0.8 0.3
## [2,] 0.8 1.0 0.2
## [3,] 0.3 0.2 1.0
----  113
DecomposiÃ§Ã£o em valores singulares em R
- FunÃ§Ã£o svd() fornece a decomposiÃ§Ã£o
svd(A)
## $d
## [1] 1.9342162 0.8726419 0.1931419
##
## $u
## [,1] [,2] [,3]
## [1,] -0.6712373 0.1815663 0.71866142
## [2,] -0.6507744 0.3198152 -0.68862977
## [3,] -0.3548708 -0.9299204 -0.09651322
##
## $v
## [,1] [,2] [,3]
## [1,] -0.6712373 0.1815663 0.71866142
## [2,] -0.6507744 0.3198152 -0.68862977
## [3,] -0.3548708 -0.9299204 -0.09651322
----  114
RegressÃ£o ridge
----  115
RegressÃ£o ridge
- Relembrando: regressÃ£o linear mÃºltipla
â¡
â¢
â¢
â£
ğ‘¦1
ğ‘¦2
â‹®
ğ‘¦ğ‘›
â¤
â¥
â¥
â¦
ğ‘›Ã—1
= â¡
â¢
â¢
â£
1 ğ‘¥11 â€¦ ğ‘¥ğ‘1
1 ğ‘¥12 â€¦ ğ‘¥ğ‘1
â‹® â‹® â‹± â‹®
1 ğ‘¥1ğ‘› â€¦ ğ‘¥ğ‘ğ‘›
â¤
â¥
â¥
â¦
ğ‘›Ã—ğ‘
â¡
â¢
â¢
â£
ğ›½0
ğ›½1
â‹®
ğ›½ğ‘
â¤
â¥
â¥
â¦
ğ‘Ã—1
- Usando uma notaÃ§Ã£o mais compacta,
y
ğ‘›Ã—1
= X
ğ‘›Ã—ğ‘ ğ›½
ğ‘Ã—1
.
- Minimiza a perda quadrÃ¡tica:Ì‚
ğ›½ = (XâŠ¤X)âˆ’1XâŠ¤y.
----  116
RegressÃ£o ridge
- Se ğ‘ > ğ‘› o sistema Ã© singular (mÃºltiplas soluÃ§Ãµes)!
- Como podemos ajustar o modelo?
- Introduzir uma penalidade pela complexidade.
- Soma de quadrados penalizada
ğ‘ƒ ğ‘†ğ‘„(ğ›½) =
ğ‘›
âˆ‘
ğ‘–=1
(ğ‘¦ğ‘– âˆ’ ğ‘¥âŠ¤
ğ‘– ğ›½)2 + ğœ†
ğ‘
âˆ‘
ğ‘—=1
ğ›½2
ğ‘— .
- Matricialmente, tem-se
ğ‘ƒ ğ‘†ğ‘„(ğ›½) = (ğ‘¦ âˆ’ Xğ›½)âŠ¤(ğ‘¦ âˆ’ Xğ›½) + ğœ†ğ›½âŠ¤ğ›½.
- IMPORTANTE !!
- ğ‘¦ centrado (mÃ©dia zero).
- X padronizada por coluna (mÃ©dia zero e variÃ¢ncia um).
----  117
RegressÃ£o ridge
- Objetivo: minizar a soma de quadrados penalizada.
- Derivada
ğœ•ğ‘ƒ ğ‘„ğ‘†(ğ›½)
ğœ•ğ›½ = ğœ•
ğœ•ğ›½ [(ğ‘¦ âˆ’ Xğ›½)âŠ¤(ğ‘¦ âˆ’ Xğ›½) + ğœ†ğ›½âŠ¤ğ›½]
= [ ğœ•
ğœ•ğ›½ (ğ‘¦ âˆ’ Xğ›½)âŠ¤] (ğ‘¦ âˆ’ Xğ›½) + (ğ‘¦ âˆ’ Xğ›½)âŠ¤ [ ğœ•
ğœ•ğ›½ (ğ‘¦ âˆ’ Xğ›½)] +
ğœ† {[ ğœ•ğ›½âŠ¤
ğœ•ğ›½ ] ğ›½ + ğ›½âŠ¤ [ ğœ•ğ›½
ğœ•ğ›½ ]}
= âˆ’2XâŠ¤(ğ‘¦ âˆ’ Xğ›½) + 2ğœ†ğ›½
= âˆ’XâŠ¤(ğ‘¦ âˆ’ Xğ›½) + ğœ†ğ›½.
----  118
AplicaÃ§Ã£o: regressÃ£o ridge
- Resolvendo o sistema linear, tem-se
âˆ’XâŠ¤(ğ‘¦ âˆ’ XÌ‚ğ›½) + ğœ†IÌ‚ ğ›½ = 0
âˆ’XâŠ¤ğ‘¦ + XâŠ¤XÌ‚ğ›½ + ğœ†IÌ‚ ğ›½ = 0
XâŠ¤XÌ‚ğ›½ + ğœ†IÌ‚ ğ›½ = XâŠ¤ğ‘¦
(XâŠ¤X + ğœ†I)Ì‚ ğ›½ = XâŠ¤ğ‘¦Ì‚
ğ›½ = (XâŠ¤X + ğœ†I)âˆ’1 XâŠ¤ğ‘¦.
- SoluÃ§Ã£o depende de ğœ†.
- A inclusÃ£o de ğœ† faz o sistema ser nÃ£o singular.
- Na verdade quando fixamos ğœ† selecionamos uma soluÃ§Ã£o em particular.
----  119
AplicaÃ§Ã£o: regressÃ£o ridge
- CalcularÌ‚ ğ›½ envolve a inversÃ£o de uma matriz ğ‘ Ã— ğ‘ potencialmente grande.Ì‚
ğ›½ = (XâŠ¤X + ğœ†I)âˆ’1 XâŠ¤ğ‘¦.
- Usando a decomposiÃ§Ã£o SVD, tem-se
X = UDVâŠ¤.
- Ã‰ possÃ­vel mostrar que,Ì‚
ğ›½ = Vdiag ( ğ‘‘ğ‘—
ğ‘‘2
ğ‘— + ğœ† ) UâŠ¤ğ‘¦.
----  120
ImplementaÃ§Ã£o computacional:
regressÃ£o ridge
----  121
ImplementaÃ§Ã£o: regressÃ£o ridge
- Simulando o conjunto de dados (ğ‘› = 100, ğ‘ = 200).
set.seed(123)
X <- matrix(NA, ncol = 200, nrow = 100)
X[,1] <- 1 ## Intercepto
for(i in 2:200) {
X[,i] <- rnorm(100, mean = 0, sd = 1)
X[,i] <- (X[,i] - mean(X[,i]))/var(X[,i])
}
## ParÃ¢metros
beta <- rbinom(200, size = 1, p = 0.1)*rnorm(200, mean = 10)
mu <- X%*%beta
## ObservaÃ§Ãµes
y <- rnorm(100, mean = mu, sd = 10)
----  122
Implementando o modelo.
- Modelo passo-a-passo
y_c <- y - mean(y)
X_svd <- svd(X) ## DecomposiÃ§Ã£o svd
lambda = 0.5 ## PenalizaÃ§Ã£o
DD <- Diagonal(100, X_svd$d/(X_svd$d^2 + lambda))
DD[1] <- 0 ## NÃ£o penalizar o intercepto
beta_hat = as.numeric(X_svd$v%*%DD%*%t(X_svd$u)%*%y_c)
----  123
Resultados: regressÃ£o ridge
- Ajustados versus verdadeiros.
plot(beta ~ beta_hat, xlab = expression(hat(beta)), ylab = expression(beta))âˆ’4 âˆ’2 0 2 4 6 8
0 2 4 6 8 10 12
Î²
^
Î²
----  124
Resultados: regressÃ£o ridge
- RegressÃ£o com penalizaÃ§Ã£o ridge, bem como, outras penalizaÃ§Ãµes sÃ£o eficientemente
implementadas em R via pacote glmnet.
- IMPORTANTE! A penalizaÃ§Ã£o no glmnet Ã© ligeiramente diferente, por isso osÌ‚ ğ›½â€™s nÃ£o
sÃ£o idÃªnticos a nossa implementaÃ§Ã£o naive.
- O glmnet oferece opÃ§Ãµes para selecionar ğœ† via validaÃ§Ã£o cruzada.
require(glmnet)
beta_glm <- cv.glmnet(X[,-1], y_c, nlambda = 100)
----  125
Resultados: regressÃ£o ridge
- ValidaÃ§Ã£o cruzada.
plot(beta_glm)âˆ’2 âˆ’1 0 1 2
800 1000 1200 1400 1600 1800
Log(Î» )
Meanâˆ’Squared Error
103 96 94 90 88 82 80 76 74 67 60 52 43 38 32 27 22 15 9 3 0
----  126
Resultados: regressÃ£o ridge
- Ajustados (glmnet) versus verdadeiros.
plot(beta ~ as.numeric(coef(beta_glm)), xlab = expression(hat(beta)), ylab = expression(beta))âˆ’2 0 2 4 6 8
0 2 4 6 8 10 12
Î²
^
Î²
----  127
ComentÃ¡rios
----  128
ComentÃ¡rios
- SoluÃ§Ã£o de sistemas lineares:
- MÃ©todos diretos: EliminaÃ§Ã£o de Gauss e Gauss-Jordan.
- MÃ©todos iterativos: Jacobi e Gauss-Seidel.
- Inversa de matrizes.
- DecomposiÃ§Ã£o ou fatorizaÃ§Ã£o
- LU resolve sistema lineares pode ser usada para obter inversas.
- Autovalores e autovetores.
- Valores singulares.
- Existem muitas outras fatorizaÃ§Ãµes: QR, Cholesky, Cholesky modificadas, etc.
----  129

