---
title: "04_3-Caderno-InfEst-parte5"
author: "Helena R. S. D'Espindula"
output:
  html_document: 
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
      number_sections: true
  pdf_document:
date: "`r Sys.Date()`"
---

```{r setup, echo=TRUE, include=TRUE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, warning = TRUE, error = TRUE)
library(ggplot2)
library(glmnet)
library(Matrix)
library(bookdown)
library(numDeriv)
library(pracma)
```
\usepackage{amsmath}

- At√© agora resolvemos problemas lineares assim:
$$
(X^TX)^{-1} X^TX \hat{\beta} = (X^TX)^{-1}X^Ty
$$
- Fun√ß√£o linear
$$
y = \beta_0 + \beta_1 x_1 + \beta_p x_p +erro
$$
- Aqui tem um exp ent√£o n√£o d√° pra resolver assim:
$$
y = \frac{exp (\beta_0 + \beta_1 x_1 )}{coisas}
$$


# M√©todos Num√©ricos

## Sistemas de equa√ß√µes n√£o-lineares

### Equa√ß√µes n√£o-lineares
- Equa√ß√µes precisam ser resolvidas frequentemente em todas as √°reas da ci√™ncia.
- Equa√ß√£o de uma vari√°vel:$f(x) = 0$.
- A solu√ß√£o ou raiz √© um valor num√©rico de $x$ que satisfaz a equa√ß√£o.‚àí1.0 ‚àí0.5 0.0 0.5 1.0
- Solu√ß√£o de uma equa√ß√£o do tipo $f(x) = 0$ √© o ponto onde $f(x)$ cruza ou toca o eixo $x$.

### Solu√ß√£o de equa√ß√µes n√£o lineares
- Em muitas situa√ß√µes √© imposs√≠vel determinar a raiz analiticamente.
- Exemplo trivial $3x + 8 = 0 \rigtharrow x = -\frac{8}{3}$
- Exemplo n√£o-trivial $8 ‚àí 4.5(x ‚àí sin(x)) = 0 ‚Üí x = ?$
- Solu√ß√£o num√©rica de $f(x) = 0$ √© um valor de $x$ que satisfaz √† equa√ß√£o de forma aproximada.
- M√©todos num√©ricos para resolver equa√ß√µes s√£o divididos em dois grupos:
  1. M√©todos de confinamento;
  2. M√©todos abertos


### M√©todos de confinamento
- Identifica-se um intervalo que possui a solu√ß√£o.
- Usando um esquema num√©rico, o tamanho do intervalo √© reduzido sucessivamente at√© uma precis√£o desejada


### M√©todos abertos
- Assume-se uma estimativa inicial.
- Tentativa inicial deve ser pr√≥xima a solu√ß√£o.
- Usando um esquema num√©rico a solu√ß√£o √© melhorada.
- O processo para quando a precis√£o desejada √© atingida.


### Erros em solu√ß√µes num√©ricas
- Crit√©rio para determinar se uma solu√ß√£o √© suficientemente precisa.
- Seja $x_{ts}$ a solu√ß√£o verdadeira e $x_{ns}$ uma solu√ß√£o num√©rica.
- Quatro medidas podem ser consideradas para avaliar o erro:
1. Erro real $x_{ts} - x_{ns}$.

2. Toler√¢ncia em $f(x)$
$$
|f(x_{ts}) - f(x_{ns})| = |0 - e| = |e|
$$

3. Toler√¢ncia no tamanho do intervalo de busca:
$$
|\frac{b-a}{2}|
$$
4. Erro relativo estimado:
$$
|\frac{x^n_{ns}-x^{n-1}_{ns}}{x^{n-1}_{ns}}|
$$

  - Tolerancia que o prof usa normalmente:
$$
1e^{-4}
$$


## M√©todos de confinamento

### M√©todo da bisse√ß√£o
- M√©todo de confinamento.
- Sabe-se que dentro de um intervalo  $[a,b], f(x)$ √© cont√≠nua e possui uma solu√ß√£o.
- Neste caso $f(x)$ tem sinais opostos nos pontos finais do intervalo.

### Algoritmo: m√©todo da bisse√ß√£o
- Encontre $[a,b]$, tal que $f(a) f(b) < 0$.
- Calcule a primeira estimativa $x^{(1)}_{ns}$ usando $x^{(1)}_{ns} = \frac{a+b}{2}$
- Determine se a solu√ß√£o exata est√° entre $a$ e $x^{(1)}_{ns}$ ou entre $x^{(1)}_{ns}$ e $b$. Isso √© feito verificando o sinal do produto $f(a)f(x^{(1)}_{ns}) $
  1. Se $f(a) f(x^{(1)}_{ns}) < 0$, a solu√ß√£o est√° entre $a$ e $x^{(1)}_{ns}$
  2. Se  $f(a) f(x^{(1)}_{ns}) > 0$, a solu√ß√£o est√° entre $x^{(1)}_{ns}$ $b$.

- Selecione o subintervalo que cont√©m a solu√ß√£o e volte ao passo 2.
- Repita os passos 2 a 4 at√© que a toler√¢ncia especificada seja satisfeita

### Implementa√ß√£o R: m√©todo da bisse√ß√£o

```{r}
bissecao <- function(fx,
                     a,
                     b,
                     tol = 1e-04,
                     max_iter = 100) {
  fa <-
    fx(a)
  fb <- fx(b)
  if (fa * fb > 0)
    stop("Solu√ß√£o n√£o est√° no intervalo")
  solucao <- c()
  sol <- (a + b) / 2
  solucao[1] <- sol
  
  limites <- matrix(NA, ncol = 2, nrow = max_iter)
  for (i in 1:max_iter) {
    ## max_inter para n√£o ir at√© infinito se n√£o atingir a tolerancia
    test <- fx(a) * fx(sol)
    if (test < 0) {
      solucao[i + 1] <- (a + sol) / 2
      b = sol
    }
    if (test > 0) {
      solucao[i + 1] <- (b + sol) / 2
      a = sol
    }
    if (abs((b - a) / 2) < tol)
      break
    sol = solucao[i + 1]
    limites[i, ] <- c(a, b)
  }
  out <-
    list("Tentativas" = solucao,
         "Limites" = limites,
         "Raiz" = solucao[i + 1])
  return(out)
}
```


### Exemplo
- Encontre as ra√≠zes de
$$
D(\theta) = 2n[ log(\frac{\hat{\theta}}{\theta} + y_{linha} (\theta - \hat{\theta})] <= 3.84
$$

$$
D(\theta) = 2n[ log(\frac{\hat{\theta}}{\theta} + y_{linha} (\theta - \hat{\theta})] - 3.84 <= 0
$$

√© uma parabola de boca para cima, tem duas raizes em zero


```{r}
ftheta <- function(theta) {
  ## Implementando a fun√ß√£o
  dd <-
    2 * length(y) * (log(theta.hat / theta) + mean(y) * (theta - theta.hat))
  return(dd - 3.84)
}
set.seed(123) ## Resolvendo numericamente
y <- rexp(20, rate = 1)
theta.hat <- 1 / mean(y)
Ic_min <- bissecao(fx = ftheta, a = 0, b = theta.hat)
Ic_max <- bissecao(fx = ftheta, a = theta.hat, b = 3)
c(Ic_min$Raiz, Ic_max$Raiz) ## Solu√ß√£o aproximada
```


### M√©todo regula falsi
- Sabe-se que dentro de um intervalo $[a,b], f(x)$ √© cont√≠nua e possui uma solu√ß√£o.

### Algoritmo: m√©todo regula falsi
- Escolha os pontos $a$ e $b$ entre os quais existe uma solu√ß√£o.
- Calcule a primeira estimativa: $x^{(i)} = \frac{a f(b) - f f(a)}{f(b)-f(a)}$
- Determine se a solu√ß√£o est√° entre ùëé e ùë•ùëñ, ou entre ùë•(ùëñ) e ùëè.
  1. Se $f(a)f(x(i)) < 0$, a solu√ß√£o est√° entre $a$ e $x(i)$.
  2. Se $f(a)f(x(i)) > 0$, a solu√ß√£o est√° entre $x(i)$ e $b$.
- Selecione o subintervalo que cont√©m a solu√ß√£o como o novo intervalo [ùëé,ùëè] e volte ao passo 2.
- Repita passos 2 a 4 at√© converg√™ncia


### Implementa√ß√£o R: m√©todo regula falsi

```{r}

regula_falsi <- function(fx,
                         a,
                         b,
                         tol = 1e-04,
                         max_iter = 100) {
  fa <-
    fx(a)
  fb <- fx(b)
  if (fa * fb > 0)
    stop("Solu√ß√£o n√£o est√° no intervalo")
  solucao <- c()
  sol <- (a * fx(b) - b * fx(a)) / (fx(b) - fx(a))
  solucao[1] <- sol
  limites <- matrix(NA, ncol = 2, nrow = max_iter)
  for (i in 1:max_iter) { 
    ## max_inter para n√£o ir at√© infinito se n√£o atingir a tolerancia
    test <- fx(a) * fx(sol)
    if (test < 0) {
      b = sol
      solucao[i + 1] <- (a * fx(b) - b * fx(a)) / (fx(b) - fx(a))
    }
    if (test > 0) {
      a = sol
      solucao[i + 1] <- sol <- (a * fx(b) - b * fx(a)) / (fx(b) - fx(a))
    }
    if (abs(solucao[i + 1] - solucao[i]) < tol)
      break
    sol = solucao[i + 1]
    limites[i, ] <- c(a, b)
  }
  out <-
    list("Tentativas" = solucao,
         "Limites" = limites,
         "Raiz" = sol)
  return(out)
}
```

### Aplica√ß√£o: regula-falsi
- Encontre as ra√≠zes de
$$
D(\theta) = 2n[ log(\frac{\hat{\theta}}{\theta} + y_{linha} (\theta - \hat{\theta})] <= 3.84
$$

```{r}
## Resolvendo numericamente
Ic_min <- regula_falsi(fx = ftheta, a = 0.1, b = theta.hat)
Ic_max <- regula_falsi(fx = ftheta, a = theta.hat, b = 3)
## Solu√ß√£o aproximada
c(Ic_min$Raiz, Ic_max$Raiz)
```



$$
D(\theta) = 2n[ log(\frac{\hat{\theta}}{\theta} + y_{linha} (\theta - \hat{\theta})] <= 3.84
$$
√© igual a:
$$ f(\theta) - 3,84 = 0 $$
$$ f(\theta = 0,1)$$
$$x^{(2)} = 0,1 - \alpha f(\theta = 0,1) = 0,9$$
$$x^{(3)} = 0,9 - \alpha f(\theta = 0,9) = 1,8$$
$$x^{(4)} = 1,8 - \alpha f(\theta = 1,8)$$



### Coment√°rios: m√©todos de confinamento

- Sempre convergem para uma resposta, desde que uma raiz esteja no intervalo.
- Podem falhar quando a fun√ß√£o √© tangente ao eixo $x$, n√£o cruzando em $f(x) = 0$.
- Converg√™ncia √© lenta em compara√ß√£o com outros m√©todos.
- S√£o dif√≠ceis de generalizar para sistemas de equa√ß√µes n√£o-lineares.


## M√©todos abertos

### M√©todo de Newton

- Fun√ß√£o deve ser cont√≠nua e diferenci√°vel.
- Fun√ß√£o deve possuir uma solu√ß√£o perto do ponto inicial

### Algoritmo: m√©todo de Newton
- Escolha um ponto $x_1$ como inicial.
- Para $i = 1,2, ...$ at√© que o erro seja menor que um valor especificado, calcule
$$
x^{(i+1)} = x^{(i)} - \frac{f(x^{(i)})}{f'(x^{(i)})}
$$

- Implementa√ß√£o computacional

```{r}
newton <- function(fx, f_prime, x1, tol = 1e-04, max_iter = 10) {
  solucao <- c()
  solucao[1] <- x1
  for(i in 1:max_iter) {
    solucao[i+1] = solucao[i] - fx(solucao[i])/f_prime(solucao[i])
    if( abs(solucao[i+1] - solucao[i]) < tol) break
  }
  return(solucao)
}
```

### Aplica√ß√£o: m√©todo de Newton
- Encontre as ra√≠zes de
$$
D(\theta) = 2n[ log(\frac{\hat{\theta}}{\theta} + y_{linha} (\theta - \hat{\theta})] <= 3.84
$$

- Derivada
$$
D'(\theta) = 2n(y - 1/ \theta)
$$


```{r}
## Derivada da fun√ß√£o a ser resolvida
fprime <- function(theta){2*length(y)*(mean(y) - 1/theta)}
## Solu√ß√£o numerica
Ic_min <- newton(fx = ftheta, f_prime = fprime, x1 = 0.1)
Ic_max <- newton(fx = ftheta, f_prime = fprime, x1 = 2)
c(Ic_min[length(Ic_min)], Ic_max[length(Ic_max)])
```

### M√©todo gradiente descendente

Obs: na matematica √© mais comum procurar o minimo do que o m√°ximo.

- M√©todo do gradiente descendente em geral √© usado para otimizar uma fun√ß√£o.
- Suponha que desejamos minimizar $F(x)$ cuja derivada √© $f(x)$.
- Sabemos que um ponto critico ser√° obtido em $f(x) = 0$
- Note que $f(x)$ √© o gradiente de $F(x)$, assim aponta na dire√ß√£o de m√°ximo.
- Assim, podemos caminhar na dire√ß√£o contr√°ria seguindo o gradiente, i.e.
$$
x^{(i+1)} = x^{(i)} - \alpha f (x^{(i)})
$$
- $\alpha$ √© um par√¢metro de tuning usado para controlar o tamanho do passo.
- A escolha do $\alpha$ √© fundamental para atingir converg√™ncia.
- Busca em gride pode ser uma op√ß√£o razo√°vel.

Obs: $\alpha$ = parametro de tuning ou taxa de aprendizagem. Idealmente valor pequeno para ir em passos pequenos.

### Algoritmo: m√©todo gradiente descendente
- Escolha um ponto $x_1$ como inicial.
- Para $i - 1,2, ....$ at√© que o erro seja menor que um valor especificado, calcule:
$$
x^{(i+1)} = x^{(i)} - \alpha f (x^{(i)})
$$

- Implementa√ß√£o computacional

```{r}
grad_des <- function(fx,
                     x1,
                     alpha,
                     max_iter = 100,
                     tol = 1e-04) {
  sol <- c()
  sol[1] <- x1
  for (i in 1:max_iter) {
    sol[i + 1] <- sol[i] - alpha * fx(sol[i])
    if (abs(fx(sol[i + 1])) < tol)
      break
  }
  return(sol)
}
```

### Aplica√ß√£o: m√©todo gradiente descendente

- Encontre as ra√≠zes de
$$
D(\theta) = 2n[ log(\frac{\hat{\theta}}{\theta} + y_{linha} (\theta - \hat{\theta})] <= 3.84
$$


```{r}
## Solu√ß√£o numerica
Ic_min <- grad_des(fx = ftheta, alpha = -0.02, x1 = 0.1)
Ic_max <- grad_des(fx = ftheta, alpha = 0.01, x1 = 4)
c(Ic_min[length(Ic_min)], Ic_max[length(Ic_max)])
```



## Sistemas de equa√ß√µes n√£o-lineares

### Sistemas de equa√ß√µes

- Sistema com duas equa√ß√µes:
$$
f_1(x_1,x_2) = 0
$$

$$
f_2(x_1,x_2) = 0
$$

- A solu√ß√£o num√©rica consiste em encontrar $\hat{x_1}$ e $\hat{x_2}$ que satisfa√ßa o sistema de equa√ß√µes.

- A ideia √© facilmente estendida para um sistema com ùëõ equa√ß√µes
$$
f_1(x_1,x_n) = 0
$$
...
$$
f_n(x_1,x_n) = 0
$$

- Genericamente, tem-se
$$
f(x) = 0
$$

## M√©todo de Newton


### Algoritmo: m√©todo de Newton
- Escolha um vetor $x1$ como inicial.
- Para $i = 1,2 ....$ at√© que o erro seja menor que um valor especificado, calcule
$$
x^{(i+1)} = x^{(i)} - J(x^{(i)})^{-1} f(x^{(i)})
$$
onde

$$
$$

√© chamado Jacobiano de $f(x).

- Implementa√ß√£o computacional

```{r}
newton <- function(fx, jacobian, x1, tol = 1e-04, max_iter = 10) {
  solucao <- matrix(NA, ncol = length(x1), nrow = max_iter)
  solucao[1,] <- x1
  for(i in 1:max_iter) {
    J <- jacobian(solucao[i,])
    grad <- fx(solucao[i,])
    solucao[i+1,] = solucao[i,] - solve(J, grad)
    if( sum(abs(solucao[i+1,] - solucao[i,])) < tol) break
  }
  return(solucao)
}
```


### Aplica√ß√£o: m√©todo de Newton
- Resolva
ùëì1(ùë•1, ùë•2) = ùë•2 ‚àí 1
2 (expùë•1/2 + exp‚àíùë•/2) = 0
ùëì2(ùë•1, ùë•2) = 9ùë•2
1 + 25ùë•2
2 ‚àí 225 = 0.
- Precisamos obter o Jacobiano, assim tem-se
J(ùë•) = [‚àí 1
2 ( expùë•1/2
2 ‚àí exp‚àíùë•1/2
2 ) 1
18ùë•1 50ùë•

```{r}
## Sistema a ser resolvido
fx <- function(x) {
  c(x[2] - 0.5 * (exp(x[1] / 2) + exp(-x[1] / 2)),
    9 * x[1] ^ 2 + 25 * x[2] ^ 2 - 225)
}
## Jacobiano
Jacobian <- function(x) {
  jac <- matrix(NA, 2, 2)
  jac[1, 1] <- -0.5 * (exp(x[1] / 2) / 2 - exp(-x[1] / 2) / 2)
  jac[1, 2] <- 1
  jac[2, 1] <- 18 * x[1]
  jac[2, 2] <- 50 * x[2]
  return(jac)
}
```

```{r}
## Resolvendo
sol <- newton(fx = fx, jacobian = Jacobian, x1 = c(1,1))
tail(sol,4) ## Solu√ß√£o
```

```{r}
fx(sol[8,]) ## OK
```

### Coment√°rios: m√©todo de Newton
- M√©todo de Newton ir√° convergir tipicamente se tr√™s condi√ß√µes forem satisfeitas:
1. As fun√ß√µes ùëì1, ùëì2, ‚Ä¶ , ùëìùëõ e suas derivadas forem cont√≠nuas e limitadas na vizinhan√ßa da
solu√ß√£o.
2. O Jacobiano deve ser diferente de zero na vizinhan√ßa da solu√ß√£o.
3. A estimativa inicial de solu√ß√£o deve estar suficientemente pr√≥xima da solu√ß√£o exata.
- Derivadas parciais (elementos da matriz Jacobiana) devem ser determinados. Isso pode ser
feito analitica ou numericamente.
- Cada passo do algoritmo envolve a invers√£o de uma matriz.


### M√©todo gradiente descendente


- Escolha um vetor ùë•1 como inicial.
- Para ùëñ = 1,2, ‚Ä¶ at√© que o erro seja menor que um valor especificado, calcule
ùë•(ùëñ+1) = ùë•(ùëñ) ‚àí ùõºf(ùë•(ùëñ)).
- Implementa√ß√£o computacional


```{r}
grad_des <- function(fx,
                     x1,
                     alpha,
                     max_iter = 100,
                     tol = 1e-04) {
  solucao <- matrix(NA, ncol = length(x1), nrow = max_iter)
  solucao[1, ] <- x1
  for (i in 1:c(max_iter - 1)) {
    solucao[i + 1, ] <- solucao[i, ] - alpha * fx(solucao[i, ])
    #print(c(i, solucao[i+1,]))
    if (sum(abs(solucao[i + 1, ] - solucao[i, ])) <= tol)
      break
  }
  return(solucao)
}
```

### Aplica√ß√£o: m√©todo gradiente descendente
- Resolva
$$
$$

$$
$$

onde $y_i = (5.15; 6.40; 2.77; 5.72; 6.25; 3.45; 5.00; 6.86; 4.86; 3.72)$ e
$z_i = (0.28; 0.78; 0.40; 0.88; 0.94; 0.04; 0.52; 0.89; 0.55; 0.45).$

- Implementa√ß√£o computacional

```{r}
fx <- function(x) {
  y <- c(5.15, 6.40, 2.77, 5.72, 6.25, 3.45, 5.00, 6.86, 4.86, 3.72)
  z <- c(0.28, 0.78, 0.40, 0.88, 0.94, 0.04, 0.52, 0.89, 0.55, 0.45)
  term1 <- -2 * sum(y - x[1] - x[2] * z)
  term2 <- -2 * sum((y - x[1] - x[2] * z) * z)
  out <- c(term1, term2)
  return(out)
}
sol_grad <-
  grad_des(
    fx = fx,
    x1 = c(5, 0),
    alpha = 0.05,
    max_iter = 140
  )
fx(x = sol_grad[137, ])
```

### Coment√°rios: m√©todo gradiente descendente

- Vantagem: n√£o precisa calcular o Jacobiano!!
- Desvantagem: precisa de tuning.
- Em geral precisa de mais itera√ß√µes que o m√©todo de Newton.
- Cada itera√ß√£o √© mais barata computacionalmente.
- Uma varia√ß√£o do m√©todo √© conhecido como steepest descent.
- Avalia a mudan√ßa em ùëì(ùë•) para um gride de ùõº e da o passo usando o ùõº que torna ùêπ (ùë•)
maior/menor.
- O tamanho do passo pode ser adaptativo.
- Cuidado! Sup√µe que a fun√ß√£o subjacente est√° sendo minimizada!


## Diferencia√ß√£o num√©rica

- Derivada d√° uma medida da taxa na qual a vari√°vel $y$ muda devido a uma mudan√ßa na vari√°vel $x$.
- A fun√ß√£o a ser diferenciada pode ser dada por uma fun√ß√£o $f(x)$, ou apenas por um conjunto de pontos $(y_i, x_i)$.
- Quando devemos usar derivadas num√©ricas?
  1. $f'(x)$ √© dificil de obter analiticamente.
  2. $f'(x)$ √© caro para calcular computacionalmente.
  3. Quando a fun√ß√£o √© especificada apenas por um conjunto de pontos.
- Abordagens para a diferencia√ß√£o num√©rica
  1. Aproxima√ß√£o por diferen√ßas finitas.
  2. Aproximar a fun√ß√£o por uma outra fun√ß√£o de f√°cil deriva√ß√£o.
  3. Diferencia√ß√£o autom√°tica (fora do nosso escopo)

### Aproxima√ß√£o da derivada por diferen√ßas finitas
- Derivada $f'(x)$ de uma fun√ß√£o $f'(x)$ no ponto $x =a$ √© definida como:
$$
f'(a) = \lim_{x \to a}{\frac{f(x)-f(a)}{x-a}}
$$

- Derivada √© a inclina√ß√£o da reta tangente √† fun√ß√£o em $x=a$
- Escolhe-se um ponto $x$ pr√≥ximo a $a$ e calcula-se a inclina√ß√£o da reta que conecta os dois pontos.
- A precis√£o do c√°lculo aumenta quando $x$ aproxima de $a$
- Aproxima√ß√£o num√©rica: a fun√ß√£o ser√° avaliada em diferentes pontos pr√≥ximos a $a$ para aproximar $f'(a)$.

- F√≥rmulas para diferencia√ß√£o num√©rica:
1. Diferen√ßa progressiva: inclina√ß√£o da reta que conecta os pontos $(x_i,f(x_i))$ e $(x_{i+1},f(x_{i+1}))$ 
$$
f'(x_i) = \frac{f(x_{i+1}) - f(x_{i})}{x_{i+1} - x_i}
$$
2. Diferen√ßa regressiva: inclina√ß√£o da reta que conecta os pontos $(x_{i-1},f(x_{i-1}))$ e $(x_{i},f(x_{i}))$ 
$$
f'(x_i) = \frac{f(x_{i}) - f(x_{i-1})}{x_{i} - x_{i-1}}
$$
3. Diferen√ßa central: inclina√ß√£o da reta que conecta os pontos $(x_{i-1},f(x_{i-1}))$ e $(x_{i+1},f(x_{i+1}))$ 
$$
f'(x_i) = \frac{f(x_{i+1}) - f(x_{i-1})}{x_{i+1} - x_{i-1}}
$$

Obs: nessas formulas o 1 n√£o precisa ser litelmente 1, √© s√≥ um valor pequeno.


- Diferen√ßa progressiva
```{r}
dif_prog <- function(fx, x, h) {
df <- (fx(x + h) - fx(x))/( (x + h) - x)
return(df)
}
```

- Diferen√ßa regressiva
```{r}
dif_reg <- function(fx, x, h) {
df <- (fx(x) - fx(x - h))/( x - (x - h))
return(df)
}
```


- Diferen√ßa central
```{r}
dif_cen <- function(fx, x, h) {
df <- (fx(x + h) - fx(x - h))/( (x + h) - (x - h))
return(df)}
```


Exemplo: aproxima√ß√£o da derivada por diferen√ßas finitas
- Considere $f(x) = x^3$, assim $f'(x) = 3x^2$
```{r}
fx <- function(x) x^3
dif_prog(fx, x = 2, h = 0.001) ## Diferen√ßa progressiva
## [1] 12.006
dif_reg(fx, x = 2, h = 0.001) ## Diferen√ßa regressiva
## [1] 11.994
dif_cen(fx, x = 2, h = 0.001) ## Diferen√ßa central
## [1] 12
3*2^2 ## Exata
## [1] 12
```

## Diferen√ßas finitas usando expans√£o em S√©ries de Taylor

- As f√≥rmulas anteriores podem ser deduzidas usando expans√£o em s√©rie de Taylor.
- O n√∫mero de pontos para aproximar a derivada pode mudar.
- Vantagem da dedu√ß√£o por s√©rie de Taylor √© que ela fornece uma estimativa do erro de truncamento.


### Diferen√ßa finita progressiva com dois pontos
- Aproxima√ß√£o de Taylor para o ponto $x_{i+1}$ **+1**
$$
f(x_{i+1}) = f(x_i) + f'(x_i)h + \frac{f''(x_i)}{2!}h^2 +  \frac{f'''(x_i)}{3!}h^3 + ....
$$
onde $h = x_{i+1} - x_{i}$
- Fixando dois termos e deixando os outros termos como um **res√≠duo**, temos
$$
f(x_{i+1}) = f(x_i) + f'(x_i)h + \frac{f''(\xi)}{2!}h^2
$$
- Resolvendo para $f'(x_i)$, temos
$$
f(x_{i}) = \frac{f(x_{i+1}) + f'(x_i)}{h} - \frac{f''(\xi)}{2!}h^2
$$
- Erro de truncamento,
$$
- \frac{f''(\xi)}{2!}h^2 = O(h)
$$

### Diferen√ßa finita regressiva com dois pontos
- Aproxima√ß√£o de Taylor para o ponto $x_{i-1}$ **-1**
$$
f(x_{i-1}) = f(x_i) - f'(x_i)h + \frac{f''(x_i)}{2!}h^2 +  \frac{f'''(x_i)}{3!}h^3 + ....
$$
onde $h = x_{i} - x_{i-1}$
- Fixando dois termos e deixando os outros termos como um res√≠duo, temos
$$
f(x_{i-1}) = f(x_i) + f'(x_i)h + \frac{f''(\xi)}{2!}h^2
$$
- Resolvendo para ùëì‚Ä≤(ùë•ùëñ), temos
$$
f(x_{i}) = \frac{f(x_{i}) - f'(x_{i-1})}{h} - \frac{f''(\xi)}{2!}h^2
$$
- Erro de truncamento, ùëì‚Ä≥(ùúâ)
$$
\frac{f''(\xi)}{2!}h^2 = O(h)
$$



### Diferen√ßa finita central com dois pontos
- Aproxima√ß√£o de Taylor para o ponto $x_{i+1}$
$$
f(x_{i+1}) = f(x_i) + f'(x_i)h + \frac{f''(x_i)}{2!}h^2 +  \frac{f'''(x_i)}{3!}h^3 + ....
$$
onde $erro_1$ est√° entre $x_1$ e $x_{i-1}$.
- Aproxima√ß√£o de Taylor para o ponto ùë•ùëñ‚àí1
$$
f(x_{i-1}) = f(x_i) - f'(x_i)h + \frac{f''(x_i)}{2!}h^2 +  \frac{f'''(x_i)}{3!}h^3 + ....
$$
onde $erro_2$ est√° entre $x_{i-1}$ e $x_{i}$.

### Diferen√ßa finita central com dois pontos
- Subtraindo as equa√ß√µes, temos
$$
$$
- Resolvendo para ùëì‚Ä≤(ùë•ùëñ), temos
$$
$$

### Diferen√ßa finita progressiva com tr√™s pontos
- Aproxima ùëì‚Ä≤(ùë•ùëñ) avaliando a fun√ß√£o no ponto e nos dois pontos seguintes ùë•ùëñ+1 e ùë•ùëñ+2.
- Aproxima√ß√£o de Taylor em ùë•ùëñ+1 e ùë•ùëñ+2,
$$
$$

$$
$$
- Equa√ß√µes 1 e 2 s√£o combinadas de forma que os termos com derivada segunda
desapare√ßam.
- Multiplicando Eq. 1 por 4 e subtraindo Eq. 2, temos
$$
$$

Diferen√ßa finita com tr√™s pontos
- Resolvendo em ùëì‚Ä≤(ùë•ùëñ), temos
$$
$$
- Diferen√ßa finita regressiva com tr√™s pontos
$$
$$


## Derivadas de segunda ordem

### F√≥rmulas de diferen√ßas finitas para a segunda derivada
- Usando as mesmas ideias podemos aproximar a derivada segunda de uma fun√ß√£o qualquer
por diferen√ßas finitas.
- A deriva√ß√£o das f√≥rmulas s√£o id√™nticas, por√©m mais tediosas.
- F√≥rmula da diferen√ßa central com tr√™s pontos para a derivada segunda
$$
$$
- Diferen√ßa central com quatro pontos
$$
$$

### F√≥rmulas de diferen√ßas finitas para a segunda derivada
- Diferen√ßa progressiva com tr√™s pontos
$$
$$
- Diferen√ßa regressiva com tr√™s pontos
$$
$$
- Uma infinidade de f√≥rmulas de v√°rias ordens est√£o dispon√≠veis.
- F√≥rmulas de diferencia√ß√£o podem ser obtidas usando polin√¥mios de Lagrange


### Erros na diferencia√ß√£o num√©rica
- Em todas as f√≥rmulas o erro de truncamento √© fun√ß√£o de ‚Ñé.
- ‚Ñé √© o espa√ßamento entre os pontos, i.e. ‚Ñé = ùë•ùëñ+1 ‚àí ùë•ùëñ.
- Com ‚Ñé pequeno o erro de truncamento ser√° pequeno.
- Em geral usa-se a precis√£o da m√°quina, algo como 1ùëí‚àí16.
- O erro de arredondamento depende da precis√£o finita de cada computador.
- Mesmo que ‚Ñé possa ser t√£o pequeno quanto desejado o erro de arredondamento pode crescer quando se diminue ‚Ñé.


## Extrapola√ß√£o de Richardson


- Extrapola√ß√£o de Richardson √© usada para obter uma aproxima√ß√£o mais precisa da derivada
a partir de duas aproxima√ß√µes menos precisas.
- Considere o valor ùê∑ de uma derivada (desconhecida) calculada pela f√≥rmula
$$
$$
onde ùê∑(‚Ñé) aproxima ùê∑ e ùëò2 e ùëò4 s√£o termos de erro.
- O uso da mesma f√≥rmula, por√©m com espa√ßamento ‚Ñé/2 resulta
$$
$$


- A Eq. (4) pode ser rescrita (ap√≥s multiplicar por 4):
$$
$$
- Subtraindo (3) de (5) elimina os termos com ‚Ñé2 e fornece
$$
$$
- Resolvendo (6), temos
$$
$$

- O erro na Eq. (7) √© agora ùëÇ(‚Ñé4). O valor de ùê∑ √© aproximado por
$$
$$
- A partir de duas aproxima√ß√µes de ordem inferiores, obtemos uma aproxima√ß√£o de ùëÇ(‚Ñé4) mais precisa.
- Procedimento a partir de duas aproxima√ß√µes com erro ùëÇ(‚Ñé4) mostra que
$$
$$
- Aproxima√ß√£o ainda mais precisa.


### Exemplo: extrapola√ß√£o de Richardson
- Calcule a derivada de ùëì(ùë•) = 2ùë• ùë• no ponto ùë• = 2.
- Solu√ß√£o exata: $ $
- Solu√ß√£o num√©rica usando diferen√ßa central

```{r}
fx <- function(x) (2^x)/x
fpx <- function(x)(log(2)*(2^x))/
x - (2^x)/x^2
erro <- fpx(x = 2)/
dif_cen(fx = fx, x = 2, h = 0.2)
(erro-1)*100
## [1] 0.345544
```

- Extrapola√ß√£o de Richardson

```{r}
D2 <- dif_cen(fx = fx, x = 2, h = 0.2/2)
D <- dif_cen(fx = fx, x = 2, h = 0.2)
der <- (1/3)*( 4*D2 - D)
erro2 <- fpx(x = 2)/der
(erro2-1)*100
## [1] -0.001585268
```


```{r}
c("Exata" = fpx(x = 2), "Richardson" = der,
"Central" = dif_cen(fx = fx, x = 2, h = 0.2))
## Exata Richardson Central
## 0.3862944 0.3863005 0.3849641
```


## Derivadas parciais

- Para fun√ß√µes com muitas vari√°veis, a derivada parcial da fun√ß√£o em rela√ß√£o a uma das
vari√°veis representa a taxa de varia√ß√£o da fun√ß√£o em rela√ß√£o a essa vari√°vel, mantendo as
demais constantes.
- Assim, as f√≥rmulas de diferen√ßas finitas podem ser usadas no c√°lculo das derivadas parciais.
- As f√≥rmulas s√£o aplicadas em cada uma das vari√°veis, mantendo as outras fixas.
- A mesma ideia se aplica para derivadas de mais alta ordem


### Implementa√ß√£o: derivadas parciais
- Derive 
$$
f(\beta_0, \beta_1) = \sum^n_{i=1}{|y_i - (\beta_0 + \beta_1 x_i)|}
$$

- Para fazer vai calcular derivada de cada beta (o outro fica como contante):
$$
\frac{\partial( \beta_0, \beta_1) }{\partial \beta_0} = ....
$$

$$
\frac{\partial( \beta_0, \beta_1) }{\partial \beta_1} = ....
$$
- F√≥rmula dois pontos central
```{r}
dif_cen <- function(fx, pt, h, ...) {
  df <- (fx(pt + h, ...) - fx(pt - h, ...))/( (pt + h) - (pt - h))
  return(df)
}
```

- Fun√ß√£o a ser diferenciada
```{r}
fx <- function(par, y, x1) {sum ( abs( y - (par[1] + par[2]*x1)) )}
```

- Gradiente usando diferen√ßas finita.
```{r}
grad_fx <- function(fx, par, h, ...) {
  fbeta0 <-
    function(beta0, beta1, y, x)
      fx(par = c(beta0, beta1),
         y = y,
         x = x) #beta1 constante
  fbeta1 <-
    function(beta1, beta0, y, x)
      fx(par = c(beta0, beta1),
         y = y,
         x = x) #beta0 constante
  db0 <-
    dif_cen(
      fx = fbeta0,
      pt = par[1],
      h = h,
      beta1 = par[2],
      y = y,
      x = x
    )
  db1 <-
    dif_cen(
      fx = fbeta1,
      pt = par[2],
      h = h,
      beta0 = par[1],
      y = y,
      x = x
    )
  return(c(db0, db1))
}
```


Exemplo: derivadas parciais

- Simulando $y_i$'s e $x_i$'s.
```{r}
set.seed(123)
x <- runif(100)
y <- rnorm(100, mean = 2 + 3*x, sd = 1)
```

- Gradiente num√©rico
```{r}
grad_fx(fx = fx, par = c(2, 3), h = 0.001, y = y, x1 = x)
## [1] 6.000000 2.272805
```

- Gradiente anal√≠tico
```{r}
c(sum(((y - 2 - 3*x)/abs(y - 2 -3*x))*(-1)),
sum(((y - 2 - 3*x)/abs(y - 2 -3*x))*(-x)))
## abs = modulo
## [1] 6.000000 2.272805
```


## Uso de fun√ß√µes residentes do R para diferencia√ß√£o num√©rica

- Pacote numDeriv implementa derivadas por diferen√ßa finita.
- Gradiente
```{r}
require(numDeriv)
args(grad)
## function (func, x, method = "Richardson", side = NULL, method.args = list(),
## ...)
## NULL
```

- Hessiano
```{r}
args(hessian)
## function (func, x, method = "Richardson", method.args = list(),
## ...)
## NULL
```


- Aplica√ß√£o
```{r}
## Fun√ß√£o a ser diferenciada:
fx <- function(par, y, x1) {sum ( abs( y - (par[1] + par[2]*x1)) )}

## Calculando:
grad(func = fx, x = c(2, 3), y = y, x1 = x)
## [1] 6.000000 2.272805
```

```{r}
hessian(func = fx, x = c(2, 3), y = y, x1 = x)
## [,1] [,2]
## [1,] 58.91271 29.53710
## [2,] 29.53710 48.86648
```


## Integra√ß√£o num√©rica

- Integrais aparecem com frequ√™ncia em c√°lculo de probabilidades.
- A probabilidade de um evento √© a √°rea abaixo de uma curva.
- Em modelos complicados a integral pode n√£o ter solu√ß√£o anal√≠tica.
- Os m√©todos de integra√ß√£o num√©rica, podem ser dividos em tr√™s grupos:
  1. M√©todos baseados em soma finita.
  2. Aproximar a fun√ß√£o por uma outra de f√°cil integra√ß√£o.
  3. Estimar o valor da integral.

- Considere a distribui√ß√£o Gaussiana (ou Normal) com fun√ß√£o densidade probabilidade dada por
$$
f(x; \mu, \sigma^2 ) = \frac{1}{\sq(2 \pi \sigma)} exp (- \frac{1}{2 \sigma^2}(x - \mu)^2)
$$
- Gr√°fico da fun√ß√£o

Integra√ß√£o num√©rica
- O c√°lculo de uma probabilidade qualquer baseado nesta distribui√ß√£o √© dado pela seguinte
integral
$$
$$

## M√©todo Trapezoidal

- Usa uma fun√ß√£o linear para aproximar o integrando.
- O integrando pode ser aproximado por S√©rie de Taylor
$$
$$
- Integrando analiticamente essa aproxima√ß√£o, tem-se
$$
$$
- Simplificando, obtem-se
$$
$$

## M√©todo de Simpson 1/3

- Aproxima o integrando por um polin√¥mio de segunda ordem.
- Pontos finais ùë•1 = ùëé, ùë•3 = ùëè, e o ponto central, ùë•2 = (ùëé + ùëè)/2.
- O polin√¥mio pode ser escrito na forma:
ùëù(ùë•) = ùõº + ùõΩ(ùë• ‚àí ùë•1) + ùúÜ(ùë• ‚àí ùë•1)(ùë• ‚àí ùë•2) (8)
onde ùõº, ùõΩ e ùúÜ s√£o constantes desconhecidas.
- Impomos a condi√ß√£o que o polin√¥mio deve passar por todos os pontos, ùëù(ùë•1) = ùëì(ùë•1), ùëù(ùë•2) = ùëì(ùë•2) e ùëù(ùë•3) = ùëì(ùë•3).


- Isso resulta em:
$$
$$
- Substituindo em 8 e integrando ùëù(ùë•), obt√©m-se
$$
$$

- A integral √© facilmente calculada com apenas tr√™s avalia√ß√µes da fun√ß√£o.
```{r}
simpson <- function(integrando, a, b, ...){
h <- (b-a)/2
x2 <-(a+b)/2
integral <- (h/3)*(integrando(a,...) +
4*integrando(x2, ...) + integrando(b, ...))
return(integral)
}
```


- Exemplo: calcule ‚à´3
```{r}
fx <- function(x) x^2
simpson(integrando = fx, a = 2, b = 3)
## [1] 6.333333
```


- Solu√ß√£o exata:
$$
$$

## Quadratura Gaussiana


- Os m√©todos trapezoidal e Simpson s√£o muito simples.
- Aproximam o integrando por um polin√¥mio de f√°cil integra√ß√£o.
- Resolvem a integral aproximada.
- Os pontos s√£o igualmente espa√ßados.
- S√£o simples e intuitivos, por√©m de dif√≠cil generaliza√ß√£o.
- A Quadratura Gaussiana √© um dos m√©todos mais populares de integra√ß√£o num√©rica.
- Aplica√ß√µes: modelos mistos n√£o-lineares, an√°lise de dados longitudinais, medidas
repetidas, modelos lineares generalizados mistos, etc.


- Forma geral da quadratura de Gauss:

\begin{equation}
  \tag{9}
  \int_{a}^{b} f(x)dx === \sum^n_{i=1}{C_i f(x_i)}
\end{equation}

onde $C_i$ s√£o pesos e $x_i$ s√£o os pontos de Gauss em $[a, b]$.
- Exemplo 1: para ùëõ = 2 a Eq. 9 tem a forma:
$$
$$
- Exemplo 2: para ùëõ = 3 a Eq. 9 tem a forma:
$$
$$

- Coeficientes ùê∂ùëñ e a localiza√ß√£o dos pontos ùë•ùëñ depende dos valores de ùëõ, ùëé e ùëè.
- ùê∂ùëñ e ùë•ùëñ s√£o determinados de forma que o lado direito da Eq. 9 seja igual ao lado esquerdo
para fun√ß√µes ùëì(ùë•) especificadas.
- A especifica√ß√£o de ùëì(ùë•) vai depender do dom√≠nio de integra√ß√£o.
- Diferentes dom√≠nios levam a diferentes varia√ß√µes do m√©todo



- Dom√≠nios comuns:
1. Gauss-Legendre, Gauss-Jacobi e Gauss-Chebyshev
$$
$$
2. Gauss-Laguerre
$$
$$
3. Gauss-Hermite
$$
$$

No dom√≠nio [‚àí1,1] a forma da quadratura de Gauss √©
$$
$$
- ùê∂ùëñ e ùë•ùëñ s√£o determinados fazendo com que a Eq. 9 seja exata quando
ùëì(ùë•) = 1, ùë•, ùë•2, ùë•3 ‚Ä¶.
- O n√∫mero de casos depende do valor de ùëõ.
- Para ùëõ = 2, tem-se
$$
$$


## Quadratura de Gauss-Legendre

Quadratura de Gauss-Legendre
- As quatro constantes ùê∂1, ùê∂2, ùë•1 e ùë•2 s√£o determinadas fazendo Eq. 10 exata quando
aplicada aos quatro casos:
Caso 1 ùëì(ùë•) = 1 ‚à´1
‚àí1 1ùëëùë• = 2 = ùê∂1 + ùê∂2
Caso 2 ùëì(ùë•) = ùë• ‚à´1
‚àí1 ùë•ùëëùë• = 0 = ùê∂1ùë•1 + ùê∂2ùë•2
Caso 3 ùëì(ùë•) = ùë•2 ‚à´1
‚àí1 ùë•2ùëëùë• = 2
3 = ùê∂1ùë•2
1 + ùê∂2ùë•2
2
Caso 4 ùëì(ùë•) = ùë•3 ‚à´1
‚àí1 ùë•3ùëëùë• = 0 = ùê∂1ùë•3
1 + ùê∂2ùë•3
2
- Sistema n√£o-linear de quatro equa√ß√µes e quatro inc√≥gnitas.
- Podem existir m√∫ltiplas solu√ß√µes.
- Uma solu√ß√£o particular √© obtido por impor que ùë•1 = ‚àíùë•2.
- Pela equa√ß√£o 2, implica que ùê∂1 = ùê∂2 e a solu√ß√£o √©
$$
$$

Exemplo: quadratura de Gauss-Legendre
- Calcule ‚à´1
‚àí1 ùë•2ùëëùë•.
- Usando Gauss-Legendre com dois pontos, tem-se
$$
$$


- Quando ùëì(ùë•) √© uma fun√ß√£o do tipo ùëì(ùë•) = 1, ùëì(ùë•) = ùë•, ùëì(ùë•) = ùë•2 ou ùëì(ùë•) = ùë•3 ou
qualquer combina√ß√£o linear destas a aproxima√ß√£o √© exata.
- Caso contr√°rio o procedimento fornece uma aproxima√ß√£o.
- Exemplo: ùëì(ùë•) = ùëêùëúùë†(ùë•) valor exato √©
$$
$$
- Usando Quadratura de Gauss-Legendre com ùëõ = 2, tem-se
$$
$$


- O n√∫mero de pontos de integra√ß√£o controla a precis√£o da aproxima√ß√£o.
- Em R o pacote pracma fornece os pesos e pontos de integra√ß√£o.
- Exemplo
```{r}
require(pracma)
gaussLegendre(n = 2, a = -1, b = 1)
## $x
## [1] -0.5773503 0.5773503
##
## $w
## [1] 1 1
```

- Baseado nos pontos e pesos de integra√ß√£o √© f√°cil construir fun√ß√µes gen√©ricas para integra√ß√£o num√©rica


- Fun√ß√£o gen√©rica
```{r}
gauss_legendre <- function(integrando, n.pontos, a, b, ...){
pontos <- gaussLegendre(n.pontos, a = a, b = b)
integral <- sum(pontos$w*integrando(pontos$x,...))
return(integral)
}
```


- Exemplo: $ $

```{r}
## n = 2
gauss_legendre(integrando = cos, n.pontos = 2, a = -1, b = 1)
## [1] 1.675824
## n = 10
gauss_legendre(integrando = cos, n.pontos = 10, a = -1, b = 1)
## [1] 1.682942
```

## Quadratura de Gauss-Laguerre

- Gauss-Laguerre resolve integrais do tipo:
$$
$$
- Integral √© aproximada por uma soma ponderada.
$$
$$
- A fun√ß√£o √© avaliada nos pontos de Gauss e pesos de integra√ß√£o.
- Os pesos e pontos de integra√ß√£o s√£o obtidos de forma similar ao caso de Gauss-Legendre, por√©m baseado no polin√¥mio de Laguerre.

- Fun√ß√£o gen√©rica para integra√ß√£o de Gauss-Laguerre

```{r}
gauss_laguerre <- function(integrando, n.pontos, ...) {
  pontos <- gaussLaguerre(n.pontos)
  integral <- sum(pontos$w * integrando(pontos$x, ...)
                  / exp(-pontos$x))
  return(integral)
}
```

- Exemplo: $ $

```{r}
fx <- function(x, lambda) lambda*exp(-lambda*x)
## n = 2
gauss_laguerre(integrando = fx, n.pontos = 2, lambda = 10)
## [1] 0.04381233
## n = 10
gauss_laguerre(integrando = fx, n.pontos = 10, lambda = 10)
## [1] 0.8981046
## n = 100
gauss_laguerre(integrando = fx, n.pontos = 100, lambda = 10)
## [1] 1
```


## Quadratura de Gauss-Hermite

- Gauss-Hermite resolve integrais do tipo:
$$
$$
- Integral √© aproximada por uma soma ponderada.
$$
$$
- A fun√ß√£o √© avaliada nos pontos de Gauss e pesos de integra√ß√£o.
- Os pesos e pontos de integra√ß√£o s√£o obtidos de forma similar ao caso de Gauss-Legendre, por√©m baseado no polin√¥mio de Hermite.

- Fun√ß√£o gen√©rica para integra√ß√£o de Gauss-Hermite

```{r}
gauss_hermite <- function(integrando, n.pontos, ...) {
  pontos <- gaussHermite(n.pontos)
  integral <- sum(pontos$w * integrando(pontos$x, ...)
                  / exp(-pontos$x ^ 2))
  return(integral)
}
```


Implementa√ß√£o: quadratura de Gauss-Hermite
- Exemplo: $ $
```{r}
## n = 2
gauss_hermite(integrando = dnorm, n.pontos = 2)
## [1] 0.9079431
## n = 10
gauss_hermite(integrando = dnorm, n.pontos = 10)
## [1] 0.9999876
## n = 100
gauss_hermite(integrando = dnorm, n.pontos = 100)
## [1] 1
```

Limita√ß√µes: quadratura de Gauss
- Quadratura de Gauss apresenta duas grandes limita√ß√µes:
1. Os pontos s√£o escolhidos ignorando a fun√ß√£o a ser integrada.
2. O n√∫mero de pontos necess√°rios para a integra√ß√£o cresce como uma pot√™ncia da dimens√£o da
integral.
3. 20 pontos em uma dimens√£o demanda 202 = 400 pontos em duas dimens√µes.
- Espalhar os pontos de forma inteligente diminui o n√∫mero de pontos necess√°rios.


Quadratura de Gauss-Hermite Adaptativa
- Os pontos de integra√ß√£o s√£o centrados e escalonados como se ùëì(ùë•)ùëí‚àíùë•2
fosse a
distribui√ß√£o Gaussiana.
- A m√©dia da aproxima√ß√£o Gaussiana ser√° a modaÃÇ ùë• de ùëôùëõ[ùëì(ùë•)ùëí‚àíùë•2
].
- A vari√¢ncia da aproxima√ß√£o Gaussiana ser√°
$$
$$
- Novos pontos de integra√ß√£o adaptados ser√£o dados por
$$
$$
com correspondentes pesos,
$$
$$

Silde 105

- Como antes, a integral √© aproximada por
$$
$$
- Problema: como encontrar a moda e o hessiano de 
$$
$$
- Analiticamente ou numericamente.
- Caso especial Gauss-Hermite Adaptativa com ùëõ = 1 ‚Üí Aproxima√ß√£o de Laplace

## Aproxima√ß√£o de Laplace

[[ARRUMAR]]

## Integra√ß√£o Monte Carlo

Integra√ß√£o Monte Carlo
- M√©todo simples e geral para resolver integrais.
- Objetivo: estimar o valor da integral de uma fun√ß√£o ùëì(ùë•) em algum dom√≠nio ùê∑ qualquer,
ou seja,
$$
$$
- Seja ùëù(ùë•) uma fdp cujo dom√≠nio coincide com ùê∑.
- Ent√£o, a integral em Eq. 11 √© equivalente a
$$
$$
- A integral corresponde a 
$$
$$

- Algoritmo: integra√ß√£o Monte Carlo
  1. Gere n√∫meros aleat√≥rios de ùëù(ùë•);
  2. Calcule ùëöùëñ = ùëì(ùë•ùëñ)/ùëù(ùë•ùëñ) para cada amostra,ùëñ = 1, ‚Ä¶ , ùëõ.
  3. Calcule a m√©dia ‚àëùëõ
$$
$$
- Implementa√ß√£o para fun√ß√µes com ùê∑ = ‚Ñú.
```{r}
monte.carlo <- function(funcao, n.pontos, ...) {
  pontos <- rnorm(n.pontos)
  norma <- dnorm(pontos)
  integral <- mean(funcao(pontos,...)/norma)
  return(integral)
}
```


- Exemplo: 
$$
$$

```{r}
## Integrando a Normal padr√£o
monte.carlo(funcao = dnorm, n.pontos = 1000)
## [1] 1
## Integrando distribui√ß√£o t com df = 30
monte.carlo(funcao = dt, n.pontos = 1000, df = 30)
## [1] 0.9982789
```






















